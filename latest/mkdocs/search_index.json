{
    "docs": [
        {
            "location": "/", 
            "text": "Introduction\n\n\nCloudbreak simplifies the provisioning, management and monitoring of on-demand HDP clusters in virtual and cloud environments. Cloudbreak leverages the cloud infrastructure platforms to create host instances, uses Docker technology to deploy the requisite containers cloud-agnostically, and uses Apache Ambari (via Ambari Blueprints) to install and manage the HDP cluster.\n\n\nUse the Cloudbreak UI or CLI to launch HDP clusters on public cloud infrastructure platforms such as Microsoft Azure, Amazon Web Services (AWS), and Google Cloud Platform (GCP) and the private cloud infrastructure platform OpenStack (available as Technical Preview).\n\n\nCloudbreak has two main components: the \nCloudbreak Application\n and the \nCloudbreak Deployer\n.\n\n\nThe \nCloudbreak Application\n is made up from microservices (Cloudbreak, Uluwatu, Sultans, ...). The \nCloudbreak Deployer\n helps you to deploy the Cloudbreak application automatically in environments with Docker support. Once the Cloudbreak Application is deployed you can use it to provision HDP clusters in different cloud environments.\n\n\n\n\nFor an architectural overview of the Cloudbreak Deployer, the Cloudbreak Application, Apache Ambari, Docker and the rest of the Cloudbreak components, please follow this \nlink\n.\n\n\n\n\nInstallation\n\n\nThe high-level process to be able to use Cloudbreak to install an HDP cluster includes the following steps:\n\n\n\n\nInstall the Cloudbreak Deployer\n by either: \ninstalling the Cloudbreak Deployer\n; or instantiating one of the \npre-built cloud images\nthat includes Cloudbreak Deployer\n pre-installed.\n\n\nConfigure the Cloudbreak Deployer and install the Cloudbreak Application\n. Once you have installed Cloudbreak Deployer (cbd), it will start up several Docker containers: Cloudbreak API, Cloudbreak UI (called Uluwatu), Identity Server, and supporting databases. You have finished this step, if you are able to login in your browser to Cloudbreak UI (Uluwatu).\n\n\nProvision an HDP Cluster\n using the Cloudbreak Application.\n\n\n\n\n\n\n\nInstalling the Cloudbreak Deployer\n\n\nYou can install the Cloudbreak Deployer on your own VM manually. Once installed, you will use the deployer to setup\nthe Cloudbreak Application. We suggest you install the Cloudbreak Application as close to the\ndesired HDP clusters as possible. For example, if you plan to launch clusters on AWS, install the Cloudbreak Application in AWS.\n\n\nFollow the instructions for \ninstalling the Cloudbreak Deployer\n. Alternatively, you can consider using one of the \npre-built cloud images that includes Cloudbreak Deployer\n pre-installed.\n\n\n\n\nIMPORTANT:\n If you plan to use Cloudbreak on Azure, you \nmust\n use the \nAzure Setup\n instructions to configure the image.\n\n\n\n\n\n\n\nUsing the Pre-Built Cloud Images\n\n\nWe have pre-built cloud images with Cloudbreak Deployer pre-installed. Following the steps will guide you through the provider specific configuration and launching clusters using that cloud provider.\n\n\n\n\n\n\n\n\nCloud\n\n\nCloud Image\n\n\n\n\n\n\n\n\n\n\nAWS\n\n\nYou can follow the AWS instructions using this \nlink\n.\n\n\n\n\n\n\nAzure\n\n\nThere are no pre-built cloud images available for Azure. See \nAzure Setup\n to get the Cloudbreak Deployer installed and configured.\n\n\n\n\n\n\nGCP\n\n\nYou can follow the GCP instructions using this \nlink\n\n\n\n\n\n\nOpenStack\n\n\nYou can follow the OpenStack instructions using this \nlink\n\n\n\n\n\n\n\n\nLearn More\n\n\nFor more information on Cloudbreak, Docker, Ambari and Ambari Blueprints, see:\n\n\n\n\n\n\n\n\nResource\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCloudbreak Project\n\n\nCloudbreak is a tool to help simplify the provisioning of HDP clusters in virtual and cloud environments.\n\n\n\n\n\n\nCloudbreak Forums\n\n\nGet connected with the community in the Cloudbreak Forums.\n\n\n\n\n\n\nApache Ambari Project\n\n\nApache Ambari is an operational platform for provisioning, managing, and monitoring Apache Hadoop clusters. Ambari exposes a robust set of REST APIs and a rich Web interface for cluster management.\n\n\n\n\n\n\nAmbari Blueprints\n\n\nAmbari Blueprints are a declarative definition of a Hadoop cluster that Ambari can use to create Hadoop clusters.\n\n\n\n\n\n\nDocker\n\n\nDocker is an open platform for developers and system administrators to build, ship, and run distributed applications.", 
            "title": "Introduction"
        }, 
        {
            "location": "/#introduction", 
            "text": "Cloudbreak simplifies the provisioning, management and monitoring of on-demand HDP clusters in virtual and cloud environments. Cloudbreak leverages the cloud infrastructure platforms to create host instances, uses Docker technology to deploy the requisite containers cloud-agnostically, and uses Apache Ambari (via Ambari Blueprints) to install and manage the HDP cluster.  Use the Cloudbreak UI or CLI to launch HDP clusters on public cloud infrastructure platforms such as Microsoft Azure, Amazon Web Services (AWS), and Google Cloud Platform (GCP) and the private cloud infrastructure platform OpenStack (available as Technical Preview).  Cloudbreak has two main components: the  Cloudbreak Application  and the  Cloudbreak Deployer .  The  Cloudbreak Application  is made up from microservices (Cloudbreak, Uluwatu, Sultans, ...). The  Cloudbreak Deployer  helps you to deploy the Cloudbreak application automatically in environments with Docker support. Once the Cloudbreak Application is deployed you can use it to provision HDP clusters in different cloud environments.   For an architectural overview of the Cloudbreak Deployer, the Cloudbreak Application, Apache Ambari, Docker and the rest of the Cloudbreak components, please follow this  link .", 
            "title": "Introduction"
        }, 
        {
            "location": "/#installation", 
            "text": "The high-level process to be able to use Cloudbreak to install an HDP cluster includes the following steps:   Install the Cloudbreak Deployer  by either:  installing the Cloudbreak Deployer ; or instantiating one of the  pre-built cloud images\nthat includes Cloudbreak Deployer  pre-installed.  Configure the Cloudbreak Deployer and install the Cloudbreak Application . Once you have installed Cloudbreak Deployer (cbd), it will start up several Docker containers: Cloudbreak API, Cloudbreak UI (called Uluwatu), Identity Server, and supporting databases. You have finished this step, if you are able to login in your browser to Cloudbreak UI (Uluwatu).  Provision an HDP Cluster  using the Cloudbreak Application.    Installing the Cloudbreak Deployer  You can install the Cloudbreak Deployer on your own VM manually. Once installed, you will use the deployer to setup\nthe Cloudbreak Application. We suggest you install the Cloudbreak Application as close to the\ndesired HDP clusters as possible. For example, if you plan to launch clusters on AWS, install the Cloudbreak Application in AWS.  Follow the instructions for  installing the Cloudbreak Deployer . Alternatively, you can consider using one of the  pre-built cloud images that includes Cloudbreak Deployer  pre-installed.   IMPORTANT:  If you plan to use Cloudbreak on Azure, you  must  use the  Azure Setup  instructions to configure the image.    Using the Pre-Built Cloud Images  We have pre-built cloud images with Cloudbreak Deployer pre-installed. Following the steps will guide you through the provider specific configuration and launching clusters using that cloud provider.     Cloud  Cloud Image      AWS  You can follow the AWS instructions using this  link .    Azure  There are no pre-built cloud images available for Azure. See  Azure Setup  to get the Cloudbreak Deployer installed and configured.    GCP  You can follow the GCP instructions using this  link    OpenStack  You can follow the OpenStack instructions using this  link", 
            "title": "Installation"
        }, 
        {
            "location": "/#learn-more", 
            "text": "For more information on Cloudbreak, Docker, Ambari and Ambari Blueprints, see:     Resource  Description      Cloudbreak Project  Cloudbreak is a tool to help simplify the provisioning of HDP clusters in virtual and cloud environments.    Cloudbreak Forums  Get connected with the community in the Cloudbreak Forums.    Apache Ambari Project  Apache Ambari is an operational platform for provisioning, managing, and monitoring Apache Hadoop clusters. Ambari exposes a robust set of REST APIs and a rich Web interface for cluster management.    Ambari Blueprints  Ambari Blueprints are a declarative definition of a Hadoop cluster that Ambari can use to create Hadoop clusters.    Docker  Docker is an open platform for developers and system administrators to build, ship, and run distributed applications.", 
            "title": "Learn More"
        }, 
        {
            "location": "/architecture/", 
            "text": "Architecture\n\n\nCloudbreak Deployer Architecture\n\n\n\n\nuaa\n: OAuth Identity Server\n\n\ncloudbreak\n - the Cloudbreak app\n\n\nperiscope\n - the Periscope app\n\n\nuluwatu\n - Cloudbreak UI\n\n\nsultans\n - user management\n\n\n\n\nSystem Level Containers\n\n\n\n\nconsul: Service Registry\n\n\nregistrator: automatically registers/unregisters containers with Consul\n\n\n\n\nCloudbreak Application Architecture\n\n\nCloudbreak is built on the foundation of cloud providers APIs, Apache Ambari, Docker containers, Swarm and Consul.\n\n\nApache Ambari\n\n\nThe Apache Ambari project is aimed at making Hadoop management simpler by developing software for provisioning, managing, and monitoring Apache Hadoop clusters. Ambari provides an intuitive, easy-to-use Hadoop management web UI backed by its RESTful APIs.\n\n\n\n\nAmbari enables System Administrators to:\n\n\n\n\nProvision a Hadoop Cluster\n\n\nAmbari provides a step-by-step wizard for installing Hadoop services across any number of hosts.\n\n\n\n\nAmbari handles configuration of Hadoop services for the cluster.\n\n\n\n\n\n\nManage a Hadoop Cluster\n\n\n\n\n\n\nAmbari provides central management for starting, stopping, and reconfiguring Hadoop services across the entire cluster.\n\n\n\n\n\n\nMonitor a Hadoop Cluster\n\n\n\n\nAmbari provides a dashboard for monitoring health and status of the Hadoop cluster.\n\n\nAmbari allows to choose between predefined alerts or add yur custom ones\n\n\n\n\nAmbari enables to integrate Hadoop provisioning, management and monitoring capabilities into applications with the Ambari REST APIs.\nAmbari Blueprints are a declarative definition of a cluster. With a Blueprint, you can specify a Stack, the Component layout and the Configurations to materialise a Hadoop cluster instance (via a REST API) without having to use the Ambari Cluster Install Wizard.\n\n\n\n\nDocker\n\n\nDocker is an open platform for developers and sysadmins to build, ship, and run distributed applications. Consisting of Docker Engine, a portable, lightweight runtime and packaging tool, and Docker Hub, a cloud service for sharing applications and automating workflows, Docker enables apps to be quickly assembled from components and eliminates the friction between development, QA, and production environments. As a result, IT can ship faster and run the same app, unchanged, on laptops, data center VMs, and any cloud.\n\n\nThe main features of Docker are:\n\n\n\n\nLightweight, portable\n\n\nBuild once, run anywhere\n\n\nVM - without the overhead of a VM\n\n\nEach virtualised application includes not only the application and the necessary binaries and libraries, but also an entire guest operating system\n\n\n\n\nThe Docker Engine container comprises just the application and its dependencies. It runs as an isolated process in userspace on the host operating system, sharing the kernel with other containers.\n    \n\n\n\n\n\n\nContainers are isolated\n\n\n\n\nIt can be automated and scripted\n\n\n\n\nSwarm\n\n\nDocker Swarm is native clustering for Docker. It turns a pool of Docker hosts into a single, virtual host. Swarm serves the standard Docker API.\n\n\n\n\nDistributed container orchestration: Allows to remotely orchestrate Docker containers on different hosts\n    \n\n\nDiscovery services: Supports different discovery backends to provide service discovery, as such: token (hosted) and file based, etcd, Consul, Zookeeper.\n\n\nAdvanced scheduling: Swarm will schedule containers on hosts based on different filters and strategies\n\n\n\n\nConsul\n\n\nConsul it is a tool for discovering and configuring services in your infrastructure. It provides several key features\n\n\n\n\n\n\nService Discovery: Clients of Consul can provide a service, such as api or mysql, and other clients can use Consul to discover providers of a given service. Using either DNS or HTTP, applications can easily find the services they depend upon.\n\n\n\n\n\n\nHealth Checking: Consul clients can provide any number of health checks, either associated with a given service (\"is the webserver returning 200 OK\"), or with the local node (\"is memory utilization below 90%\"). This information can be used by an operator to monitor cluster health, and it is used by the service discovery components to route traffic away from unhealthy hosts.\n\n\n\n\n\n\nKey/Value Store: Applications can make use of Consul's hierarchical key/value store for any number of purposes, including dynamic configuration, feature flagging, coordination, leader election, and more. The simple HTTP API makes it easy to use.\n\n\n\n\n\n\nMulti Datacenter: Consul supports multiple datacenters out of the box. This means users of Consul do not have to worry about building additional layers of abstraction to grow to multiple regions.", 
            "title": "Architecture"
        }, 
        {
            "location": "/architecture/#architecture", 
            "text": "", 
            "title": "Architecture"
        }, 
        {
            "location": "/architecture/#cloudbreak-deployer-architecture", 
            "text": "uaa : OAuth Identity Server  cloudbreak  - the Cloudbreak app  periscope  - the Periscope app  uluwatu  - Cloudbreak UI  sultans  - user management   System Level Containers   consul: Service Registry  registrator: automatically registers/unregisters containers with Consul", 
            "title": "Cloudbreak Deployer Architecture"
        }, 
        {
            "location": "/architecture/#cloudbreak-application-architecture", 
            "text": "Cloudbreak is built on the foundation of cloud providers APIs, Apache Ambari, Docker containers, Swarm and Consul.  Apache Ambari  The Apache Ambari project is aimed at making Hadoop management simpler by developing software for provisioning, managing, and monitoring Apache Hadoop clusters. Ambari provides an intuitive, easy-to-use Hadoop management web UI backed by its RESTful APIs.   Ambari enables System Administrators to:   Provision a Hadoop Cluster  Ambari provides a step-by-step wizard for installing Hadoop services across any number of hosts.   Ambari handles configuration of Hadoop services for the cluster.    Manage a Hadoop Cluster    Ambari provides central management for starting, stopping, and reconfiguring Hadoop services across the entire cluster.    Monitor a Hadoop Cluster   Ambari provides a dashboard for monitoring health and status of the Hadoop cluster.  Ambari allows to choose between predefined alerts or add yur custom ones   Ambari enables to integrate Hadoop provisioning, management and monitoring capabilities into applications with the Ambari REST APIs.\nAmbari Blueprints are a declarative definition of a cluster. With a Blueprint, you can specify a Stack, the Component layout and the Configurations to materialise a Hadoop cluster instance (via a REST API) without having to use the Ambari Cluster Install Wizard.   Docker  Docker is an open platform for developers and sysadmins to build, ship, and run distributed applications. Consisting of Docker Engine, a portable, lightweight runtime and packaging tool, and Docker Hub, a cloud service for sharing applications and automating workflows, Docker enables apps to be quickly assembled from components and eliminates the friction between development, QA, and production environments. As a result, IT can ship faster and run the same app, unchanged, on laptops, data center VMs, and any cloud.  The main features of Docker are:   Lightweight, portable  Build once, run anywhere  VM - without the overhead of a VM  Each virtualised application includes not only the application and the necessary binaries and libraries, but also an entire guest operating system   The Docker Engine container comprises just the application and its dependencies. It runs as an isolated process in userspace on the host operating system, sharing the kernel with other containers.\n        Containers are isolated   It can be automated and scripted   Swarm  Docker Swarm is native clustering for Docker. It turns a pool of Docker hosts into a single, virtual host. Swarm serves the standard Docker API.   Distributed container orchestration: Allows to remotely orchestrate Docker containers on different hosts\n      Discovery services: Supports different discovery backends to provide service discovery, as such: token (hosted) and file based, etcd, Consul, Zookeeper.  Advanced scheduling: Swarm will schedule containers on hosts based on different filters and strategies   Consul  Consul it is a tool for discovering and configuring services in your infrastructure. It provides several key features    Service Discovery: Clients of Consul can provide a service, such as api or mysql, and other clients can use Consul to discover providers of a given service. Using either DNS or HTTP, applications can easily find the services they depend upon.    Health Checking: Consul clients can provide any number of health checks, either associated with a given service (\"is the webserver returning 200 OK\"), or with the local node (\"is memory utilization below 90%\"). This information can be used by an operator to monitor cluster health, and it is used by the service discovery components to route traffic away from unhealthy hosts.    Key/Value Store: Applications can make use of Consul's hierarchical key/value store for any number of purposes, including dynamic configuration, feature flagging, coordination, leader election, and more. The simple HTTP API makes it easy to use.    Multi Datacenter: Consul supports multiple datacenters out of the box. This means users of Consul do not have to worry about building additional layers of abstraction to grow to multiple regions.", 
            "title": "Cloudbreak Application Architecture"
        }, 
        {
            "location": "/onprem/", 
            "text": "Install Cloudbreak Deployer\n\n\nTo install Cloudbreak Deployer on your selected environment you have to follow the steps below. The instruction describe a CentOS-based installation.\n\n\n\n\nIMPORTANT:\n If you plan to use Cloudbreak on Azure, you \nmust\n use the \nAzure Setup\n instructions to install and configure the Cloudbreak.\n\n\n\n\nSystem Requirements\n\n\nTo run the Cloudbreak Deployer and install the Cloudbreak Application, you must meet the following system requirements:\n\n\n\n\nRHEL / CentOS / Oracle Linux 7 (64-bit)\n\n\nDocker 1.8.3 (or later)\n\n\n\n\n\n\nYou can install Cloudbreak on Mac OS X \"Darwin\" for \nevaluation purposes only\n. This operating system is not supported for a production deployment of Cloudbreak.\n\n\n\n\nMake sure you opened the following ports:\n\n\n\n\nSSH (22)\n\n\nAmbari (8080)\n\n\nIdentity server (8089)\n\n\nCloudbreak GUI (3000)\n\n\nUser authentication (3001)\n\n\n\n\nAssume \nroot\n privileges with this command:\n\n\nsudo su\n\n\n\n\nTo permanently disable \nSELinux\n set SELINUX=disabled in /etc/selinux/config This ensures that SELinux does not turn itself on after you reboot the machine:\n\n\nsetenforce 0 \n sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config\n\n\n\n\nYou need to install iptables-services, otherwise the 'iptables save' command will not be available:\n\n\nyum -y install iptables-services net-tools unzip\n\n\n\n\nPlease configure your iptables on your machine:\n\n\niptables --flush INPUT \n \\\niptables --flush FORWARD \n \\\nservice iptables save \n \\\nsed -i 's/net.ipv4.ip_forward = 0/net.ipv4.ip_forward = 1/g' /etc/sysctl.conf\n\n\n\n\nConfigure a custom Docker repository for installing the correct version of Docker:\n\n\ncat \n /etc/yum.repos.d/docker.repo \nEOF\n\n[dockerrepo]\nname=Docker Repository\nbaseurl=https://yum.dockerproject.org/repo/main/centos/7\nenabled=1\ngpgcheck=1\ngpgkey=https://yum.dockerproject.org/gpg\nEOF\n\n\n\n\nThen you are able to install the Docker service:\n\n\nyum install -y docker-engine-1.8.3\n\n\n\n\nConfigure your installed Docker service:\n\n\ncat \n /usr/lib/systemd/system/docker.service \nEOF\n\n[Unit]\nDescription=Docker Application Container Engine\nDocumentation=https://docs.docker.com\nAfter=network.target docker.socket cloud-final.service\nRequires=docker.socket\nWants=cloud-final.service\n\n[Service]\nExecStart=/usr/bin/docker -d -H fd:// -H tcp://0.0.0.0:2376 --selinux-enabled=false --storage-driver=devicemapper --storage-opt=dm.basesize=30G\nMountFlags=slave\nLimitNOFILE=200000\nLimitNPROC=16384\nLimitCORE=infinity\n\n[Install]\nWantedBy=multi-user.target\nEOF\n\n\n\n\nRemove docker folder and restart Docker service:\n\n\nrm -rf /var/lib/docker \n systemctl daemon-reload \n service docker start \n systemctl enable docker.service\n\n\n\n\nInstall Cloudbreak deployer\n\n\nInstall the Cloudbreak deployer and unzip the platform specific single binary to your PATH. The one-liner way is:\n\n\ncurl https://raw.githubusercontent.com/sequenceiq/cloudbreak-deployer/master/install-latest | sh \n cbd --version\n\n\n\n\nOnce the Cloudbreak deployer is installed, you can start to setup the Cloudbreak application.\n\n\nInitialize your Profile\n\n\nFirst initialize cbd by creating a \nProfile\n file:\n\n\ncbd init\n\n\n\n\nIt will create a \nProfile\n file in the current directory. Please edit the file - the only required\nconfiguration is the \nPUBLIC_IP\n. This IP will be used to access the Cloudbreak UI\n(called Uluwatu). In some cases the \ncbd\n tool tries to guess it, if can't than will give a hint.\n\n\nGenerate your Profile\n\n\nYou are done with the configuration of Cloudbreak deployer. The last thing you have to do is to regenerate the configurations in order to take effect.\n\n\nrm *.yml\ncbd generate\n\n\n\n\nThis command applies the following steps:\n\n\n\n\ncreates the \ndocker-compose.yml\n file that describes the configuration of all the Docker containers needed for the Cloudbreak deployment.\n\n\ncreates the \nuaa.yml\n file that holds the configuration of the identity server used to authenticate users to Cloudbreak.\n\n\n\n\nStart Cloudbreak\n\n\nTo start the Cloudbreak application use the following command.\nThis will start all the Docker containers and initialize the application. It will take a few minutes until all the services start.\n\n\ncbd start\n\n\n\n\n\n\nLaunching it first will take more time as it downloads all the docker images needed by Cloudbreak.\n\n\n\n\nAfter the \ncbd start\n command finishes you can check the logs of the Cloudbreak server with this command:\n\n\ncbd logs cloudbreak\n\n\n\n\n\n\nCloudbreak server should start within a minute - you should see a line like this: \nStarted CloudbreakApplication in 36.823 seconds\n\n\n\n\nNext steps\n\n\nNow that you all pre-requisites for Cloudbreak are in place you can follow with the \ncloud provider specific\n configuration. Based on the location where you plan to launch HDP clusters select one of the providers documentation and follow the steps from the \nDeployment\n section.\n\n\nYou can find the provider specific documentations here:\n\n\n\n\nAWS\n\n\nAzure\n\n\nGCP\n\n\nOpenStack", 
            "title": "Cloudbreak Deployer"
        }, 
        {
            "location": "/onprem/#install-cloudbreak-deployer", 
            "text": "To install Cloudbreak Deployer on your selected environment you have to follow the steps below. The instruction describe a CentOS-based installation.   IMPORTANT:  If you plan to use Cloudbreak on Azure, you  must  use the  Azure Setup  instructions to install and configure the Cloudbreak.", 
            "title": "Install Cloudbreak Deployer"
        }, 
        {
            "location": "/onprem/#system-requirements", 
            "text": "To run the Cloudbreak Deployer and install the Cloudbreak Application, you must meet the following system requirements:   RHEL / CentOS / Oracle Linux 7 (64-bit)  Docker 1.8.3 (or later)    You can install Cloudbreak on Mac OS X \"Darwin\" for  evaluation purposes only . This operating system is not supported for a production deployment of Cloudbreak.   Make sure you opened the following ports:   SSH (22)  Ambari (8080)  Identity server (8089)  Cloudbreak GUI (3000)  User authentication (3001)   Assume  root  privileges with this command:  sudo su  To permanently disable  SELinux  set SELINUX=disabled in /etc/selinux/config This ensures that SELinux does not turn itself on after you reboot the machine:  setenforce 0   sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config  You need to install iptables-services, otherwise the 'iptables save' command will not be available:  yum -y install iptables-services net-tools unzip  Please configure your iptables on your machine:  iptables --flush INPUT   \\\niptables --flush FORWARD   \\\nservice iptables save   \\\nsed -i 's/net.ipv4.ip_forward = 0/net.ipv4.ip_forward = 1/g' /etc/sysctl.conf  Configure a custom Docker repository for installing the correct version of Docker:  cat   /etc/yum.repos.d/docker.repo  EOF \n[dockerrepo]\nname=Docker Repository\nbaseurl=https://yum.dockerproject.org/repo/main/centos/7\nenabled=1\ngpgcheck=1\ngpgkey=https://yum.dockerproject.org/gpg\nEOF  Then you are able to install the Docker service:  yum install -y docker-engine-1.8.3  Configure your installed Docker service:  cat   /usr/lib/systemd/system/docker.service  EOF \n[Unit]\nDescription=Docker Application Container Engine\nDocumentation=https://docs.docker.com\nAfter=network.target docker.socket cloud-final.service\nRequires=docker.socket\nWants=cloud-final.service\n\n[Service]\nExecStart=/usr/bin/docker -d -H fd:// -H tcp://0.0.0.0:2376 --selinux-enabled=false --storage-driver=devicemapper --storage-opt=dm.basesize=30G\nMountFlags=slave\nLimitNOFILE=200000\nLimitNPROC=16384\nLimitCORE=infinity\n\n[Install]\nWantedBy=multi-user.target\nEOF  Remove docker folder and restart Docker service:  rm -rf /var/lib/docker   systemctl daemon-reload   service docker start   systemctl enable docker.service", 
            "title": "System Requirements"
        }, 
        {
            "location": "/onprem/#install-cloudbreak-deployer_1", 
            "text": "Install the Cloudbreak deployer and unzip the platform specific single binary to your PATH. The one-liner way is:  curl https://raw.githubusercontent.com/sequenceiq/cloudbreak-deployer/master/install-latest | sh   cbd --version  Once the Cloudbreak deployer is installed, you can start to setup the Cloudbreak application.", 
            "title": "Install Cloudbreak deployer"
        }, 
        {
            "location": "/onprem/#initialize-your-profile", 
            "text": "First initialize cbd by creating a  Profile  file:  cbd init  It will create a  Profile  file in the current directory. Please edit the file - the only required\nconfiguration is the  PUBLIC_IP . This IP will be used to access the Cloudbreak UI\n(called Uluwatu). In some cases the  cbd  tool tries to guess it, if can't than will give a hint.", 
            "title": "Initialize your Profile"
        }, 
        {
            "location": "/onprem/#generate-your-profile", 
            "text": "You are done with the configuration of Cloudbreak deployer. The last thing you have to do is to regenerate the configurations in order to take effect.  rm *.yml\ncbd generate  This command applies the following steps:   creates the  docker-compose.yml  file that describes the configuration of all the Docker containers needed for the Cloudbreak deployment.  creates the  uaa.yml  file that holds the configuration of the identity server used to authenticate users to Cloudbreak.", 
            "title": "Generate your Profile"
        }, 
        {
            "location": "/onprem/#start-cloudbreak", 
            "text": "To start the Cloudbreak application use the following command.\nThis will start all the Docker containers and initialize the application. It will take a few minutes until all the services start.  cbd start   Launching it first will take more time as it downloads all the docker images needed by Cloudbreak.   After the  cbd start  command finishes you can check the logs of the Cloudbreak server with this command:  cbd logs cloudbreak   Cloudbreak server should start within a minute - you should see a line like this:  Started CloudbreakApplication in 36.823 seconds", 
            "title": "Start Cloudbreak"
        }, 
        {
            "location": "/onprem/#next-steps", 
            "text": "Now that you all pre-requisites for Cloudbreak are in place you can follow with the  cloud provider specific  configuration. Based on the location where you plan to launch HDP clusters select one of the providers documentation and follow the steps from the  Deployment  section.  You can find the provider specific documentations here:   AWS  Azure  GCP  OpenStack", 
            "title": "Next steps"
        }, 
        {
            "location": "/aws-image/", 
            "text": "AWS Cloud Images\n\n\nWe have pre-built cloud images for AWS with the Cloudbreak Deployer pre-installed. You can launch the latest Cloudbreak Deployer image based on your region at the \nAWS Management Console\n.\n\n\n\n\nAlternatively, instead of using the pre-built cloud images for AWS, you can install Cloudbreak Deployer on your own VM. See \ninstall the Cloudbreak Deployer\n for more information.\n\n\n\n\nMake sure you opened the following ports on your virtual machine:\n\n\n\n\nSSH (22)\n\n\nAmbari (8080)\n\n\nIdentity server (8089)\n\n\nCloudbreak GUI (3000)\n\n\nUser authentication (3001)\n\n\n\n\nAWS Image Details\n\n\nSetup Cloudbreak Deployer\n\n\nOnce you have the Cloudbreak Deployer installed, proceed to \nSetup Cloudbreak Deployer\n.", 
            "title": "AWS Cloud Images"
        }, 
        {
            "location": "/aws-image/#aws-cloud-images", 
            "text": "We have pre-built cloud images for AWS with the Cloudbreak Deployer pre-installed. You can launch the latest Cloudbreak Deployer image based on your region at the  AWS Management Console .   Alternatively, instead of using the pre-built cloud images for AWS, you can install Cloudbreak Deployer on your own VM. See  install the Cloudbreak Deployer  for more information.   Make sure you opened the following ports on your virtual machine:   SSH (22)  Ambari (8080)  Identity server (8089)  Cloudbreak GUI (3000)  User authentication (3001)   AWS Image Details", 
            "title": "AWS Cloud Images"
        }, 
        {
            "location": "/aws-image/#setup-cloudbreak-deployer", 
            "text": "Once you have the Cloudbreak Deployer installed, proceed to  Setup Cloudbreak Deployer .", 
            "title": "Setup Cloudbreak Deployer"
        }, 
        {
            "location": "/gcp-image/", 
            "text": "Google Cloud Images\n\n\nWe have pre-built cloud images for GCP with the Cloudbreak Deployer pre-installed. Following the steps will guide you through the provider specific configuration then launch.\n\n\n\n\nAlternatively, instead of using the pre-built cloud images, you can install Cloudbreak Deployer on your own VM. See \ninstall the Cloudbreak Deployer\n for more information.\n\n\n\n\nConfigured Image\n\n\nYou can create the latest Cloudbreak deployer image on the \nGoogle Developers Console\n with the help\n of the \nGoogle Cloud Shell\n.\n\n\n\n\nImages are global resources, so they can be used across zones and projects.\n\n\nGCP Image Details\n\n\n\n\nPlease make sure you opened the following ports on your virtual machine:\n\n\n\n\nSSH (22)\n\n\nAmbari (8080)\n\n\nIdentity server (8089)\n\n\nCloudbreak GUI (3000)\n\n\nUser authentication (3001)\n\n\n\n\nSetup Cloudbreak Deployer\n\n\nOnce you have the Cloudbreak Deployer installed, proceed to \nSetup Cloudbreak Deployer\n.", 
            "title": "GCP Cloud Images"
        }, 
        {
            "location": "/gcp-image/#google-cloud-images", 
            "text": "We have pre-built cloud images for GCP with the Cloudbreak Deployer pre-installed. Following the steps will guide you through the provider specific configuration then launch.   Alternatively, instead of using the pre-built cloud images, you can install Cloudbreak Deployer on your own VM. See  install the Cloudbreak Deployer  for more information.", 
            "title": "Google Cloud Images"
        }, 
        {
            "location": "/gcp-image/#configured-image", 
            "text": "You can create the latest Cloudbreak deployer image on the  Google Developers Console  with the help\n of the  Google Cloud Shell .   Images are global resources, so they can be used across zones and projects.  GCP Image Details   Please make sure you opened the following ports on your virtual machine:   SSH (22)  Ambari (8080)  Identity server (8089)  Cloudbreak GUI (3000)  User authentication (3001)", 
            "title": "Configured Image"
        }, 
        {
            "location": "/gcp-image/#setup-cloudbreak-deployer", 
            "text": "Once you have the Cloudbreak Deployer installed, proceed to  Setup Cloudbreak Deployer .", 
            "title": "Setup Cloudbreak Deployer"
        }, 
        {
            "location": "/openstack-image/", 
            "text": "OpenStack Cloud Images\n\n\nWe have pre-built cloud images for OpenStack with the Cloudbreak Deployer pre-installed. Following the steps will guide you through the provider specific configuration then launch.\n\n\n\n\nAlternatively, instead of using the pre-built cloud images, you can install Cloudbreak Deployer on your own VM. See \ninstall the Cloudbreak Deployer\n for more information.\n\n\n\n\nSystem Requirements\n\n\nCloudbreak currently only supports the \nOpenStack Juno\n release.\n\n\nDownload the Cloudbreak image\n\n\nYou can download the latest pre-configured Cloudbreak deployer image for OpenStack with the following script in the \nfollowing section.\n\n\nPlease make sure you opened the following ports on your virtual machine:\n\n\n\n\nSSH (22)\n\n\nAmbari (8080)\n\n\nIdentity server (8089)\n\n\nCloudbreak GUI (3000)\n\n\nUser authentication (3001)\n\n\n\n\nOpenStack image details\n\n\nImport the image into OpenStack\n\n\nexport OS_IMAGE_NAME=\nname_in_openstack\n\nexport OS_USERNAME=...\nexport OS_AUTH_URL=\nhttp://.../v2.0\n\nexport OS_TENANT_NAME=...\nglance image-create --name \n$OS_IMAGE_NAME\n --file \n$LATEST_IMAGE\n --disk-format qcow2 --container-format bare --progress\n\n\n\n\nSetup Cloudbreak Deployer\n\n\nOnce you have the Cloudbreak Deployer installed, proceed to \nSetup Cloudbreak Deployer\n.", 
            "title": "OpenStack Cloud Images"
        }, 
        {
            "location": "/openstack-image/#openstack-cloud-images", 
            "text": "We have pre-built cloud images for OpenStack with the Cloudbreak Deployer pre-installed. Following the steps will guide you through the provider specific configuration then launch.   Alternatively, instead of using the pre-built cloud images, you can install Cloudbreak Deployer on your own VM. See  install the Cloudbreak Deployer  for more information.", 
            "title": "OpenStack Cloud Images"
        }, 
        {
            "location": "/openstack-image/#system-requirements", 
            "text": "Cloudbreak currently only supports the  OpenStack Juno  release.", 
            "title": "System Requirements"
        }, 
        {
            "location": "/openstack-image/#download-the-cloudbreak-image", 
            "text": "You can download the latest pre-configured Cloudbreak deployer image for OpenStack with the following script in the \nfollowing section.  Please make sure you opened the following ports on your virtual machine:   SSH (22)  Ambari (8080)  Identity server (8089)  Cloudbreak GUI (3000)  User authentication (3001)   OpenStack image details", 
            "title": "Download the Cloudbreak image"
        }, 
        {
            "location": "/openstack-image/#import-the-image-into-openstack", 
            "text": "export OS_IMAGE_NAME= name_in_openstack \nexport OS_USERNAME=...\nexport OS_AUTH_URL= http://.../v2.0 \nexport OS_TENANT_NAME=...\nglance image-create --name  $OS_IMAGE_NAME  --file  $LATEST_IMAGE  --disk-format qcow2 --container-format bare --progress", 
            "title": "Import the image into OpenStack"
        }, 
        {
            "location": "/openstack-image/#setup-cloudbreak-deployer", 
            "text": "Once you have the Cloudbreak Deployer installed, proceed to  Setup Cloudbreak Deployer .", 
            "title": "Setup Cloudbreak Deployer"
        }, 
        {
            "location": "/releasenotes/", 
            "text": "Release Notes\n\n\nThe Release Notes summarize and describe changes released in Cloudbreak.\n\n\nNew Features\n\n\nThis release includes the following new features and improvements:\n\n\n\n\n\n\n\n\nFeature\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nRecipes\n\n\nAbility to script extensions that run before/after cluster installation. See \nRecipes\n for more information.\n\n\n\n\n\n\nCloudbreak Shell\n\n\nA Command Line Interface (CLI) for interactively managing Cloudbreak. See \nShell\n for more information.\n\n\n\n\n\n\nPre-built Cloud Images\n\n\nTechnical Preview\n Pre-built Cloud images for AWS, GCP and OpenStack that include Cloudbreak Deployer pre-installed and configured.\n\n\n\n\n\n\nKerberos\n\n\nTechnical Preview\n Support for enabling Kerberos on the HDP clusters deployed by Cloudbreak. See \nKerberos\n for more information.\n\n\n\n\n\n\nOpenStack Cloud Provider\n\n\nTechnical Preview\n Support for OpenStack Juno cloud provider. See \nOpenStack\n for more information.\n\n\n\n\n\n\nCloud Provider SPI\n\n\nTechnical Preview\n Cloudbreak Service Provider Interface (SPI) for pluging-in new providers. See \nSPI\n for more information.\n\n\n\n\n\n\n\n\nBehavioral Changes\n\n\nThis release introduces the following changes in behavior as compared to previous Cloudbreak versions:\n\n\n\n\n\n\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nUI changes\n\n\nCluster creation is based on a step-by-step wizard.\n\n\n\n\n\n\nCustom Security Groups\n\n\nAbility to define and create custom security groups and rules.\n\n\n\n\n\n\nAzure ARM support\n\n\nWith this release we have switched to the new \nAzure ARM API\n aka \nAzure API v2\n. Using the old API is not supported anymore - users have the option to \nterminate only\n clusters lunched with the old API. All new clusters are lunched with the new API.\n\n\n\n\n\n\nWASB support\n\n\nFor clusters launched on Microsoft Azure the default file system in use will be \nWASB\n. Users will still have to option to use local HDFS with attached disk but the recommended file system will be WASB. See \nFilesystem configuration\n for more information.\n\n\n\n\n\n\nDASH support for WASB\n\n\nWhen WASB is used as a Hadoop filesystem the files are full-value blobs in a storage account. It means better performance compared to the data disks and the WASB filesystem can be configured very easily but Azure storage accounts have their own \nlimitations\n as well. There is a space limitation for TB per storage account (500 TB) as well but the real bottleneck is the total request rate that is only 20000 IOPS where Azure will start to throw errors when trying to do an I/O operation. To bypass those limits Microsoft created a small service called \nDASH\n. See \nFilesystem configuration\n for more information.\n\n\n\n\n\n\nSupport for new regions\n\n\nOn AWS we added support for \nFrankfurt\n. On GCP we added support for \nus-east-1\n.\n\n\n\n\n\n\nUAA zones\n\n\nUpdated to UAA 2.7.1 version, which introduced the concept of \nzones\n. See \nAccess from custom domains\n for more information.\n\n\n\n\n\n\n\n\nPatch Information\n\n\nTBD\n\n\nKnown Issues\n\n\nCloudbreak has the following known issues, scheduled for resolution in a future release. Please work around the following issues:\n\n\n\n\n\n\n\n\nJIRA\n\n\nProblem\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\nX\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\nFixed Issues\n\n\nThe following sections list selected issues resolved in Cloudbreak 1.1:\n\n\n\n\n\n\n\n\nJIRA\n\n\nCategory\n\n\nSummary\n\n\n\n\n\n\n\n\n\n\nX\n\n\nPotential Data Loss\n\n\nX\n\n\n\n\n\n\nX\n\n\nStability\n\n\nX\n\n\n\n\n\n\nX\n\n\nSecurity\n\n\nX\n\n\n\n\n\n\nX\n\n\nUpgrade\n\n\nX\n\n\n\n\n\n\nX\n\n\nUsability\n\n\nX\n\n\n\n\n\n\nX\n\n\nPerformance\n\n\nX\n\n\n\n\n\n\nX\n\n\nOther\n\n\nX", 
            "title": "Release Notes"
        }, 
        {
            "location": "/releasenotes/#release-notes", 
            "text": "The Release Notes summarize and describe changes released in Cloudbreak.", 
            "title": "Release Notes"
        }, 
        {
            "location": "/releasenotes/#new-features", 
            "text": "This release includes the following new features and improvements:     Feature  Description      Recipes  Ability to script extensions that run before/after cluster installation. See  Recipes  for more information.    Cloudbreak Shell  A Command Line Interface (CLI) for interactively managing Cloudbreak. See  Shell  for more information.    Pre-built Cloud Images  Technical Preview  Pre-built Cloud images for AWS, GCP and OpenStack that include Cloudbreak Deployer pre-installed and configured.    Kerberos  Technical Preview  Support for enabling Kerberos on the HDP clusters deployed by Cloudbreak. See  Kerberos  for more information.    OpenStack Cloud Provider  Technical Preview  Support for OpenStack Juno cloud provider. See  OpenStack  for more information.    Cloud Provider SPI  Technical Preview  Cloudbreak Service Provider Interface (SPI) for pluging-in new providers. See  SPI  for more information.", 
            "title": "New Features"
        }, 
        {
            "location": "/releasenotes/#behavioral-changes", 
            "text": "This release introduces the following changes in behavior as compared to previous Cloudbreak versions:     Title  Description      UI changes  Cluster creation is based on a step-by-step wizard.    Custom Security Groups  Ability to define and create custom security groups and rules.    Azure ARM support  With this release we have switched to the new  Azure ARM API  aka  Azure API v2 . Using the old API is not supported anymore - users have the option to  terminate only  clusters lunched with the old API. All new clusters are lunched with the new API.    WASB support  For clusters launched on Microsoft Azure the default file system in use will be  WASB . Users will still have to option to use local HDFS with attached disk but the recommended file system will be WASB. See  Filesystem configuration  for more information.    DASH support for WASB  When WASB is used as a Hadoop filesystem the files are full-value blobs in a storage account. It means better performance compared to the data disks and the WASB filesystem can be configured very easily but Azure storage accounts have their own  limitations  as well. There is a space limitation for TB per storage account (500 TB) as well but the real bottleneck is the total request rate that is only 20000 IOPS where Azure will start to throw errors when trying to do an I/O operation. To bypass those limits Microsoft created a small service called  DASH . See  Filesystem configuration  for more information.    Support for new regions  On AWS we added support for  Frankfurt . On GCP we added support for  us-east-1 .    UAA zones  Updated to UAA 2.7.1 version, which introduced the concept of  zones . See  Access from custom domains  for more information.", 
            "title": "Behavioral Changes"
        }, 
        {
            "location": "/releasenotes/#patch-information", 
            "text": "TBD", 
            "title": "Patch Information"
        }, 
        {
            "location": "/releasenotes/#known-issues", 
            "text": "Cloudbreak has the following known issues, scheduled for resolution in a future release. Please work around the following issues:     JIRA  Problem  Solution      X  X  X", 
            "title": "Known Issues"
        }, 
        {
            "location": "/releasenotes/#fixed-issues", 
            "text": "The following sections list selected issues resolved in Cloudbreak 1.1:     JIRA  Category  Summary      X  Potential Data Loss  X    X  Stability  X    X  Security  X    X  Upgrade  X    X  Usability  X    X  Performance  X    X  Other  X", 
            "title": "Fixed Issues"
        }, 
        {
            "location": "/aws/", 
            "text": "AWS Setup\n\n\nSetup Cloudbreak Deployer\n\n\nIf you already have Cloudbreak Deployer either by \nusing the AWS Cloud Images\n or by \ninstalling the Cloudbreak Deployer\n manually on your own VM,\nyou can start to setup the Cloudbreak Application with the deployer.\n\n\nCreate and open the \ncloudbreak-deployment\n directory:\n\n\ncd cloudbreak-deployment\n\n\n\n\nThis is the directory of the config files and the supporting binaries that will be downloaded by Cloudbreak deployer.\n\n\nInitialize your Profile\n\n\nFirst initialize cbd by creating a \nProfile\n file:\n\n\ncbd init\n\n\n\n\nIt will create a \nProfile\n file in the current directory. Please edit the file - one of the required configurations is the \nPUBLIC_IP\n.\nThis IP will be used to access the Cloudbreak UI (called Uluwatu). In some cases the \ncbd\n tool tries to guess it, if can't than will give a hint.\n\n\nThe other required configuration in the \nProfile\n are the AWS keys belonging to the AWS account used by the Cloudbreak application.\nIn order for Cloudbreak to be able to launch clusters on AWS on your behalf you need to set your AWS keys in the \nProfile\n file.\nWe suggest to use the keys of an \nIAM User\n here. The IAM User's policies must be configured to have permission to assume roles (\nsts:AssumeRole\n) on all (\n*\n) resources.\n\n\nexport AWS_ACCESS_KEY_ID=AKIA**************W7SA\nexport AWS_SECRET_ACCESS_KEY=RWCT4Cs8******************/*skiOkWD\n\n\n\n\nYou can learn more about the concepts used by Cloudbreak with AWS accounts in the \nprerequisites chapter\n \n\n\nGenerate your Profile\n\n\nYou are done with the initialization of Cloudbreak deployer. The last thing you have to do is to regenerate the configurations in order to take effect.\n\n\nrm *.yml\ncbd generate\n\n\n\n\nThis command applies the following steps: \n\n\n\n\ncreates the \ndocker-compose.yml\n file that describes the configuration of all the Docker containers needed for the Cloudbreak deployment.\n\n\ncreates the \nuaa.yml\n file that holds the configuration of the identity server used to authenticate users to Cloudbreak.\n\n\n\n\nStart Cloudbreak\n\n\nTo start the Cloudbreak application use the following command.\nThis will start all the Docker containers and initialize the application. It will take a few minutes until all the services start.\n\n\ncbd start\n\n\n\n\n\n\nLaunching it first will take more time as it downloads all the docker images needed by Cloudbreak.\n\n\n\n\nAfter the \ncbd start\n command finishes you can check the logs of the Cloudbreak server with this command:\n\n\ncbd logs cloudbreak\n\n\n\n\n\n\nCloudbreak server should start within a minute - you should see a line like this: \nStarted CloudbreakApplication in 36.823 seconds\n\n\n\n\nNext steps\n\n\nOnce Cloudbreak is up and running you should check out the \nProvisioning Prerequisites\n needed to create AWS \nclusters with Cloudbreak.", 
            "title": "Setup"
        }, 
        {
            "location": "/aws/#aws-setup", 
            "text": "", 
            "title": "AWS Setup"
        }, 
        {
            "location": "/aws/#setup-cloudbreak-deployer", 
            "text": "If you already have Cloudbreak Deployer either by  using the AWS Cloud Images  or by  installing the Cloudbreak Deployer  manually on your own VM,\nyou can start to setup the Cloudbreak Application with the deployer.  Create and open the  cloudbreak-deployment  directory:  cd cloudbreak-deployment  This is the directory of the config files and the supporting binaries that will be downloaded by Cloudbreak deployer.  Initialize your Profile  First initialize cbd by creating a  Profile  file:  cbd init  It will create a  Profile  file in the current directory. Please edit the file - one of the required configurations is the  PUBLIC_IP .\nThis IP will be used to access the Cloudbreak UI (called Uluwatu). In some cases the  cbd  tool tries to guess it, if can't than will give a hint.  The other required configuration in the  Profile  are the AWS keys belonging to the AWS account used by the Cloudbreak application.\nIn order for Cloudbreak to be able to launch clusters on AWS on your behalf you need to set your AWS keys in the  Profile  file.\nWe suggest to use the keys of an  IAM User  here. The IAM User's policies must be configured to have permission to assume roles ( sts:AssumeRole ) on all ( * ) resources.  export AWS_ACCESS_KEY_ID=AKIA**************W7SA\nexport AWS_SECRET_ACCESS_KEY=RWCT4Cs8******************/*skiOkWD  You can learn more about the concepts used by Cloudbreak with AWS accounts in the  prerequisites chapter    Generate your Profile  You are done with the initialization of Cloudbreak deployer. The last thing you have to do is to regenerate the configurations in order to take effect.  rm *.yml\ncbd generate  This command applies the following steps:    creates the  docker-compose.yml  file that describes the configuration of all the Docker containers needed for the Cloudbreak deployment.  creates the  uaa.yml  file that holds the configuration of the identity server used to authenticate users to Cloudbreak.   Start Cloudbreak  To start the Cloudbreak application use the following command.\nThis will start all the Docker containers and initialize the application. It will take a few minutes until all the services start.  cbd start   Launching it first will take more time as it downloads all the docker images needed by Cloudbreak.   After the  cbd start  command finishes you can check the logs of the Cloudbreak server with this command:  cbd logs cloudbreak   Cloudbreak server should start within a minute - you should see a line like this:  Started CloudbreakApplication in 36.823 seconds   Next steps  Once Cloudbreak is up and running you should check out the  Provisioning Prerequisites  needed to create AWS \nclusters with Cloudbreak.", 
            "title": "Setup Cloudbreak Deployer"
        }, 
        {
            "location": "/aws_pre_prov/", 
            "text": "Provisioning Prerequisites\n\n\nIAM role setup\n\n\nCloudbreak works by connecting your AWS account through so called \nCredentials\n, and then uses these credentials to create resources on your behalf.\n\n\n\n\nImportant\n Cloudbreak deployment uses two different AWS accounts for two different purposes:\n\n\n\n\n\n\nThe account belonging to the \nCloudbreak webapp\n itself that acts as a \nthird party\n that creates resources on the account of the \nend-user\n. This account is configured at server-deployment time.\n\n\nThe account belonging to the \nend user\n who uses the UI or the Shell to create clusters. This account is configured when setting up credentials.\n\n\n\n\nThese two accounts are usually \nthe same\n when the end user is the same who deployed the Cloudbreak server, but it allows Cloudbreak to act as a SaaS project as well if needed.\n\n\nCredentials use \nIAM Roles\n to give access to the third party to act on behalf of the end user without giving full access to your resources.\nThis IAM Role will be \nassumed\n later by the deployment account.\nThis section is about how to setup the IAM role, to see how to setup the \ndeployment\n account check out \nthis description\n.\n\n\nTo connect your (\nend user\n) AWS account with a credential in Cloudbreak you'll have to create an IAM role on your AWS account that is configured to allow the third-party account to access and create resources on your behalf.\nThe easiest way to do this is with cbd commands (but it can also be done manually from the \nAWS Console\n):\n\n\ncbd aws generate-role  - Generates an AWS IAM role for Cloudbreak provisioning on AWS\ncbd aws show-role      - Show assumers and policies for an AWS role\ncbd aws delete-role    - Deletes an AWS IAM role, removes all inline policies\n\n\n\n\nThe \ngenerate-role\n command creates a role that is assumable by the Cloudbreak Deployer AWS account and has a broad policy setup.\nBy default the \ngenerate-role\n command creates a role with the name \ncbreak-deployer\n.\nIf you'd like to create the role with a different name or if you'd like to create multiple roles then the role's name can be changed by adding this line to your \nProfile\n:\n\n\nexport AWS_ROLE_NAME=my-cloudbreak-role\n\n\n\n\nYou can check the generated role on your AWS console, under IAM roles:\n\n\n\n\nGenerate a new SSH key\n\n\nAll the instances created by Cloudbreak are configured to allow key-based SSH,\nso you'll need to provide an SSH public key that can be used later to SSH onto the instances in the clusters you'll create with Cloudbreak.\nYou can use one of your existing keys or you can generate a new one.\n\n\nTo generate a new SSH keypair:\n\n\nssh-keygen -t rsa -b 4096 -C \nyour_email@example.com\n\n# Creates a new ssh key, using the provided email as a label\n# Generating public/private rsa key pair.\n\n\n\n\n# Enter file in which to save the key (/Users/you/.ssh/id_rsa): [Press enter]\nYou'll be asked to enter a passphrase, but you can leave it empty.\n\n# Enter passphrase (empty for no passphrase): [Type a passphrase]\n# Enter same passphrase again: [Type passphrase again]\n\n\n\n\nAfter you enter a passphrase the keypair is generated. The output should look something like below.\n\n\n# Your identification has been saved in /Users/you/.ssh/id_rsa.\n# Your public key has been saved in /Users/you/.ssh/id_rsa.pub.\n# The key fingerprint is:\n# 01:0f:f4:3b:ca:85:sd:17:sd:7d:sd:68:9d:sd:a2:sd your_email@example.com\n\n\n\n\nLater you'll need to pass the \n.pub\n file's contents to Cloudbreak and use the private part to SSH to the instances.\n\n\nNext steps\n\n\nAfter your IAM role is configured and you have an SSH key you can move on to create clusters on the \nUI\n or with the \nShell\n.", 
            "title": "Provisioning Prerequisites"
        }, 
        {
            "location": "/aws_pre_prov/#provisioning-prerequisites", 
            "text": "", 
            "title": "Provisioning Prerequisites"
        }, 
        {
            "location": "/aws_pre_prov/#iam-role-setup", 
            "text": "Cloudbreak works by connecting your AWS account through so called  Credentials , and then uses these credentials to create resources on your behalf.   Important  Cloudbreak deployment uses two different AWS accounts for two different purposes:    The account belonging to the  Cloudbreak webapp  itself that acts as a  third party  that creates resources on the account of the  end-user . This account is configured at server-deployment time.  The account belonging to the  end user  who uses the UI or the Shell to create clusters. This account is configured when setting up credentials.   These two accounts are usually  the same  when the end user is the same who deployed the Cloudbreak server, but it allows Cloudbreak to act as a SaaS project as well if needed.  Credentials use  IAM Roles  to give access to the third party to act on behalf of the end user without giving full access to your resources.\nThis IAM Role will be  assumed  later by the deployment account.\nThis section is about how to setup the IAM role, to see how to setup the  deployment  account check out  this description .  To connect your ( end user ) AWS account with a credential in Cloudbreak you'll have to create an IAM role on your AWS account that is configured to allow the third-party account to access and create resources on your behalf.\nThe easiest way to do this is with cbd commands (but it can also be done manually from the  AWS Console ):  cbd aws generate-role  - Generates an AWS IAM role for Cloudbreak provisioning on AWS\ncbd aws show-role      - Show assumers and policies for an AWS role\ncbd aws delete-role    - Deletes an AWS IAM role, removes all inline policies  The  generate-role  command creates a role that is assumable by the Cloudbreak Deployer AWS account and has a broad policy setup.\nBy default the  generate-role  command creates a role with the name  cbreak-deployer .\nIf you'd like to create the role with a different name or if you'd like to create multiple roles then the role's name can be changed by adding this line to your  Profile :  export AWS_ROLE_NAME=my-cloudbreak-role  You can check the generated role on your AWS console, under IAM roles:", 
            "title": "IAM role setup"
        }, 
        {
            "location": "/aws_pre_prov/#generate-a-new-ssh-key", 
            "text": "All the instances created by Cloudbreak are configured to allow key-based SSH,\nso you'll need to provide an SSH public key that can be used later to SSH onto the instances in the clusters you'll create with Cloudbreak.\nYou can use one of your existing keys or you can generate a new one.  To generate a new SSH keypair:  ssh-keygen -t rsa -b 4096 -C  your_email@example.com \n# Creates a new ssh key, using the provided email as a label\n# Generating public/private rsa key pair.  # Enter file in which to save the key (/Users/you/.ssh/id_rsa): [Press enter]\nYou'll be asked to enter a passphrase, but you can leave it empty.\n\n# Enter passphrase (empty for no passphrase): [Type a passphrase]\n# Enter same passphrase again: [Type passphrase again]  After you enter a passphrase the keypair is generated. The output should look something like below.  # Your identification has been saved in /Users/you/.ssh/id_rsa.\n# Your public key has been saved in /Users/you/.ssh/id_rsa.pub.\n# The key fingerprint is:\n# 01:0f:f4:3b:ca:85:sd:17:sd:7d:sd:68:9d:sd:a2:sd your_email@example.com  Later you'll need to pass the  .pub  file's contents to Cloudbreak and use the private part to SSH to the instances.", 
            "title": "Generate a new SSH key"
        }, 
        {
            "location": "/aws_pre_prov/#next-steps", 
            "text": "After your IAM role is configured and you have an SSH key you can move on to create clusters on the  UI  or with the  Shell .", 
            "title": "Next steps"
        }, 
        {
            "location": "/aws_cb_ui/", 
            "text": "Provisioning via Browser\n\n\nYou can log into the Cloudbreak application at http://PUBLIC_IP:3000.\n\n\nThe main goal of the Cloudbreak UI is to easily create clusters on your own cloud provider account.\nThis description details the AWS setup - if you'd like to use a different cloud provider check out its manual.\n\n\nThis document explains the four steps that need to be followed to create Cloudbreak clusters from the UI:\n\n\n\n\nconnect your AWS account with Cloudbreak\n\n\ncreate some template resources on the UI that describe the infrastructure of your clusters\n\n\ncreate a blueprint that describes the HDP services in your clusters and add some recipes for customization\n\n\nlaunch the cluster itself based on these template resources\n\n\n\n\nSetting up AWS credentials\n\n\nCloudbreak works by connecting your AWS account through so called \nCredentials\n, and then uses these credentials to create resources on your behalf.\nThe credentials can be configured on the \"manage credentials\".\n\n\nAdd a \nname\n and a \ndescription\n for the credential, copy your IAM role's Amazon Resource Name (ARN) to the corresponding field (\nIAM Role ARN\n) and copy your SSH public key to the \nSSH public key\n field.\nTo learn more about how to setup the IAM Role on your AWS account check out the \nprerequisites\n.\n\n\nThe SSH public key must be in OpenSSH format and it's private keypair can be used later to \nSSH onto every instance\n of every cluster you'll create with this credential.\nThe SSH username for the EC2 instances is \nec2-user\n.\n\n\nThere is a last option called \nPublic in account\n - it means that all the users belonging to your account will be able to use this credential to create clusters, but cannot delete or modify it.\n\n\n\n\nInfrastructure templates\n\n\nAfter your AWS account is linked to Cloudbreak you can start creating templates that describe your clusters' infrastructure:\n\n\n\n\nresources\n\n\nnetworks\n\n\nsecurity groups\n\n\n\n\nWhen you create a template, Cloudbreak \ndoesn't make any requests\n to AWS.\nResources are only created on AWS after the \ncreate cluster\n button is pushed.\nThese templates are saved to Cloudbreak's database and can be reused with multiple clusters to describe the infrastructure.\n\n\nResources\n\n\nResources describe the instances of your cluster - the instance type and the attached volumes.\nA typical setup is to combine multiple resources in a cluster for the different types of nodes.\nFor example you may want to attach multiple large disks to the datanodes or have memory optimized instances for Spark nodes.\n\n\nThere are some additional configuration options here:\n\n\n\n\nSpot price (USD)\n is not mandatory, if specified Cloudbreak will request spot price instances (which might take a while or never be fulfilled by Amazon). This option is \nnot supported\n by the default RedHat images.\n\n\nEBS encryption\n is supported for all volume types. If this option is checked then all the attached disks \nwill be encrypted\n by Amazon using the AWS KMS master keys.\n\n\nIf \nPublic in account\n is checked all the users belonging to your account will be able to use this resource to create clusters, but cannot delete or modify it.\n\n\n\n\n\n\nNetworks\n\n\nYour clusters can be created in their own Virtual Private Cloud (VPC) or in one of your already existing VPCs.\nCurrently Cloudbreak creates a new subnet in both cases, in a later release it may change.\nThe subnet's IP range must be defined in the \nSubnet (CIDR)\n field using the general CIDR notation.\n\n\nIf you don't want to use your already existing VPC, you can use the default network (\ndefault-aws-network\n) for all your clusters.\nIt will create a new VPC with a \n10.0.0.0/16\n subnet every time a cluster is created.\n\n\nIf you'd like to deploy a cluster to an already existing VPC you'll have to create a new network template where you configure the identifier of your VPC and the internet gateway (IGW) that's attached to the VPC.\nIn this case you'll have to create a different network template for every one of your clusters because the Subnet CIDR cannot overlap an already existing subnet in the VPC.\nFor example you can create 3 different clusters with 3 different network templates for the subnets \n10.0.0.0/24\n, \n10.0.1.0/24\n, \n10.0.2.0/24\n but with the same VPC and IGW identifiers.\n\n\n\n\nImportant\n Please make sure that the subnet you define here doesn't overlap with any of your already deployed \nsubnets in the VPC because the validation only happens after the cluster creation starts.\n\n\n\n\nIf \nPublic in account\n is checked all the users belonging to your account will be able to use this network template to create clusters, but cannot delete or modify it.\n\n\n\n\nNote\n that the VPCs, IGWs and/or subnets are \nnot created\n on AWS after the \nCreate Network\n button is pushed, \nonly after the cluster provisioning starts with the selected network template.\n\n\n\n\n\n\nSecurity groups\n\n\nSecurity group templates are very similar to the security groups on the AWS Console.\nThey describe the allowed inbound traffic to the instances in the cluster.\nCurrently only one security group template can be selected for a Cloudbreak cluster and all the instances have a public IP address so all the instances in the cluster will belong to the same security group.\nThis may change in a later release.\n\n\nYou can define your own security group by adding all the ports, protocols and CIDR range you'd like to use. 443 needs to be there in every security group otherwise Cloudbreak won't be able to communicate with the provisioned cluster. The rules defined here doesn't need to contain the internal rules, those are automatically added by Cloudbreak to the security group on AWS.\n\n\nYou can also use the two pre-defined security groups in Cloudbreak:\n\n\nonly-ssh-and-ssl:\n all ports are locked down except for SSH and gateway HTTPS (you can't access Hadoop services outside of the VPC):\n\n\n\n\nSSH (22)\n\n\nHTTPS (443)\n\n\n\n\nall-services-port:\n all Hadoop services and SSH/gateway HTTPS are accessible by default:\n\n\n\n\nSSH (22)\n\n\nHTTPS (443)\n\n\nAmbari (8080)\n\n\nConsul (8500)\n\n\nNN (50070)\n\n\nRM Web (8088)\n\n\nScheduler (8030RM)\n\n\nIPC (8050RM)\n\n\nJob history server (19888)\n\n\nHBase master (60000)\n\n\nHBase master web (60010)\n\n\nHBase RS (16020)\n\n\nHBase RS info (60030)\n\n\nFalcon (15000)\n\n\nStorm (8744)\n\n\nHive metastore (9083)\n\n\nHive server (10000)\n\n\nHive server HTTP (10001)\n\n\nAccumulo master (9999)\n\n\nAccumulo Tserver (9997)\n\n\nAtlas (21000)\n\n\nKNOX (8443)\n\n\nOozie (11000)\n\n\nSpark HS (18080)\n\n\nNM Web (8042)\n\n\nZeppelin WebSocket (9996)\n\n\nZeppelin UI (9995)\n\n\nKibana (3080)\n\n\nElasticsearch (9200)\n\n\n\n\nIf \nPublic in account\n is checked all the users belonging to your account will be able to use this security group template to create clusters, but cannot delete or modify it.\n\n\n\n\nNote\n that the security groups are \nnot created\n on AWS after the \nCreate Security Group\n button is pushed, only \nafter the cluster provisioning starts with the selected security group template.\n\n\n\n\n\n\nDefining cluster services\n\n\nBlueprints\n\n\nBlueprints are your declarative definition of a Hadoop cluster. These are the same blueprints that are \nused by Ambari\n.\n\n\nYou can use the 3 default blueprints pre-defined in Cloudbreak or you can create your own.\nBlueprints can be added from an URL (an example \nblueprint\n) or the whole JSON can be copied to the \nManual copy\n field.\n\n\nThe hostgroups added in the JSON will be mapped to a set of instances when starting the cluster and the services and components defined in the hostgroup will be installed on the corresponding nodes.\nIt is not necessary to define all the configuration fields in the blueprints - if a configuration is missing, Ambari will fill that with a default value.\nThe configurations defined in the blueprint can also be modified later from the Ambari UI.\n\n\nIf \nPublic in account\n is checked all the users belonging to your account will be able to use this blueprint to create clusters, but cannot delete or modify it.\n\n\n\n\nA blueprint can be exported from a running Ambari cluster that can be reused in Cloudbreak with slight modifications.\nThere is no automatic way to modify an exported blueprint and make it instantly usable in Cloudbreak, the modifications have to be done manually.\nWhen the blueprint is exported some configurations will have hardcoded for example domain names, or memory configurations that won't be applicable to the Cloudbreak cluster.\n\n\nCluster customization\n\n\nSometimes it can be useful to define some custom scripts that run during cluster creation and add some additional functionality.\nFor example it can be a service you'd like to install but it's not supported by Ambari or some script that automatically downloads some data to the necessary nodes.\nThe most notable example is Ranger setup: it has a prerequisite of a running database when Ranger Admin is installing.\nA PostgreSQL database can be easily started and configured with a recipe before the blueprint installation starts.\n\n\nTo learn more about these so called \nRecipes\n, and to check out the Ranger database recipe, take a look at the \nCluster customization\n part of the documentation.\n\n\nCluster deployment\n\n\nAfter all the templates are configured you can deploy a new HDP cluster. Start by selecting a previously created AWS credential in the header.\nClick on \ncreate cluster\n, give it a \nname\n, select a \nRegion\n where the cluster infrastructure will be provisioned and select one of the \nNetworks\n and \nSecurity Groups\n created earlier.\nAfter you've selected a \nBlueprint\n as well you should be able to configure the \nTemplate resources\n and the number of nodes for all of the hostgroups in the blueprint.\n\n\nIf \nPublic in account\n is checked all the users belonging to your account will be able to see the newly created cluster on the UI, but cannot delete or modify it.\n\n\nIf \nEnable security\n is checked as well, Cloudbreak will install Key Distribution Center (KDC) and the cluster will be Kerberized. See more about it in the \nKerberos\n section of this documentation.\n\n\nAfter the \ncreate and start cluster\n button is pushed Cloudbreak will start to create resources on your AWS account.\nCloudbreak uses \nCloudFormation\n to create the resources - you can check out the resources created by Cloudbreak on the AWS Console under the CloudFormation page.\n\n\n\n\nImportant\n Always use Cloudbreak to delete the cluster. If that fails for some reason always try to delete the \nCloudFormation stack first.\nInstances are started in an Auto Scaling Group so they may be restarted if you terminate an instance manually!\n\n\n\n\nAdvanced options\n\n\nThere are some advanced features when deploying a new cluster, these are the following:\n\n\nAvailability Zone\n: You can restrict the instances to a specific availability zone. It may be useful if you're using reserved instances.\n\n\nUse dedicated instances:\n You can use \ndedicated instances\n on EC2\n\n\nMinimum cluster size:\n the provisioning strategy in case of the cloud provider cannot allocate all the requested nodes\n\n\nValidate blueprint:\n feature to validate the Ambari blueprint. By default is switched on.\n\n\nNext steps\n\n\nCongrats! Your cluster should now be up and running. To learn more about it we have some \ninteresting insights\n about Cloudbreak clusters.", 
            "title": "Provisioning - UI"
        }, 
        {
            "location": "/aws_cb_ui/#provisioning-via-browser", 
            "text": "You can log into the Cloudbreak application at http://PUBLIC_IP:3000.  The main goal of the Cloudbreak UI is to easily create clusters on your own cloud provider account.\nThis description details the AWS setup - if you'd like to use a different cloud provider check out its manual.  This document explains the four steps that need to be followed to create Cloudbreak clusters from the UI:   connect your AWS account with Cloudbreak  create some template resources on the UI that describe the infrastructure of your clusters  create a blueprint that describes the HDP services in your clusters and add some recipes for customization  launch the cluster itself based on these template resources", 
            "title": "Provisioning via Browser"
        }, 
        {
            "location": "/aws_cb_ui/#setting-up-aws-credentials", 
            "text": "Cloudbreak works by connecting your AWS account through so called  Credentials , and then uses these credentials to create resources on your behalf.\nThe credentials can be configured on the \"manage credentials\".  Add a  name  and a  description  for the credential, copy your IAM role's Amazon Resource Name (ARN) to the corresponding field ( IAM Role ARN ) and copy your SSH public key to the  SSH public key  field.\nTo learn more about how to setup the IAM Role on your AWS account check out the  prerequisites .  The SSH public key must be in OpenSSH format and it's private keypair can be used later to  SSH onto every instance  of every cluster you'll create with this credential.\nThe SSH username for the EC2 instances is  ec2-user .  There is a last option called  Public in account  - it means that all the users belonging to your account will be able to use this credential to create clusters, but cannot delete or modify it.", 
            "title": "Setting up AWS credentials"
        }, 
        {
            "location": "/aws_cb_ui/#infrastructure-templates", 
            "text": "After your AWS account is linked to Cloudbreak you can start creating templates that describe your clusters' infrastructure:   resources  networks  security groups   When you create a template, Cloudbreak  doesn't make any requests  to AWS.\nResources are only created on AWS after the  create cluster  button is pushed.\nThese templates are saved to Cloudbreak's database and can be reused with multiple clusters to describe the infrastructure.  Resources  Resources describe the instances of your cluster - the instance type and the attached volumes.\nA typical setup is to combine multiple resources in a cluster for the different types of nodes.\nFor example you may want to attach multiple large disks to the datanodes or have memory optimized instances for Spark nodes.  There are some additional configuration options here:   Spot price (USD)  is not mandatory, if specified Cloudbreak will request spot price instances (which might take a while or never be fulfilled by Amazon). This option is  not supported  by the default RedHat images.  EBS encryption  is supported for all volume types. If this option is checked then all the attached disks  will be encrypted  by Amazon using the AWS KMS master keys.  If  Public in account  is checked all the users belonging to your account will be able to use this resource to create clusters, but cannot delete or modify it.    Networks  Your clusters can be created in their own Virtual Private Cloud (VPC) or in one of your already existing VPCs.\nCurrently Cloudbreak creates a new subnet in both cases, in a later release it may change.\nThe subnet's IP range must be defined in the  Subnet (CIDR)  field using the general CIDR notation.  If you don't want to use your already existing VPC, you can use the default network ( default-aws-network ) for all your clusters.\nIt will create a new VPC with a  10.0.0.0/16  subnet every time a cluster is created.  If you'd like to deploy a cluster to an already existing VPC you'll have to create a new network template where you configure the identifier of your VPC and the internet gateway (IGW) that's attached to the VPC.\nIn this case you'll have to create a different network template for every one of your clusters because the Subnet CIDR cannot overlap an already existing subnet in the VPC.\nFor example you can create 3 different clusters with 3 different network templates for the subnets  10.0.0.0/24 ,  10.0.1.0/24 ,  10.0.2.0/24  but with the same VPC and IGW identifiers.   Important  Please make sure that the subnet you define here doesn't overlap with any of your already deployed \nsubnets in the VPC because the validation only happens after the cluster creation starts.   If  Public in account  is checked all the users belonging to your account will be able to use this network template to create clusters, but cannot delete or modify it.   Note  that the VPCs, IGWs and/or subnets are  not created  on AWS after the  Create Network  button is pushed, \nonly after the cluster provisioning starts with the selected network template.    Security groups  Security group templates are very similar to the security groups on the AWS Console.\nThey describe the allowed inbound traffic to the instances in the cluster.\nCurrently only one security group template can be selected for a Cloudbreak cluster and all the instances have a public IP address so all the instances in the cluster will belong to the same security group.\nThis may change in a later release.  You can define your own security group by adding all the ports, protocols and CIDR range you'd like to use. 443 needs to be there in every security group otherwise Cloudbreak won't be able to communicate with the provisioned cluster. The rules defined here doesn't need to contain the internal rules, those are automatically added by Cloudbreak to the security group on AWS.  You can also use the two pre-defined security groups in Cloudbreak:  only-ssh-and-ssl:  all ports are locked down except for SSH and gateway HTTPS (you can't access Hadoop services outside of the VPC):   SSH (22)  HTTPS (443)   all-services-port:  all Hadoop services and SSH/gateway HTTPS are accessible by default:   SSH (22)  HTTPS (443)  Ambari (8080)  Consul (8500)  NN (50070)  RM Web (8088)  Scheduler (8030RM)  IPC (8050RM)  Job history server (19888)  HBase master (60000)  HBase master web (60010)  HBase RS (16020)  HBase RS info (60030)  Falcon (15000)  Storm (8744)  Hive metastore (9083)  Hive server (10000)  Hive server HTTP (10001)  Accumulo master (9999)  Accumulo Tserver (9997)  Atlas (21000)  KNOX (8443)  Oozie (11000)  Spark HS (18080)  NM Web (8042)  Zeppelin WebSocket (9996)  Zeppelin UI (9995)  Kibana (3080)  Elasticsearch (9200)   If  Public in account  is checked all the users belonging to your account will be able to use this security group template to create clusters, but cannot delete or modify it.   Note  that the security groups are  not created  on AWS after the  Create Security Group  button is pushed, only \nafter the cluster provisioning starts with the selected security group template.", 
            "title": "Infrastructure templates"
        }, 
        {
            "location": "/aws_cb_ui/#defining-cluster-services", 
            "text": "Blueprints  Blueprints are your declarative definition of a Hadoop cluster. These are the same blueprints that are  used by Ambari .  You can use the 3 default blueprints pre-defined in Cloudbreak or you can create your own.\nBlueprints can be added from an URL (an example  blueprint ) or the whole JSON can be copied to the  Manual copy  field.  The hostgroups added in the JSON will be mapped to a set of instances when starting the cluster and the services and components defined in the hostgroup will be installed on the corresponding nodes.\nIt is not necessary to define all the configuration fields in the blueprints - if a configuration is missing, Ambari will fill that with a default value.\nThe configurations defined in the blueprint can also be modified later from the Ambari UI.  If  Public in account  is checked all the users belonging to your account will be able to use this blueprint to create clusters, but cannot delete or modify it.   A blueprint can be exported from a running Ambari cluster that can be reused in Cloudbreak with slight modifications.\nThere is no automatic way to modify an exported blueprint and make it instantly usable in Cloudbreak, the modifications have to be done manually.\nWhen the blueprint is exported some configurations will have hardcoded for example domain names, or memory configurations that won't be applicable to the Cloudbreak cluster.  Cluster customization  Sometimes it can be useful to define some custom scripts that run during cluster creation and add some additional functionality.\nFor example it can be a service you'd like to install but it's not supported by Ambari or some script that automatically downloads some data to the necessary nodes.\nThe most notable example is Ranger setup: it has a prerequisite of a running database when Ranger Admin is installing.\nA PostgreSQL database can be easily started and configured with a recipe before the blueprint installation starts.  To learn more about these so called  Recipes , and to check out the Ranger database recipe, take a look at the  Cluster customization  part of the documentation.", 
            "title": "Defining cluster services"
        }, 
        {
            "location": "/aws_cb_ui/#cluster-deployment", 
            "text": "After all the templates are configured you can deploy a new HDP cluster. Start by selecting a previously created AWS credential in the header.\nClick on  create cluster , give it a  name , select a  Region  where the cluster infrastructure will be provisioned and select one of the  Networks  and  Security Groups  created earlier.\nAfter you've selected a  Blueprint  as well you should be able to configure the  Template resources  and the number of nodes for all of the hostgroups in the blueprint.  If  Public in account  is checked all the users belonging to your account will be able to see the newly created cluster on the UI, but cannot delete or modify it.  If  Enable security  is checked as well, Cloudbreak will install Key Distribution Center (KDC) and the cluster will be Kerberized. See more about it in the  Kerberos  section of this documentation.  After the  create and start cluster  button is pushed Cloudbreak will start to create resources on your AWS account.\nCloudbreak uses  CloudFormation  to create the resources - you can check out the resources created by Cloudbreak on the AWS Console under the CloudFormation page.   Important  Always use Cloudbreak to delete the cluster. If that fails for some reason always try to delete the \nCloudFormation stack first.\nInstances are started in an Auto Scaling Group so they may be restarted if you terminate an instance manually!   Advanced options  There are some advanced features when deploying a new cluster, these are the following:  Availability Zone : You can restrict the instances to a specific availability zone. It may be useful if you're using reserved instances.  Use dedicated instances:  You can use  dedicated instances  on EC2  Minimum cluster size:  the provisioning strategy in case of the cloud provider cannot allocate all the requested nodes  Validate blueprint:  feature to validate the Ambari blueprint. By default is switched on.", 
            "title": "Cluster deployment"
        }, 
        {
            "location": "/aws_cb_ui/#next-steps", 
            "text": "Congrats! Your cluster should now be up and running. To learn more about it we have some  interesting insights  about Cloudbreak clusters.", 
            "title": "Next steps"
        }, 
        {
            "location": "/aws_cb_shell/", 
            "text": "Interactive mode\n\n\nStart the shell with \ncbd util cloudbreak-shell\n. This will launch the Cloudbreak shell inside a Docker container and you are ready to start using it.\n\n\nYou have to copy files into the cbd working directory, which you would like to use from shell. For example if your \ncbd\n working directory is \n~/prj/cbd\n then copy your blueprint and public ssh key file into this directory. You can refer to these files with their names from the shell.\n\n\nCreate a cloud credential\n\n\nIn order to start using Cloudbreak you will need to have an AWS cloud credential configured.\n\n\n\n\nNote\n that Cloudbreak \ndoes not\n store your cloud user details - we work around the concept of \nIAM\n - on Amazon (or other cloud providers) you will have to create an IAM role, a policy and associate that with your Cloudbreak account.\n\n\n\n\ncredential create --EC2 --description \ndescription\n --name my-aws-credential --roleArn \narn role\n --sshKeyPath \npath of your AWS public key\n\n\n\n\n\nAlternatively you can upload your public key from an url as well, by using the \n\u2014sshKeyUrl\n switch. You can check whether the credential was created successfully by using the \ncredential list\n command. You can switch between your cloud credentials - when you\u2019d like to use one and act with that you will have to use:\n\n\ncredential select --name my-aws-credential\n\n\n\n\nCreate a template\n\n\nA template gives developers and systems administrators an easy way to create and manage a collection of cloud infrastructure related resources, maintaining and updating them in an orderly and predictable fashion. A template can be used repeatedly to create identical copies of the same stack (or to use as a foundation to start a new stack).\n\n\ntemplate create --EC2 --name awstemplate --description aws-template --instanceType M3Xlarge --volumeSize 100 --volumeCount 2\n\n\n\n\nYou can check whether the template was created successfully by using the \ntemplate list\n or \ntemplate show\n command.\n\n\nYou can delete your cloud template - when you\u2019d like to delete one you will have to use:\n\n\ntemplate delete --name awstemplate\n\n\n\n\nCreate or select a blueprint\n\n\nYou can define Ambari blueprints with cloudbreak-shell:\n\n\nblueprint add --name myblueprint --description myblueprint-description --file \nthe path of the blueprint\n\n\n\n\n\nOther available options:\n\n\n--url\n the url of the blueprint\n\n\n--publicInAccount\n flags if the network is public in the account\n\n\nWe ship default Ambari blueprints with Cloudbreak. You can use these blueprints or add yours. To see the available blueprints and use one of them please use:\n\n\nblueprint list\n\nblueprint select --name hdp-small-default\n\n\n\n\nCreate a network\n\n\nA network gives developers and systems administrators an easy way to create and manage a collection of cloud infrastructure related networking, maintaining and updating them in an orderly and predictable fashion. A network can be used repeatedly to create identical copies of the same stack (or to use as a foundation to start a new stack).\n\n\nnetwork create --AWS --name awsnetwork --description aws-network --subnet 10.0.0.0/16\n\n\n\n\nOther available options:\n\n\n--vpcID\n your existing vpc on amazon\n\n\n--internetGatewayID\n your amazon internet gateway of the given VPC\n\n\n--publicInAccount\n flags if the network is public in the account\n\n\nThere is a default network with name \ndefault-aws-network\n. If we use this for cluster creation, Cloudbreak will create a new VPC with 10.0.0.0/16 subnet.\n\n\nYou can check whether the network was created successfully by using the \nnetwork list\n command. Check the network and select it if you are happy with it:\n\n\nnetwork show --name awsnetwork\n\nnetwork select --name awsnetwork\n\n\n\n\nCreate a security group\n\n\nA security group gives developers and systems administrators an easy way to create and manage a collection of cloud infrastructure related security rules.\n\n\nsecuritygroup create --name secgroup_example --description securitygroup-example --rules 0.0.0.0/0:tcp:8080,9090;10.0.33.0/24:tcp:1234,1235\n\n\n\n\nYou can check whether the security group was created successfully by using the \nsecuritygroup list\n command. Check the security group and select it if you are happy with it:\n\n\nsecuritygroup show --name secgroup_example\n\nsecuritygroup select --name secgroup_example\n\n\n\n\nThere are two default security groups defined: \nall-services-port\n and \nonly-ssh-and-ssl\n\n\nonly-ssh-and-ssl:\n all ports are locked down except for SSH and gateway HTTPS (you can't access Hadoop services outside of the VPC)\n\n\n\n\nSSH (22)\n\n\nHTTPS (443)\n\n\n\n\nall-services-port:\n all Hadoop services and SSH/gateway HTTPS are accessible by default:\n\n\n\n\nSSH (22)\n\n\nHTTPS (443)\n\n\nAmbari (8080)\n\n\nConsul (8500)\n\n\nNN (50070)\n\n\nRM Web (8088)\n\n\nScheduler (8030RM)\n\n\nIPC (8050RM)\n\n\nJob history server (19888)\n\n\nHBase master (60000)\n\n\nHBase master web (60010)\n\n\nHBase RS (16020)\n\n\nHBase RS info (60030)\n\n\nFalcon (15000)\n\n\nStorm (8744)\n\n\nHive metastore (9083)\n\n\nHive server (10000)\n\n\nHive server HTTP (10001)\n\n\nAccumulo master (9999)\n\n\nAccumulo Tserver (9997)\n\n\nAtlas (21000)\n\n\nKNOX (8443)\n\n\nOozie (11000)\n\n\nSpark HS (18080)\n\n\nNM Web (8042)\n\n\nZeppelin WebSocket (9996)\n\n\nZeppelin UI (9995)\n\n\nKibana (3080)\n\n\nElasticsearch (9200)\n\n\n\n\nConfigure instance groups\n\n\nYou have to configure the instancegroups before the provisioning. An instancegroup is defining a group of your nodes with a specified template. Usually we create instancegroups for the hostgroups defined in the blueprints.\n\n\ninstancegroup configure --instanceGroup host_group_slave_1 --nodecount 3 --templateName minviable-aws\n\n\n\n\nOther available option:\n\n\n--templateId\n Id of the template\n\n\nCreate a Hadoop cluster\n\n\nYou are almost done - two more command and this will create your Hadoop cluster on your favorite cloud provider. Same as the API, or UI this will use your \ncredential\n, \ninstancegroups\n, \nnetwork\n, \nsecuritygroup\n, and by using CloudFormation will launch a cloud stack\n\n\nstack create --name my-first-stack --region US_EAST_1\n\n\n\n\nOnce the \nstack\n is up and running (cloud provisioning is done) it will use your selected \nblueprint\n and install your custom Hadoop cluster with the selected components and services.\n\n\ncluster create --description \nmy first cluster\n\n\n\n\n\nYou are done - you can check the progress through the Ambari UI. If you log back to Cloudbreak UI you can check the progress over there as well, and learn the IP address of Ambari.\n\n\nStop/Restart cluster and stack\n\n\nYou have the ability to \nstop your existing stack then its cluster\n if you want to suspend the work on it.\n\n\nSelect a stack for example with its name:\n\n\nstack select --name my-stack\n\n\n\n\nOther available option to define a stack is its \n--id\n (instead of the \n--name\n).\n\n\nApply the following commands to stop the previously selected stack:\n\n\ncluster stop\nstack stop\n\n\n\n\n\n\nImportant!\n The related cluster should be stopped before you can stop the stack.\n\n\n\n\nApply the following command to \nrestart the previously selected and stopped stack\n:\n\n\nstack start\n\n\n\n\nAfter the selected stack has restarted, you can \nrestart the related cluster as well\n:\n\n\ncluster start\n\n\n\n\nUpscale/Downscale cluster and stack\n\n\nYou can \nupscale your selected stack\n if you need more instances to your infrastructure:\n\n\nstack node --ADD --instanceGroup host_group_slave_1 --adjustment 6\n\n\n\n\nOther available options:\n\n\n--withClusterUpScale\n indicates cluster upscale after stack upscale\nor you can upscale the related cluster separately as well:\n\n\ncluster node --ADD --hostgroup host_group_slave_1 --adjustment 6\n\n\n\n\nApply the following command to \ndownscale the previously selected stack\n:\n\n\nstack node --REMOVE  --instanceGroup host_group_slave_1 --adjustment -2\n\n\n\n\nand the related cluster separately:\n\n\ncluster node --REMOVE  --hostgroup host_group_slave_1 --adjustment -2\n\n\n\n\nSilent mode\n\n\nWith Cloudbreak shell you can execute script files as well. A script file contains cloudbreak shell commands and can be executed with the \nscript\n cloudbreak shell command\n\n\nscript \nyour script file\n\n\n\n\n\nor with the \ncbd util cloudbreak-shell-quiet\n cbd command:\n\n\ncbd util cloudbreak-shell-quiet \n example.sh\n\n\n\n\nExample\n\n\nThe following example creates a hadoop cluster with \nhdp-small-default\n blueprint on M3Xlarge instances with 2X100G attached disks on \ndefault-aws-network\n network using \nall-services-port\n security group. You should copy your ssh public key file into your cbd working directory with name \nid_rsa.pub\n and change the \narn role\n part with your arn role.\n\n\ncredential create --EC2 --description description --name my-aws-credential --roleArn \narn role\n --sshKeyPath id_rsa.pub\ncredential select --name my-aws-credential\ntemplate create --EC2 --name awstemplate --description aws-template --instanceType M3Xlarge --volumeSize 100 --volumeCount 2\nblueprint select --name hdp-small-default\ninstancegroup configure --instanceGroup cbgateway --nodecount 1 --templateName awstemplate\ninstancegroup configure --instanceGroup host_group_master_1 --nodecount 1 --templateName awstemplate\ninstancegroup configure --instanceGroup host_group_master_2 --nodecount 1 --templateName awstemplate\ninstancegroup configure --instanceGroup host_group_master_3 --nodecount 1 --templateName awstemplate\ninstancegroup configure --instanceGroup host_group_client_1  --nodecount 1 --templateName awstemplate\ninstancegroup configure --instanceGroup host_group_slave_1 --nodecount 3 --templateName awstemplate\nnetwork select --name default-aws-network\nsecuritygroup select --name all-services-port\nstack create --name my-first-stack --region US_EAST_1\ncluster create --description \nMy first cluster\n\n\n\n\n\nNext steps\n\n\nCongrats! Your cluster should now be up and running. To learn more about it we have some \ninteresting insights\n about Cloudbreak clusters.", 
            "title": "Provisioning - CLI"
        }, 
        {
            "location": "/aws_cb_shell/#interactive-mode", 
            "text": "Start the shell with  cbd util cloudbreak-shell . This will launch the Cloudbreak shell inside a Docker container and you are ready to start using it.  You have to copy files into the cbd working directory, which you would like to use from shell. For example if your  cbd  working directory is  ~/prj/cbd  then copy your blueprint and public ssh key file into this directory. You can refer to these files with their names from the shell.  Create a cloud credential  In order to start using Cloudbreak you will need to have an AWS cloud credential configured.   Note  that Cloudbreak  does not  store your cloud user details - we work around the concept of  IAM  - on Amazon (or other cloud providers) you will have to create an IAM role, a policy and associate that with your Cloudbreak account.   credential create --EC2 --description  description  --name my-aws-credential --roleArn  arn role  --sshKeyPath  path of your AWS public key   Alternatively you can upload your public key from an url as well, by using the  \u2014sshKeyUrl  switch. You can check whether the credential was created successfully by using the  credential list  command. You can switch between your cloud credentials - when you\u2019d like to use one and act with that you will have to use:  credential select --name my-aws-credential  Create a template  A template gives developers and systems administrators an easy way to create and manage a collection of cloud infrastructure related resources, maintaining and updating them in an orderly and predictable fashion. A template can be used repeatedly to create identical copies of the same stack (or to use as a foundation to start a new stack).  template create --EC2 --name awstemplate --description aws-template --instanceType M3Xlarge --volumeSize 100 --volumeCount 2  You can check whether the template was created successfully by using the  template list  or  template show  command.  You can delete your cloud template - when you\u2019d like to delete one you will have to use:  template delete --name awstemplate  Create or select a blueprint  You can define Ambari blueprints with cloudbreak-shell:  blueprint add --name myblueprint --description myblueprint-description --file  the path of the blueprint   Other available options:  --url  the url of the blueprint  --publicInAccount  flags if the network is public in the account  We ship default Ambari blueprints with Cloudbreak. You can use these blueprints or add yours. To see the available blueprints and use one of them please use:  blueprint list\n\nblueprint select --name hdp-small-default  Create a network  A network gives developers and systems administrators an easy way to create and manage a collection of cloud infrastructure related networking, maintaining and updating them in an orderly and predictable fashion. A network can be used repeatedly to create identical copies of the same stack (or to use as a foundation to start a new stack).  network create --AWS --name awsnetwork --description aws-network --subnet 10.0.0.0/16  Other available options:  --vpcID  your existing vpc on amazon  --internetGatewayID  your amazon internet gateway of the given VPC  --publicInAccount  flags if the network is public in the account  There is a default network with name  default-aws-network . If we use this for cluster creation, Cloudbreak will create a new VPC with 10.0.0.0/16 subnet.  You can check whether the network was created successfully by using the  network list  command. Check the network and select it if you are happy with it:  network show --name awsnetwork\n\nnetwork select --name awsnetwork  Create a security group  A security group gives developers and systems administrators an easy way to create and manage a collection of cloud infrastructure related security rules.  securitygroup create --name secgroup_example --description securitygroup-example --rules 0.0.0.0/0:tcp:8080,9090;10.0.33.0/24:tcp:1234,1235  You can check whether the security group was created successfully by using the  securitygroup list  command. Check the security group and select it if you are happy with it:  securitygroup show --name secgroup_example\n\nsecuritygroup select --name secgroup_example  There are two default security groups defined:  all-services-port  and  only-ssh-and-ssl  only-ssh-and-ssl:  all ports are locked down except for SSH and gateway HTTPS (you can't access Hadoop services outside of the VPC)   SSH (22)  HTTPS (443)   all-services-port:  all Hadoop services and SSH/gateway HTTPS are accessible by default:   SSH (22)  HTTPS (443)  Ambari (8080)  Consul (8500)  NN (50070)  RM Web (8088)  Scheduler (8030RM)  IPC (8050RM)  Job history server (19888)  HBase master (60000)  HBase master web (60010)  HBase RS (16020)  HBase RS info (60030)  Falcon (15000)  Storm (8744)  Hive metastore (9083)  Hive server (10000)  Hive server HTTP (10001)  Accumulo master (9999)  Accumulo Tserver (9997)  Atlas (21000)  KNOX (8443)  Oozie (11000)  Spark HS (18080)  NM Web (8042)  Zeppelin WebSocket (9996)  Zeppelin UI (9995)  Kibana (3080)  Elasticsearch (9200)   Configure instance groups  You have to configure the instancegroups before the provisioning. An instancegroup is defining a group of your nodes with a specified template. Usually we create instancegroups for the hostgroups defined in the blueprints.  instancegroup configure --instanceGroup host_group_slave_1 --nodecount 3 --templateName minviable-aws  Other available option:  --templateId  Id of the template  Create a Hadoop cluster  You are almost done - two more command and this will create your Hadoop cluster on your favorite cloud provider. Same as the API, or UI this will use your  credential ,  instancegroups ,  network ,  securitygroup , and by using CloudFormation will launch a cloud stack  stack create --name my-first-stack --region US_EAST_1  Once the  stack  is up and running (cloud provisioning is done) it will use your selected  blueprint  and install your custom Hadoop cluster with the selected components and services.  cluster create --description  my first cluster   You are done - you can check the progress through the Ambari UI. If you log back to Cloudbreak UI you can check the progress over there as well, and learn the IP address of Ambari.  Stop/Restart cluster and stack  You have the ability to  stop your existing stack then its cluster  if you want to suspend the work on it.  Select a stack for example with its name:  stack select --name my-stack  Other available option to define a stack is its  --id  (instead of the  --name ).  Apply the following commands to stop the previously selected stack:  cluster stop\nstack stop   Important!  The related cluster should be stopped before you can stop the stack.   Apply the following command to  restart the previously selected and stopped stack :  stack start  After the selected stack has restarted, you can  restart the related cluster as well :  cluster start  Upscale/Downscale cluster and stack  You can  upscale your selected stack  if you need more instances to your infrastructure:  stack node --ADD --instanceGroup host_group_slave_1 --adjustment 6  Other available options:  --withClusterUpScale  indicates cluster upscale after stack upscale\nor you can upscale the related cluster separately as well:  cluster node --ADD --hostgroup host_group_slave_1 --adjustment 6  Apply the following command to  downscale the previously selected stack :  stack node --REMOVE  --instanceGroup host_group_slave_1 --adjustment -2  and the related cluster separately:  cluster node --REMOVE  --hostgroup host_group_slave_1 --adjustment -2", 
            "title": "Interactive mode"
        }, 
        {
            "location": "/aws_cb_shell/#silent-mode", 
            "text": "With Cloudbreak shell you can execute script files as well. A script file contains cloudbreak shell commands and can be executed with the  script  cloudbreak shell command  script  your script file   or with the  cbd util cloudbreak-shell-quiet  cbd command:  cbd util cloudbreak-shell-quiet   example.sh", 
            "title": "Silent mode"
        }, 
        {
            "location": "/aws_cb_shell/#example", 
            "text": "The following example creates a hadoop cluster with  hdp-small-default  blueprint on M3Xlarge instances with 2X100G attached disks on  default-aws-network  network using  all-services-port  security group. You should copy your ssh public key file into your cbd working directory with name  id_rsa.pub  and change the  arn role  part with your arn role.  credential create --EC2 --description description --name my-aws-credential --roleArn  arn role  --sshKeyPath id_rsa.pub\ncredential select --name my-aws-credential\ntemplate create --EC2 --name awstemplate --description aws-template --instanceType M3Xlarge --volumeSize 100 --volumeCount 2\nblueprint select --name hdp-small-default\ninstancegroup configure --instanceGroup cbgateway --nodecount 1 --templateName awstemplate\ninstancegroup configure --instanceGroup host_group_master_1 --nodecount 1 --templateName awstemplate\ninstancegroup configure --instanceGroup host_group_master_2 --nodecount 1 --templateName awstemplate\ninstancegroup configure --instanceGroup host_group_master_3 --nodecount 1 --templateName awstemplate\ninstancegroup configure --instanceGroup host_group_client_1  --nodecount 1 --templateName awstemplate\ninstancegroup configure --instanceGroup host_group_slave_1 --nodecount 3 --templateName awstemplate\nnetwork select --name default-aws-network\nsecuritygroup select --name all-services-port\nstack create --name my-first-stack --region US_EAST_1\ncluster create --description  My first cluster", 
            "title": "Example"
        }, 
        {
            "location": "/aws_cb_shell/#next-steps", 
            "text": "Congrats! Your cluster should now be up and running. To learn more about it we have some  interesting insights  about Cloudbreak clusters.", 
            "title": "Next steps"
        }, 
        {
            "location": "/azure/", 
            "text": "Azure Setup\n\n\nSetup Cloudbreak Deployer\n\n\nTo install and configure the Cloudbreak Deployer on Azure, start\nan \nOpenLogic 7.1\n VM on Azure.\n\n\nMake sure you opened the following ports:\n\n\n\n\nSSH (22)\n\n\nAmbari (8080)\n\n\nIdentity server (8089)\n\n\nCloudbreak GUI (3000)\n\n\nUser authentication (3001)\n\n\n\n\nPlease log in to the machine with SSH or use username and password authentication (the following example shows how to ssh into the machine):\n\n\nssh -i \nazure-ssh-pem-file\n \nusername\n@\nvirtual-machine-ip\n\n\n\n\n\nAssume \nroot\n privileges with this command:\n\n\nsudo su\n\n\n\n\nConfigure the correct yum repository on the machine:\n\n\ncat \n /etc/yum.repos.d/CentOS-Base.repo \nEOF\n\n# CentOS-Base.repo\n#\n# The mirror system uses the connecting IP address of the client and the\n# update status of each mirror to pick mirrors that are updated to and\n# geographically close to the client.  You should use this for CentOS updates\n# unless you are manually picking other mirrors.\n#\n# If the mirrorlist= does not work for you, as a fall back you can try the\n# remarked out baseurl= line instead.\n#\n#\n\n[base]\nname=CentOS-$releasever - Base\nmirrorlist=http://mirrorlist.centos.org/?release=$releasever\narch=$basearch\nrepo=os\ninfra=$infra\n#baseurl=http://mirror.centos.org/centos/$releasever/os/$basearch/\ngpgcheck=1\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7\n\n#released updates\n[updates]\nname=CentOS-$releasever - Updates\nmirrorlist=http://mirrorlist.centos.org/?release=$releasever\narch=$basearch\nrepo=updates\ninfra=$infra\n#baseurl=http://mirror.centos.org/centos/$releasever/updates/$basearch/\ngpgcheck=1\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7\n\n#additional packages that may be useful\n[extras]\nname=CentOS-$releasever - Extras\nmirrorlist=http://mirrorlist.centos.org/?release=$releasever\narch=$basearch\nrepo=extras\ninfra=$infra\n#baseurl=http://mirror.centos.org/centos/$releasever/extras/$basearch/\ngpgcheck=1\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7\n\n#additional packages that extend functionality of existing packages\n[centosplus]\nname=CentOS-$releasever - Plus\nmirrorlist=http://mirrorlist.centos.org/?release=$releasever\narch=$basearch\nrepo=centosplus\ninfra=$infra\n#baseurl=http://mirror.centos.org/centos/$releasever/centosplus/$basearch/\ngpgcheck=1\nenabled=0\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7\nEOF\n\n\n\n\nInstall the correct version of \nkernel\n, \nkernel-tools\n and \nsystemd\n:\n\n\nyum install -y kernel-3.10.0-229.14.1.el7 kernel-tools-3.10.0-229.14.1.el7 systemd-208-20.el7_1.6\n\n\n\n\nTo permanently disable \nSELinux\n set SELINUX=disabled in /etc/selinux/config This ensures that SELinux does not turn itself on after you reboot the machine:\n\n\nsetenforce 0 \n sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config\n\n\n\n\nYou need to install \niptables-services\n, otherwise the \niptables save\n command will not be available:\n\n\nyum -y install iptables-services net-tools unzip\n\n\n\n\nPlease configure your \niptables\n on your machine:\n\n\niptables --flush INPUT \n \\\niptables --flush FORWARD \n \\\nservice iptables save \n \\\nsed -i 's/net.ipv4.ip_forward = 0/net.ipv4.ip_forward = 1/g' /etc/sysctl.conf\n\n\n\n\nConfigure a custom Docker repository for installing the correct version of Docker:\n\n\ncat \n /etc/yum.repos.d/docker.repo \nEOF\n\n[dockerrepo]\nname=Docker Repository\nbaseurl=https://yum.dockerproject.org/repo/main/centos/7\nenabled=1\ngpgcheck=1\ngpgkey=https://yum.dockerproject.org/gpg\nEOF\n\n\n\n\nThen you are able to install the Docker service:\n\n\nyum install -y docker-engine-1.8.3\n\n\n\n\nConfigure your installed Docker service:\n\n\ncat \n /usr/lib/systemd/system/docker.service \nEOF\n\n[Unit]\nDescription=Docker Application Container Engine\nDocumentation=https://docs.docker.com\nAfter=network.target docker.socket cloud-final.service\nRequires=docker.socket\nWants=cloud-final.service\n\n[Service]\nExecStart=/usr/bin/docker -d -H fd:// -H tcp://0.0.0.0:2376 --selinux-enabled=false --storage-driver=devicemapper --storage-opt=dm.basesize=30G\nMountFlags=slave\nLimitNOFILE=200000\nLimitNPROC=16384\nLimitCORE=infinity\n\n[Install]\nWantedBy=multi-user.target\nEOF\n\n\n\n\nRemove docker folder and restart Docker service:\n\n\nrm -rf /var/lib/docker \n systemctl daemon-reload \n service docker start \n systemctl enable docker.service\n\n\n\n\nDownload \ncloudbreak-deployer\n:\n\n\ncurl https://raw.githubusercontent.com/sequenceiq/cloudbreak-deployer/master/install-latest | sh \n cbd --version\n\n\n\n\nInitialize your Profile\n\n\nFirst initialize cbd by creating a \nProfile\n file:\n\n\ncbd init\n\n\n\n\nIt will create a \nProfile\n file in the current directory. Please edit the file - the only required\nconfiguration is the \nPUBLIC_IP\n. This IP will be used to access the Cloudbreak UI\n(called Uluwatu). In some cases the \ncbd\n tool tries to guess it, if can't than will give a hint.\n\n\nGenerate your Profile\n\n\nYou are done with the configuration of Cloudbreak deployer. The last thing you have to do is to regenerate the configurations in order to take effect.\n\n\nrm *.yml\ncbd generate\n\n\n\n\nThis command applies the following steps:\n\n\n\n\ncreates the \ndocker-compose.yml\n file that describes the configuration of all the Docker containers needed for the Cloudbreak deployment.\n\n\ncreates the \nuaa.yml\n file that holds the configuration of the identity server used to authenticate users to Cloudbreak.\n\n\n\n\nStart Cloudbreak\n\n\nTo start the Cloudbreak application use the following command.\nThis will start all the Docker containers and initialize the application. It will take a few minutes until all the services start.\n\n\ncbd start\n\n\n\n\n\n\nLaunching it first will take more time as it downloads all the docker images needed by Cloudbreak.\n\n\n\n\nAfter the \ncbd start\n command finishes you can check the logs of the Cloudbreak server with this command:\n\n\ncbd logs cloudbreak\n\n\n\n\n\n\nCloudbreak server should start within a minute - you should see a line like this: \nStarted CloudbreakApplication in 36.823 seconds\n\n\n\n\nNext steps\n\n\nOnce Cloudbreak is up and running you should check out the \nProvisioning Prerequisites\n needed to create Azure\nclusters with Cloudbreak.", 
            "title": "Setup"
        }, 
        {
            "location": "/azure/#azure-setup", 
            "text": "", 
            "title": "Azure Setup"
        }, 
        {
            "location": "/azure/#setup-cloudbreak-deployer", 
            "text": "To install and configure the Cloudbreak Deployer on Azure, start\nan  OpenLogic 7.1  VM on Azure.  Make sure you opened the following ports:   SSH (22)  Ambari (8080)  Identity server (8089)  Cloudbreak GUI (3000)  User authentication (3001)   Please log in to the machine with SSH or use username and password authentication (the following example shows how to ssh into the machine):  ssh -i  azure-ssh-pem-file   username @ virtual-machine-ip   Assume  root  privileges with this command:  sudo su  Configure the correct yum repository on the machine:  cat   /etc/yum.repos.d/CentOS-Base.repo  EOF \n# CentOS-Base.repo\n#\n# The mirror system uses the connecting IP address of the client and the\n# update status of each mirror to pick mirrors that are updated to and\n# geographically close to the client.  You should use this for CentOS updates\n# unless you are manually picking other mirrors.\n#\n# If the mirrorlist= does not work for you, as a fall back you can try the\n# remarked out baseurl= line instead.\n#\n#\n\n[base]\nname=CentOS-$releasever - Base\nmirrorlist=http://mirrorlist.centos.org/?release=$releasever arch=$basearch repo=os infra=$infra\n#baseurl=http://mirror.centos.org/centos/$releasever/os/$basearch/\ngpgcheck=1\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7\n\n#released updates\n[updates]\nname=CentOS-$releasever - Updates\nmirrorlist=http://mirrorlist.centos.org/?release=$releasever arch=$basearch repo=updates infra=$infra\n#baseurl=http://mirror.centos.org/centos/$releasever/updates/$basearch/\ngpgcheck=1\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7\n\n#additional packages that may be useful\n[extras]\nname=CentOS-$releasever - Extras\nmirrorlist=http://mirrorlist.centos.org/?release=$releasever arch=$basearch repo=extras infra=$infra\n#baseurl=http://mirror.centos.org/centos/$releasever/extras/$basearch/\ngpgcheck=1\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7\n\n#additional packages that extend functionality of existing packages\n[centosplus]\nname=CentOS-$releasever - Plus\nmirrorlist=http://mirrorlist.centos.org/?release=$releasever arch=$basearch repo=centosplus infra=$infra\n#baseurl=http://mirror.centos.org/centos/$releasever/centosplus/$basearch/\ngpgcheck=1\nenabled=0\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7\nEOF  Install the correct version of  kernel ,  kernel-tools  and  systemd :  yum install -y kernel-3.10.0-229.14.1.el7 kernel-tools-3.10.0-229.14.1.el7 systemd-208-20.el7_1.6  To permanently disable  SELinux  set SELINUX=disabled in /etc/selinux/config This ensures that SELinux does not turn itself on after you reboot the machine:  setenforce 0   sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config  You need to install  iptables-services , otherwise the  iptables save  command will not be available:  yum -y install iptables-services net-tools unzip  Please configure your  iptables  on your machine:  iptables --flush INPUT   \\\niptables --flush FORWARD   \\\nservice iptables save   \\\nsed -i 's/net.ipv4.ip_forward = 0/net.ipv4.ip_forward = 1/g' /etc/sysctl.conf  Configure a custom Docker repository for installing the correct version of Docker:  cat   /etc/yum.repos.d/docker.repo  EOF \n[dockerrepo]\nname=Docker Repository\nbaseurl=https://yum.dockerproject.org/repo/main/centos/7\nenabled=1\ngpgcheck=1\ngpgkey=https://yum.dockerproject.org/gpg\nEOF  Then you are able to install the Docker service:  yum install -y docker-engine-1.8.3  Configure your installed Docker service:  cat   /usr/lib/systemd/system/docker.service  EOF \n[Unit]\nDescription=Docker Application Container Engine\nDocumentation=https://docs.docker.com\nAfter=network.target docker.socket cloud-final.service\nRequires=docker.socket\nWants=cloud-final.service\n\n[Service]\nExecStart=/usr/bin/docker -d -H fd:// -H tcp://0.0.0.0:2376 --selinux-enabled=false --storage-driver=devicemapper --storage-opt=dm.basesize=30G\nMountFlags=slave\nLimitNOFILE=200000\nLimitNPROC=16384\nLimitCORE=infinity\n\n[Install]\nWantedBy=multi-user.target\nEOF  Remove docker folder and restart Docker service:  rm -rf /var/lib/docker   systemctl daemon-reload   service docker start   systemctl enable docker.service  Download  cloudbreak-deployer :  curl https://raw.githubusercontent.com/sequenceiq/cloudbreak-deployer/master/install-latest | sh   cbd --version  Initialize your Profile  First initialize cbd by creating a  Profile  file:  cbd init  It will create a  Profile  file in the current directory. Please edit the file - the only required\nconfiguration is the  PUBLIC_IP . This IP will be used to access the Cloudbreak UI\n(called Uluwatu). In some cases the  cbd  tool tries to guess it, if can't than will give a hint.  Generate your Profile  You are done with the configuration of Cloudbreak deployer. The last thing you have to do is to regenerate the configurations in order to take effect.  rm *.yml\ncbd generate  This command applies the following steps:   creates the  docker-compose.yml  file that describes the configuration of all the Docker containers needed for the Cloudbreak deployment.  creates the  uaa.yml  file that holds the configuration of the identity server used to authenticate users to Cloudbreak.   Start Cloudbreak  To start the Cloudbreak application use the following command.\nThis will start all the Docker containers and initialize the application. It will take a few minutes until all the services start.  cbd start   Launching it first will take more time as it downloads all the docker images needed by Cloudbreak.   After the  cbd start  command finishes you can check the logs of the Cloudbreak server with this command:  cbd logs cloudbreak   Cloudbreak server should start within a minute - you should see a line like this:  Started CloudbreakApplication in 36.823 seconds   Next steps  Once Cloudbreak is up and running you should check out the  Provisioning Prerequisites  needed to create Azure\nclusters with Cloudbreak.", 
            "title": "Setup Cloudbreak Deployer"
        }, 
        {
            "location": "/azure_pre_prov/", 
            "text": "Provisioning Prerequisites\n\n\nWe use the new \nAzure ARM\n in \norder to launch clusters. In order to work we need to create an Active Directory application with the configured name and password and adds the permissions that are needed to call the Azure Resource Manager API. Cloudbreak deployer automates all this for you.\n\n\nGenerate a new SSH key\n\n\nAll the instances created by Cloudbreak are configured to allow key-based SSH,\nso you'll need to provide an SSH public key that can be used later to SSH onto the instances in the clusters you'll create with Cloudbreak.\nYou can use one of your existing keys or you can generate a new one.\n\n\nTo generate a new SSH keypair:\n\n\nssh-keygen -t rsa -b 4096 -C \nyour_email@example.com\n\n# Creates a new ssh key, using the provided email as a label\n# Generating public/private rsa key pair.\n\n\n\n\n# Enter file in which to save the key (/Users/you/.ssh/id_rsa): [Press enter]\nYou'll be asked to enter a passphrase, but you can leave it empty.\n\n# Enter passphrase (empty for no passphrase): [Type a passphrase]\n# Enter same passphrase again: [Type passphrase again]\n\n\n\n\nAfter you enter a passphrase the keypair is generated. The output should look something like below.\n\n\n# Your identification has been saved in /Users/you/.ssh/id_rsa.\n# Your public key has been saved in /Users/you/.ssh/id_rsa.pub.\n# The key fingerprint is:\n# 01:0f:f4:3b:ca:85:sd:17:sd:7d:sd:68:9d:sd:a2:sd your_email@example.com\n\n\n\n\nLater you'll need to pass the \n.pub\n file's contents to Cloudbreak and use the private part to SSH to the instances.\n\n\nAzure access setup\n\n\nIf you do not have an Active directory user then you have to configure it before deploying a cluster with Cloudbreak.\n\n\n\n\n\n\nGo to \nmanage.windowsazure.com\n \n \nActive Directory\n\n\n\n\n\n\n\n\nYou can configure your AD users on \nYour active directory\n \n \nUsers\n menu\n\n\n\n\n\n\n\nHere you can add the new user to AD. Simply click on \nAdd User\n on the bottom of the page\n\n\n\n\n\n\n\nType the new user name into the box\n\n\n\n\n\n\n\nYou will see the new user in the list. You have got a temporary password so you have to change it before you start using the new user.\n\n\n\n\n\n\n\nAfter you add the user to the AD you need to add your AD user to the \nmanage.windowsazure.com\n \n \nSettings\n \n \nAdministrators\n\n\n\n\n\n\n\n\nHere you can add the new user to Administrators. Simply click on \nAdd\n on the bottom of the page\n\n\n\n\n\n\n\nAzure application setup with Cloudbreak Deployer\n\n\nIn order for Cloudbreak to be able to launch clusters on Azure on your behalf you need to set up your \nAzure ARM application\n. We have automated the Azure configurations in the Cloudbreak Deployer (CBD). After the CBD has installed, simply run the following command:\n\n\ncbd azure configure-arm --app_name myapp --app_password password123 --subscription_id 1234-abcd-efgh-1234\n\n\n\n\nOptions:\n\n\n--app_name\n: Your application name. Default is \napp\n.\n\n\n--app_password\n: Your application password. Default is \npassword\n.\n\n\n--subscription_id\n: Your Azure subscription ID.\n\n\n--username\n: Your Azure username.\n\n\n--password\n: Your Azure password.\n\n\nThe command first creates an Active Directory application with the configured name and password and adds the permissions that are needed to call the Azure Resource Manager API.\nPlease use the output of the command when you creating your Azure credential in Cloudbreak.\nThe output of the command something like this:\n\n\nSubscription ID: sdf324-26b3-sdf234-ad10-234dfsdfsd\nApp ID: 234sdf-c469-sdf234-9062-dsf324\nPassword: password123\nApp Owner Tenant ID: sdwerwe1-d98e-dsf12-dsf123-df123232\n\n\n\n\nFilesystem configuration\n\n\nWhen starting a cluster with Cloudbreak on Azure, the default filesystem is \u201cWindows Azure Blob Storage with DASH\u201d. Hadoop has built-in support for the \nWASB filesystem\n so it can be used easily as HDFS instead of disks.\n\n\nDisks and blob storage\n\n\nIn Azure every data disk attached to a virtual machine \nis stored\n as a virtual hard disk (VHD) in a page blob inside an Azure storage account. Because these are not local disks and the operations must be done on the VHD files it causes degraded performance when used as HDFS.\nWhen WASB is used as a Hadoop filesystem the files are full-value blobs in a storage account. It means better performance compared to the data disks and the WASB filesystem can be configured very easily but Azure storage accounts have their own \nlimitations\n as well. There is a space limitation for TB per storage account (500 TB) as well but the real bottleneck is the total request rate that is only 20000 IOPS where Azure will start to throw errors when trying to do an I/O operation.\nTo bypass those limits Microsoft created a small service called \nDASH\n. DASH itself is a service that imitates the API of the Azure Blob Storage API and it can be deployed as a Microsoft Azure Cloud Service. Because its API is the same as the standard blob storage API it can be used \nalmost\n in the same way as the default WASB filesystem from a Hadoop deployment.\nDASH works by sharding the storage access across multiple storage accounts. It can be configured to distribute storage account load to at most 15 \nscaleout\n storage accounts. It needs one more \nnamespace\n storage account where it keeps track of where the data is stored.\nWhen configuring a WASB filesystem with Hadoop, the only required config entries are the ones where the access details are described. To access a storage account Azure generates an access key that is displayed on the Azure portal or can be queried through the API while the account name is the name of the storage account itself. A DASH service has a similar account name and key, those can be configured in the configuration file while deploying the cloud service.\n\n\n\n\nDeploying a DASH service with Cloudbreak deployer\n\n\nWe have automated the deployment of a DASH service in cloudbreak-deployer. After cbd is installed, simply run the following command to deploy a DASH cloud service with 5 scale out storage accounts:\n\n\ncbd azure deploy-dash --accounts 5 --prefix dash --location \nWest Europe\n --instances 3\n\n\n\n\nThe command first creates the namespace account and the scaleout storage accounts, builds the \n.cscfg\n configuration file based on the created storage account names and keys, generates an Account Name and an Account Key for the DASH service and finally deploys the cloud service package file to a new cloud service.\n\n\nThe WASB filesystem configured with DASH can be used as a data lake - when multiple clusters are deployed with the same DASH filesystem configuration the same data can be accessed from all the clusters, but every cluster can have a different service configured as well. In that case deploy as many DASH services with cbd as clusters with Cloudbreak and configure them accordingly.\n\n\nNext steps\n\n\nAfter these prerequisites are done you can move on to create clusters on the \nUI\n or with the \nShell\n.", 
            "title": "Provisioning Prerequisites"
        }, 
        {
            "location": "/azure_pre_prov/#provisioning-prerequisites", 
            "text": "We use the new  Azure ARM  in \norder to launch clusters. In order to work we need to create an Active Directory application with the configured name and password and adds the permissions that are needed to call the Azure Resource Manager API. Cloudbreak deployer automates all this for you.", 
            "title": "Provisioning Prerequisites"
        }, 
        {
            "location": "/azure_pre_prov/#generate-a-new-ssh-key", 
            "text": "All the instances created by Cloudbreak are configured to allow key-based SSH,\nso you'll need to provide an SSH public key that can be used later to SSH onto the instances in the clusters you'll create with Cloudbreak.\nYou can use one of your existing keys or you can generate a new one.  To generate a new SSH keypair:  ssh-keygen -t rsa -b 4096 -C  your_email@example.com \n# Creates a new ssh key, using the provided email as a label\n# Generating public/private rsa key pair.  # Enter file in which to save the key (/Users/you/.ssh/id_rsa): [Press enter]\nYou'll be asked to enter a passphrase, but you can leave it empty.\n\n# Enter passphrase (empty for no passphrase): [Type a passphrase]\n# Enter same passphrase again: [Type passphrase again]  After you enter a passphrase the keypair is generated. The output should look something like below.  # Your identification has been saved in /Users/you/.ssh/id_rsa.\n# Your public key has been saved in /Users/you/.ssh/id_rsa.pub.\n# The key fingerprint is:\n# 01:0f:f4:3b:ca:85:sd:17:sd:7d:sd:68:9d:sd:a2:sd your_email@example.com  Later you'll need to pass the  .pub  file's contents to Cloudbreak and use the private part to SSH to the instances.", 
            "title": "Generate a new SSH key"
        }, 
        {
            "location": "/azure_pre_prov/#azure-access-setup", 
            "text": "If you do not have an Active directory user then you have to configure it before deploying a cluster with Cloudbreak.    Go to  manage.windowsazure.com     Active Directory     You can configure your AD users on  Your active directory     Users  menu    Here you can add the new user to AD. Simply click on  Add User  on the bottom of the page    Type the new user name into the box    You will see the new user in the list. You have got a temporary password so you have to change it before you start using the new user.    After you add the user to the AD you need to add your AD user to the  manage.windowsazure.com     Settings     Administrators     Here you can add the new user to Administrators. Simply click on  Add  on the bottom of the page", 
            "title": "Azure access setup"
        }, 
        {
            "location": "/azure_pre_prov/#azure-application-setup-with-cloudbreak-deployer", 
            "text": "In order for Cloudbreak to be able to launch clusters on Azure on your behalf you need to set up your  Azure ARM application . We have automated the Azure configurations in the Cloudbreak Deployer (CBD). After the CBD has installed, simply run the following command:  cbd azure configure-arm --app_name myapp --app_password password123 --subscription_id 1234-abcd-efgh-1234  Options:  --app_name : Your application name. Default is  app .  --app_password : Your application password. Default is  password .  --subscription_id : Your Azure subscription ID.  --username : Your Azure username.  --password : Your Azure password.  The command first creates an Active Directory application with the configured name and password and adds the permissions that are needed to call the Azure Resource Manager API.\nPlease use the output of the command when you creating your Azure credential in Cloudbreak.\nThe output of the command something like this:  Subscription ID: sdf324-26b3-sdf234-ad10-234dfsdfsd\nApp ID: 234sdf-c469-sdf234-9062-dsf324\nPassword: password123\nApp Owner Tenant ID: sdwerwe1-d98e-dsf12-dsf123-df123232", 
            "title": "Azure application setup with Cloudbreak Deployer"
        }, 
        {
            "location": "/azure_pre_prov/#filesystem-configuration", 
            "text": "When starting a cluster with Cloudbreak on Azure, the default filesystem is \u201cWindows Azure Blob Storage with DASH\u201d. Hadoop has built-in support for the  WASB filesystem  so it can be used easily as HDFS instead of disks.  Disks and blob storage  In Azure every data disk attached to a virtual machine  is stored  as a virtual hard disk (VHD) in a page blob inside an Azure storage account. Because these are not local disks and the operations must be done on the VHD files it causes degraded performance when used as HDFS.\nWhen WASB is used as a Hadoop filesystem the files are full-value blobs in a storage account. It means better performance compared to the data disks and the WASB filesystem can be configured very easily but Azure storage accounts have their own  limitations  as well. There is a space limitation for TB per storage account (500 TB) as well but the real bottleneck is the total request rate that is only 20000 IOPS where Azure will start to throw errors when trying to do an I/O operation.\nTo bypass those limits Microsoft created a small service called  DASH . DASH itself is a service that imitates the API of the Azure Blob Storage API and it can be deployed as a Microsoft Azure Cloud Service. Because its API is the same as the standard blob storage API it can be used  almost  in the same way as the default WASB filesystem from a Hadoop deployment.\nDASH works by sharding the storage access across multiple storage accounts. It can be configured to distribute storage account load to at most 15  scaleout  storage accounts. It needs one more  namespace  storage account where it keeps track of where the data is stored.\nWhen configuring a WASB filesystem with Hadoop, the only required config entries are the ones where the access details are described. To access a storage account Azure generates an access key that is displayed on the Azure portal or can be queried through the API while the account name is the name of the storage account itself. A DASH service has a similar account name and key, those can be configured in the configuration file while deploying the cloud service.   Deploying a DASH service with Cloudbreak deployer  We have automated the deployment of a DASH service in cloudbreak-deployer. After cbd is installed, simply run the following command to deploy a DASH cloud service with 5 scale out storage accounts:  cbd azure deploy-dash --accounts 5 --prefix dash --location  West Europe  --instances 3  The command first creates the namespace account and the scaleout storage accounts, builds the  .cscfg  configuration file based on the created storage account names and keys, generates an Account Name and an Account Key for the DASH service and finally deploys the cloud service package file to a new cloud service.  The WASB filesystem configured with DASH can be used as a data lake - when multiple clusters are deployed with the same DASH filesystem configuration the same data can be accessed from all the clusters, but every cluster can have a different service configured as well. In that case deploy as many DASH services with cbd as clusters with Cloudbreak and configure them accordingly.", 
            "title": "Filesystem configuration"
        }, 
        {
            "location": "/azure_pre_prov/#next-steps", 
            "text": "After these prerequisites are done you can move on to create clusters on the  UI  or with the  Shell .", 
            "title": "Next steps"
        }, 
        {
            "location": "/azure_cb_ui/", 
            "text": "Provisioning via Browser\n\n\nYou can log into the Cloudbreak application at http://PUBLIC_IP:3000.\n\n\nThe main goal of the Cloudbreak UI is to easily create clusters on your own cloud provider account.\nThis description details the AZURE setup - if you'd like to use a different cloud provider check out its manual.\n\n\nThis document explains the four steps that need to be followed to create Cloudbreak clusters from the UI:\n\n\n\n\nconnect your AZURE account with Cloudbreak\n\n\ncreate some template resources on the UI that describe the infrastructure of your clusters\n\n\ncreate a blueprint that describes the HDP services in your clusters and add some recipes for customization\n\n\nlaunch the cluster itself based on these template resources\n\n\n\n\nSetting up Azure credentials\n\n\nIf you do not have an Azure Resource manager application you can simply create it with Cloudbreak deployer. Please read the \nProvisioning prerequisites\n for more information.\n\n\nName:\n name of your credential\n\n\nDescription:\n short description of your linked credential\n\n\nSubscription Id:\n your Azure subscription id - see Accounts (\nportal.azure.com\n \nBrowse all\n \nSubscription\n)\n\n\nPassword:\n your password which was setted up when you create the AD app\n\n\nApp Id:\n You app Id (\nportal.azure.com\n \nBrowse all\n \nSubscription\n \nSubscription detail\n \nUsers\n \nYou application\n \nProperties\n)\n\n\nApp Owner Tenant Id:\n You Tenant Id (\nportal.azure.com\n \nBrowse all\n \nSubscription\n \nSubscription detail\n \nUsers\n \nYou application\n \nProperties\n)\n\n\nSSH public key:\n the SSH public key in OpenSSH format that's private keypair can be used to \nlog into the launched instances\n later\n\n\n\n\nCloudbreak is supporting simple rsa public key instead of X509 certificate file after 1.0.4 version\n\n\n\n\nThe ssh username is \ncloudbreak\n\n\nInfrastructure templates\n\n\nAfter your AZURE account is linked to Cloudbreak you can start creating templates that describe your clusters' infrastructure:\n\n\n\n\nresources\n\n\nnetworks\n\n\nsecurity groups\n\n\n\n\nWhen you create a template, Cloudbreak \ndoesn't make any requests\n to AZURE.\nResources are only created on AZURE after the \ncreate cluster\n button is pushed.\nThese templates are saved to Cloudbreak's database and can be reused with multiple clusters to describe the infrastructure.\n\n\nManage resources\n\n\nUsing manage resources you can create infrastructure templates. Templates describes the infrastructure where the HDP cluster will be provisioned. We support heterogenous clusters - this means that one cluster can be built by combining different templates.\n\n\nName:\n name of your template\n\n\nDescription:\n short description of your template\n\n\nInstance type:\n the Azure instance type to be used\n\n\nAttached volumes per instance:\n the number of disks to be attached\n\n\nVolume size (GB):\n the size of the attached disks (in GB)\n\n\nPublic in account:\n share it with others in the account\n\n\nManage blueprints\n\n\nBlueprints are your declarative definition of a Hadoop cluster.\n\n\nName:\n name of your blueprint\n\n\nDescription:\n short description of your blueprint\n\n\nSource URL:\n you can add a blueprint by pointing to a URL. As an example you can use this \nblueprint\n.\n\n\nManual copy:\n you can copy paste your blueprint in this text area\n\n\nPublic in account:\n share it with others in the account\n\n\nManage networks\n\n\nManage networks allows you to create or reuse existing networks and configure them.\n\n\nName:\n name of the network\n\n\nDescription:\n short description of your network\n\n\nSubnet (CIDR):\n a subnet in the VPC with CIDR block\n\n\nAddress prefix (CIDR):\n the address space that is used for subnets\n\n\nPublic in account:\n share it with others in the account\n\n\nSecurity groups\n\n\nThey describe the allowed inbound traffic to the instances in the cluster.\nCurrently only one security group template can be selected for a Cloudbreak cluster and all the instances have a public IP address so all the instances in the cluster will belong to the same security group.\nThis may change in a later release.\n\n\nYou can define your own security group by adding all the ports, protocols and CIDR range you'd like to use. 443 needs to be there in every security group otherwise Cloudbreak won't be able to communicate with the provisioned cluster. The rules defined here doesn't need to contain the internal rules, those are automatically added by Cloudbreak to the security group on Azure.\n\n\nYou can also use the two pre-defined security groups in Cloudbreak:\n\n\nonly-ssh-and-ssl:\n all ports are locked down except for SSH and gateway HTTPS (you can't access Hadoop services outside of the VPC):\n\n\n\n\nSSH (22)\n\n\nHTTPS (443)\n\n\n\n\nall-services-port:\n all Hadoop services and SSH/gateway HTTPS are accessible by default:\n\n\n\n\nSSH (22)\n\n\nHTTPS (443)\n\n\nAmbari (8080)\n\n\nConsul (8500)\n\n\nNN (50070)\n\n\nRM Web (8088)\n\n\nScheduler (8030RM)\n\n\nIPC (8050RM)\n\n\nJob history server (19888)\n\n\nHBase master (60000)\n\n\nHBase master web (60010)\n\n\nHBase RS (16020)\n\n\nHBase RS info (60030)\n\n\nFalcon (15000)\n\n\nStorm (8744)\n\n\nHive metastore (9083)\n\n\nHive server (10000)\n\n\nHive server HTTP (10001)\n\n\nAccumulo master (9999)\n\n\nAccumulo Tserver (9997)\n\n\nAtlas (21000)\n\n\nKNOX (8443)\n\n\nOozie (11000)\n\n\nSpark HS (18080)\n\n\nNM Web (8042)\n\n\nZeppelin WebSocket (9996)\n\n\nZeppelin UI (9995)\n\n\nKibana (3080)\n\n\nElasticsearch (9200)\n\n\n\n\nIf \nPublic in account\n is checked all the users belonging to your account will be able to use this security group template to create clusters, but cannot delete or modify it.\n\n\n\n\nNote that the security groups are \nnot created\n on AZURE after the \nCreate Security Group\n button is pushed, only \nafter the cluster provisioning starts with the selected security group template.\n\n\n\n\nCluster installation\n\n\nThis section describes\n\n\nBlueprints\n\n\nBlueprints are your declarative definition of a Hadoop cluster. These are the same blueprints that are \nused by Ambari\n.\n\n\nYou can use the 3 default blueprints pre-defined in Cloudbreak or you can create your own.\nBlueprints can be added from an URL or the whole JSON can be copied to the \nManual copy\n field.\n\n\nThe hostgroups added in the JSON will be mapped to a set of instances when starting the cluster and the services and components defined in the hostgroup will be installed on the corresponding nodes.\nIt is not necessary to define all the configuration fields in the blueprints - if a configuration is missing, Ambari will fill that with a default value.\nThe configurations defined in the blueprint can also be modified later from the Ambari UI.\n\n\nIf \nPublic in account\n is checked all the users belonging to your account will be able to use this blueprint to create clusters, but cannot delete or modify it.\n\n\nA blueprint can be exported from a running Ambari cluster that can be reused in Cloudbreak with slight modifications.\nThere is no automatic way to modify an exported blueprint and make it instantly usable in Cloudbreak, the modifications have to be done manually.\nWhen the blueprint is exported some configurations will have for example hardcoded domain names, or memory configurations that won't be applicable to the Cloudbreak cluster.\n\n\nCluster customization\n\n\nSometimes it can be useful to define some custom scripts that run during cluster creation and add some additional functionality.\nFor example it can be a service you'd like to install but it's not supported by Ambari or some script that automatically downloads some data to the necessary nodes.\nThe most notable example is Ranger setup: it has a prerequisite of a running database when Ranger Admin is installing.\nA PostgreSQL database can be easily started and configured with a recipe before the blueprint installation starts.\n\n\nTo learn more about these so called \nRecipes\n, and to check out the Ranger database recipe, take a look at the \nCluster customization\n part of the documentation.\n\n\nCluster deployment\n\n\nAfter all the templates are configured you can deploy a new HDP cluster. Start by selecting a previously created credential in the header.\nClick on \ncreate cluster\n, give it a \nName\n, select a \nRegion\n where the cluster infrastructure will be provisioned and select one of the \nNetworks\n and \nSecurity Groups\n created earlier.\nAfter you've selected a \nBlueprint\n as well you should be able to configure the \nTemplate resources\n and the number of nodes for all of the hostgroups in the blueprint.\n\n\nIf \nPublic in account\n is checked all the users belonging to your account will be able to see the newly created cluster on the UI, but cannot delete or modify it.\n\n\nIf \nEnable security\n is checked as well, Cloudbreak will install KDC and the cluster will be Kerberized. See more about it in the \nKerberos\n section of this documentation.\n\n\nAfter the \ncreate and start cluster\n button is pushed Cloudbreak will start to create resources on your AZURE account.\nCloudbreak uses \nARM template\n to create the resources - you can check out the resources created by Cloudbreak on the \nARM Portal\n on the 'Resource groups' page.\n\n\n\n\nImportant\n Always use Cloudbreak to delete the cluster, or if that fails for some reason always try to delete \nthe ARM first.\n\n\n\n\nAdvanced options\n\n\nThere are some advanced features when deploying a new cluster, these are the following:\n\n\nFile system:\n read more \nDeploying a DASH service with Cloudbreak deployer\n\n\nMinimum cluster size:\n the provisioning strategy in case of the cloud provider can't allocate all the requested nodes\n\n\nValidate blueprint:\n feature to validate or not the Ambari blueprint. By default is switched on.\n\n\nNext steps\n\n\nCongrats! Your cluster should now be up and running. To learn more about it we have some \ninteresting insights\n about Cloudbreak clusters.", 
            "title": "Provisioning - UI"
        }, 
        {
            "location": "/azure_cb_ui/#provisioning-via-browser", 
            "text": "You can log into the Cloudbreak application at http://PUBLIC_IP:3000.  The main goal of the Cloudbreak UI is to easily create clusters on your own cloud provider account.\nThis description details the AZURE setup - if you'd like to use a different cloud provider check out its manual.  This document explains the four steps that need to be followed to create Cloudbreak clusters from the UI:   connect your AZURE account with Cloudbreak  create some template resources on the UI that describe the infrastructure of your clusters  create a blueprint that describes the HDP services in your clusters and add some recipes for customization  launch the cluster itself based on these template resources", 
            "title": "Provisioning via Browser"
        }, 
        {
            "location": "/azure_cb_ui/#setting-up-azure-credentials", 
            "text": "If you do not have an Azure Resource manager application you can simply create it with Cloudbreak deployer. Please read the  Provisioning prerequisites  for more information.  Name:  name of your credential  Description:  short description of your linked credential  Subscription Id:  your Azure subscription id - see Accounts ( portal.azure.com   Browse all   Subscription )  Password:  your password which was setted up when you create the AD app  App Id:  You app Id ( portal.azure.com   Browse all   Subscription   Subscription detail   Users   You application   Properties )  App Owner Tenant Id:  You Tenant Id ( portal.azure.com   Browse all   Subscription   Subscription detail   Users   You application   Properties )  SSH public key:  the SSH public key in OpenSSH format that's private keypair can be used to  log into the launched instances  later   Cloudbreak is supporting simple rsa public key instead of X509 certificate file after 1.0.4 version   The ssh username is  cloudbreak", 
            "title": "Setting up Azure credentials"
        }, 
        {
            "location": "/azure_cb_ui/#infrastructure-templates", 
            "text": "After your AZURE account is linked to Cloudbreak you can start creating templates that describe your clusters' infrastructure:   resources  networks  security groups   When you create a template, Cloudbreak  doesn't make any requests  to AZURE.\nResources are only created on AZURE after the  create cluster  button is pushed.\nThese templates are saved to Cloudbreak's database and can be reused with multiple clusters to describe the infrastructure.  Manage resources  Using manage resources you can create infrastructure templates. Templates describes the infrastructure where the HDP cluster will be provisioned. We support heterogenous clusters - this means that one cluster can be built by combining different templates.  Name:  name of your template  Description:  short description of your template  Instance type:  the Azure instance type to be used  Attached volumes per instance:  the number of disks to be attached  Volume size (GB):  the size of the attached disks (in GB)  Public in account:  share it with others in the account  Manage blueprints  Blueprints are your declarative definition of a Hadoop cluster.  Name:  name of your blueprint  Description:  short description of your blueprint  Source URL:  you can add a blueprint by pointing to a URL. As an example you can use this  blueprint .  Manual copy:  you can copy paste your blueprint in this text area  Public in account:  share it with others in the account  Manage networks  Manage networks allows you to create or reuse existing networks and configure them.  Name:  name of the network  Description:  short description of your network  Subnet (CIDR):  a subnet in the VPC with CIDR block  Address prefix (CIDR):  the address space that is used for subnets  Public in account:  share it with others in the account  Security groups  They describe the allowed inbound traffic to the instances in the cluster.\nCurrently only one security group template can be selected for a Cloudbreak cluster and all the instances have a public IP address so all the instances in the cluster will belong to the same security group.\nThis may change in a later release.  You can define your own security group by adding all the ports, protocols and CIDR range you'd like to use. 443 needs to be there in every security group otherwise Cloudbreak won't be able to communicate with the provisioned cluster. The rules defined here doesn't need to contain the internal rules, those are automatically added by Cloudbreak to the security group on Azure.  You can also use the two pre-defined security groups in Cloudbreak:  only-ssh-and-ssl:  all ports are locked down except for SSH and gateway HTTPS (you can't access Hadoop services outside of the VPC):   SSH (22)  HTTPS (443)   all-services-port:  all Hadoop services and SSH/gateway HTTPS are accessible by default:   SSH (22)  HTTPS (443)  Ambari (8080)  Consul (8500)  NN (50070)  RM Web (8088)  Scheduler (8030RM)  IPC (8050RM)  Job history server (19888)  HBase master (60000)  HBase master web (60010)  HBase RS (16020)  HBase RS info (60030)  Falcon (15000)  Storm (8744)  Hive metastore (9083)  Hive server (10000)  Hive server HTTP (10001)  Accumulo master (9999)  Accumulo Tserver (9997)  Atlas (21000)  KNOX (8443)  Oozie (11000)  Spark HS (18080)  NM Web (8042)  Zeppelin WebSocket (9996)  Zeppelin UI (9995)  Kibana (3080)  Elasticsearch (9200)   If  Public in account  is checked all the users belonging to your account will be able to use this security group template to create clusters, but cannot delete or modify it.   Note that the security groups are  not created  on AZURE after the  Create Security Group  button is pushed, only \nafter the cluster provisioning starts with the selected security group template.", 
            "title": "Infrastructure templates"
        }, 
        {
            "location": "/azure_cb_ui/#cluster-installation", 
            "text": "This section describes  Blueprints  Blueprints are your declarative definition of a Hadoop cluster. These are the same blueprints that are  used by Ambari .  You can use the 3 default blueprints pre-defined in Cloudbreak or you can create your own.\nBlueprints can be added from an URL or the whole JSON can be copied to the  Manual copy  field.  The hostgroups added in the JSON will be mapped to a set of instances when starting the cluster and the services and components defined in the hostgroup will be installed on the corresponding nodes.\nIt is not necessary to define all the configuration fields in the blueprints - if a configuration is missing, Ambari will fill that with a default value.\nThe configurations defined in the blueprint can also be modified later from the Ambari UI.  If  Public in account  is checked all the users belonging to your account will be able to use this blueprint to create clusters, but cannot delete or modify it.  A blueprint can be exported from a running Ambari cluster that can be reused in Cloudbreak with slight modifications.\nThere is no automatic way to modify an exported blueprint and make it instantly usable in Cloudbreak, the modifications have to be done manually.\nWhen the blueprint is exported some configurations will have for example hardcoded domain names, or memory configurations that won't be applicable to the Cloudbreak cluster.  Cluster customization  Sometimes it can be useful to define some custom scripts that run during cluster creation and add some additional functionality.\nFor example it can be a service you'd like to install but it's not supported by Ambari or some script that automatically downloads some data to the necessary nodes.\nThe most notable example is Ranger setup: it has a prerequisite of a running database when Ranger Admin is installing.\nA PostgreSQL database can be easily started and configured with a recipe before the blueprint installation starts.  To learn more about these so called  Recipes , and to check out the Ranger database recipe, take a look at the  Cluster customization  part of the documentation.", 
            "title": "Cluster installation"
        }, 
        {
            "location": "/azure_cb_ui/#cluster-deployment", 
            "text": "After all the templates are configured you can deploy a new HDP cluster. Start by selecting a previously created credential in the header.\nClick on  create cluster , give it a  Name , select a  Region  where the cluster infrastructure will be provisioned and select one of the  Networks  and  Security Groups  created earlier.\nAfter you've selected a  Blueprint  as well you should be able to configure the  Template resources  and the number of nodes for all of the hostgroups in the blueprint.  If  Public in account  is checked all the users belonging to your account will be able to see the newly created cluster on the UI, but cannot delete or modify it.  If  Enable security  is checked as well, Cloudbreak will install KDC and the cluster will be Kerberized. See more about it in the  Kerberos  section of this documentation.  After the  create and start cluster  button is pushed Cloudbreak will start to create resources on your AZURE account.\nCloudbreak uses  ARM template  to create the resources - you can check out the resources created by Cloudbreak on the  ARM Portal  on the 'Resource groups' page.   Important  Always use Cloudbreak to delete the cluster, or if that fails for some reason always try to delete \nthe ARM first.   Advanced options  There are some advanced features when deploying a new cluster, these are the following:  File system:  read more  Deploying a DASH service with Cloudbreak deployer  Minimum cluster size:  the provisioning strategy in case of the cloud provider can't allocate all the requested nodes  Validate blueprint:  feature to validate or not the Ambari blueprint. By default is switched on.", 
            "title": "Cluster deployment"
        }, 
        {
            "location": "/azure_cb_ui/#next-steps", 
            "text": "Congrats! Your cluster should now be up and running. To learn more about it we have some  interesting insights  about Cloudbreak clusters.", 
            "title": "Next steps"
        }, 
        {
            "location": "/azure_cb_shell/", 
            "text": "Interactive mode\n\n\nStart the shell with \ncbd util cloudbreak-shell\n. This will launch the Cloudbreak shell inside a Docker container and you are ready to start using it.\n\n\nYou have to copy files into the cbd working directory, which you would like to use from shell. For example if your \ncbd\n working directory is \n~/prj/cbd\n then copy your blueprint and public ssh key file into this directory. You can refer to these files with their names from the shell.\n\n\nCreate a cloud credential\n\n\ncredential create --AZURE --description \ncredential description\n --name myazurecredential --subscriptionId \nyour Azure subscription id\n --appId \nyour Azure application id\n --tenantId \nyour tenant id\n --password \nyour Azure application password\n --sshKeyPath \npath of your public SSH key file\n\n\n\n\n\n\n\nCloudbreak is supporting simple rsa public key instead of X509 certificate file after 1.0.4 version\n\n\n\n\nAlternatively you can upload your public key from an url as well, by using the \n\u2014sshKeyUrl\n switch. You can check whether the credential was creates successfully by using the \ncredential list\n command.\nYou can switch between your cloud credential - when you\u2019d like to use one and act with that you will have to use:\n\n\ncredential select --name myazurecredential\n\n\n\n\nYou can delete your cloud credential - when you\u2019d like to delete one you will have to use:\n\n\ncredential delete --name myazurecredential\n\n\n\n\nYou can show your cloud credential - when you\u2019d like to show one you will have to use:\n\n\ncredential show --name myazurecredential\n\n\n\n\nCreate a template\n\n\nA template gives developers and systems administrators an easy way to create and manage a collection of cloud infrastructure related resources, maintaining and updating them in an orderly and predictable fashion. A template can be used repeatedly to create identical copies of the same stack (or to use as a foundation to start a new stack).\n\n\ntemplate create --AZURE --name azuretemplate --description azure-template --instanceType STANDARD_D3 --volumeSize 100 --volumeCount 2\n\n\n\n\nYou can check whether the template was created successfully by using the \ntemplate list\n or \ntemplate show\n command.\n\n\nYou can delete your cloud template - when you\u2019d like to delete one you will have to use:\n\n\ntemplate delete --name azuretemplate\n\n\n\n\nCreate or select a blueprint\n\n\nYou can define Ambari blueprints with cloudbreak-shell:\n\n\nblueprint add --name myblueprint --description myblueprint-description --file \nthe path of the blueprint\n\n\n\n\n\nOther available options:\n\n\n--url\n the url of the blueprint\n\n\n--publicInAccount\n flags if the network is public in the account\n\n\nWe ship default Ambari blueprints with Cloudbreak. You can use these blueprints or add yours. To see the available blueprints and use one of them please use:\n\n\nblueprint list\n\nblueprint select --name hdp-small-default\n\n\n\n\nCreate a network\n\n\nA network gives developers and systems administrators an easy way to create and manage a collection of cloud infrastructure related networking, maintaining and updating them in an orderly and predictable fashion. A network can be used repeatedly to create identical copies of the same stack (or to use as a foundation to start a new stack).\n\n\nnetwork create --AZURE --name azurenetwork --description azure-network --subnet 10.0.0.0/16 --addressPrefix 10.0.0.0/8\n\n\n\n\nOther available options:\n\n\n--publicInAccount\n flags if the network is public in the account\n\n\nThere is a default network with name \ndefault-azure-network\n with 10.0.0.0/16 subnet and 10.0.0.0/8 addressPrefix.\n\n\nYou can check whether the network was created successfully by using the \nnetwork list\n command. Check the network and select it if you are happy with it:\n\n\nnetwork show --name azurenetwork\n\nnetwork select --name azurenetwork\n\n\n\n\nCreate a security group\n\n\nA security group gives developers and systems administrators an easy way to create and manage a collection of cloud infrastructure related security rules.\n\n\nsecuritygroup create --name secgroup_example --description securitygroup-example --rules 0.0.0.0/0:tcp:8080,9090;10.0.33.0/24:tcp:1234,1235\n\n\n\n\nYou can check whether the security group was created successfully by using the \nsecuritygroup list\n command. Check the security group and select it if you are happy with it:\n\n\nsecuritygroup show --name secgroup_example\n\nsecuritygroup select --name secgroup_example\n\n\n\n\nThere are two default security groups defined: \nall-services-port\n and \nonly-ssh-and-ssl\n\n\nonly-ssh-and-ssl:\n all ports are locked down (you can't access Hadoop services outside of the VPC)\n\n\n\n\nSSH (22)\n\n\nHTTPS (443)\n\n\n\n\nall-services-port:\n all Hadoop services + SSH/HTTP are accessible by default:\n\n\n\n\nSSH (22)\n\n\nHTTPS (443)\n\n\nAmbari (8080)\n\n\nConsul (8500)\n\n\nNN (50070)\n\n\nRM Web (8088)\n\n\nScheduler (8030RM)\n\n\nIPC (8050RM)\n\n\nJob history server (19888)\n\n\nHBase master (60000)\n\n\nHBase master web (60010)\n\n\nHBase RS (16020)\n\n\nHBase RS info (60030)\n\n\nFalcon (15000)\n\n\nStorm (8744)\n\n\nHive metastore (9083)\n\n\nHive server (10000)\n\n\nHive server HTTP (10001)\n\n\nAccumulo master (9999)\n\n\nAccumulo Tserver (9997)\n\n\nAtlas (21000)\n\n\nKNOX (8443)\n\n\nOozie (11000)\n\n\nSpark HS (18080)\n\n\nNM Web (8042)\n\n\nZeppelin WebSocket (9996)\n\n\nZeppelin UI (9995)\n\n\nKibana (3080)\n\n\nElasticsearch (9200)\n\n\n\n\nConfigure instance groups\n\n\nYou have to configure the instancegroups before the provisioning. An instancegroup is defining a group of your nodes with a specified template. Usually we create instancegroups for the hostgroups defined in the blueprints.\n\n\ninstancegroup configure --instanceGroup host_group_slave_1 --nodecount 3 --templateName minviable-azure\n\n\n\n\nOther available options:\n\n\n--templateId\n Id of the template\n\n\nCreate a Hadoop cluster\n\n\nYou are almost done - two more command and this will create your Hadoop cluster on your favorite cloud provider. Same as the API, or UI this will use your \ncredential\n, \ninstancegroups\n, \nnetwork\n, \nsecuritygroup\n, and by using Azure ResourceManager will launch a cloud stack\n\n\nstack create --name my-first-stack --region WEST_US\n\n\n\n\nOnce the \nstack\n is up and running (cloud provisioning is done) it will use your selected \nblueprint\n and install your custom Hadoop cluster with the selected components and services.\n\n\ncluster create --description \nmy first cluster\n\n\n\n\n\nYou are done - you can check the progress through the Ambari UI. If you log back to Cloudbreak UI you can check the progress over there as well, and learn the IP address of Ambari.\n\n\nStop/Restart cluster and stack\n\n\nYou have the ability to \nstop your existing stack then its cluster\n if you want to suspend the work on it.\n\n\nSelect a stack for example with its name:\n\n\nstack select --name my-stack\n\n\n\n\nOther available option to define a stack is its \n--id\n (instead of the \n--name\n).\n\n\nApply the following commands to stop the previously selected stack:\n\n\ncluster stop\nstack stop\n\n\n\n\n\n\nImportant!\n The related cluster should be stopped before you can stop the stack.\n\n\n\n\nApply the following command to \nrestart the previously selected and stopped stack\n:\n\n\nstack start\n\n\n\n\nAfter the selected stack has restarted, you can \nrestart the related cluster as well\n:\n\n\ncluster start\n\n\n\n\nUpscale/Downscale cluster and stack\n\n\nYou can \nupscale your selected stack\n if you need more instances to your infrastructure:\n\n\nstack node --ADD --instanceGroup host_group_slave_1 --adjustment 6\n\n\n\n\nOther available options:\n\n\n--withClusterUpScale\n indicates cluster upscale after stack upscale\nor you can upscale the related cluster separately as well:\n\n\ncluster node --ADD --hostgroup host_group_slave_1 --adjustment 6\n\n\n\n\nApply the following command to \ndownscale the previously selected stack\n:\n\n\nstack node --REMOVE  --instanceGroup host_group_slave_1 --adjustment -2\n\n\n\n\nand the related cluster separately:\n\n\ncluster node --REMOVE  --hostgroup host_group_slave_1 --adjustment -2\n\n\n\n\nSilent mode\n\n\nWith Cloudbreak shell you can execute script files as well. A script file contains cloudbreak shell commands and can be executed with the \nscript\n cloudbreak shell command\n\n\nscript \nyour script file\n\n\n\n\n\nor with the \ncbd util cloudbreak-shell-quiet\n cbd command:\n\n\ncbd util cloudbreak-shell-quiet \n example.sh\n\n\n\n\nExample\n\n\nThe following example creates a hadoop cluster with \nhdp-small-default\n blueprint on STANDARD_D3 instances with 2X100G attached disks on \ndefault-azure-network\n network using \nall-services-port\n security group. You should copy your ssh public key file into your cbd working directory with name \nid_rsa.pub\n and change the \n...\n parts with your azure credential details.\n\n\ncredential create --AZURE --description \ncredential description\n --name myazurecredential --subscriptionId \nyour Azure subscription id\n --appId \nyour Azure application id\n --tenantId \nyour tenant id\n --password \nyour Azure application password\n --sshKeyPath id_rsa.pub\ncredential select --name myazurecredential\ntemplate create --AZURE --name azuretemplate --description azure-template --instanceType STANDARD_D3 --volumeSize 100 --volumeCount 2\nblueprint select --name hdp-small-default\ninstancegroup configure --instanceGroup cbgateway --nodecount 1 --templateName azuretemplate\ninstancegroup configure --instanceGroup host_group_master_1 --nodecount 1 --templateName azuretemplate\ninstancegroup configure --instanceGroup host_group_master_2 --nodecount 1 --templateName azuretemplate\ninstancegroup configure --instanceGroup host_group_master_3 --nodecount 1 --templateName azuretemplate\ninstancegroup configure --instanceGroup host_group_client_1  --nodecount 1 --templateName azuretemplate\ninstancegroup configure --instanceGroup host_group_slave_1 --nodecount 3 --templateName azuretemplate\nnetwork select --name default-azure-network\nsecuritygroup select --name all-services-port\nstack create --name my-first-stack --region WEST_US\ncluster create --description \nMy first cluster\n\n\n\n\n\nNext steps\n\n\nCongrats! Your cluster should now be up and running. To learn more about it we have some \ninteresting insights\n about Cloudbreak clusters.", 
            "title": "Provisioning - CLI"
        }, 
        {
            "location": "/azure_cb_shell/#interactive-mode", 
            "text": "Start the shell with  cbd util cloudbreak-shell . This will launch the Cloudbreak shell inside a Docker container and you are ready to start using it.  You have to copy files into the cbd working directory, which you would like to use from shell. For example if your  cbd  working directory is  ~/prj/cbd  then copy your blueprint and public ssh key file into this directory. You can refer to these files with their names from the shell.  Create a cloud credential  credential create --AZURE --description  credential description  --name myazurecredential --subscriptionId  your Azure subscription id  --appId  your Azure application id  --tenantId  your tenant id  --password  your Azure application password  --sshKeyPath  path of your public SSH key file    Cloudbreak is supporting simple rsa public key instead of X509 certificate file after 1.0.4 version   Alternatively you can upload your public key from an url as well, by using the  \u2014sshKeyUrl  switch. You can check whether the credential was creates successfully by using the  credential list  command.\nYou can switch between your cloud credential - when you\u2019d like to use one and act with that you will have to use:  credential select --name myazurecredential  You can delete your cloud credential - when you\u2019d like to delete one you will have to use:  credential delete --name myazurecredential  You can show your cloud credential - when you\u2019d like to show one you will have to use:  credential show --name myazurecredential  Create a template  A template gives developers and systems administrators an easy way to create and manage a collection of cloud infrastructure related resources, maintaining and updating them in an orderly and predictable fashion. A template can be used repeatedly to create identical copies of the same stack (or to use as a foundation to start a new stack).  template create --AZURE --name azuretemplate --description azure-template --instanceType STANDARD_D3 --volumeSize 100 --volumeCount 2  You can check whether the template was created successfully by using the  template list  or  template show  command.  You can delete your cloud template - when you\u2019d like to delete one you will have to use:  template delete --name azuretemplate  Create or select a blueprint  You can define Ambari blueprints with cloudbreak-shell:  blueprint add --name myblueprint --description myblueprint-description --file  the path of the blueprint   Other available options:  --url  the url of the blueprint  --publicInAccount  flags if the network is public in the account  We ship default Ambari blueprints with Cloudbreak. You can use these blueprints or add yours. To see the available blueprints and use one of them please use:  blueprint list\n\nblueprint select --name hdp-small-default  Create a network  A network gives developers and systems administrators an easy way to create and manage a collection of cloud infrastructure related networking, maintaining and updating them in an orderly and predictable fashion. A network can be used repeatedly to create identical copies of the same stack (or to use as a foundation to start a new stack).  network create --AZURE --name azurenetwork --description azure-network --subnet 10.0.0.0/16 --addressPrefix 10.0.0.0/8  Other available options:  --publicInAccount  flags if the network is public in the account  There is a default network with name  default-azure-network  with 10.0.0.0/16 subnet and 10.0.0.0/8 addressPrefix.  You can check whether the network was created successfully by using the  network list  command. Check the network and select it if you are happy with it:  network show --name azurenetwork\n\nnetwork select --name azurenetwork  Create a security group  A security group gives developers and systems administrators an easy way to create and manage a collection of cloud infrastructure related security rules.  securitygroup create --name secgroup_example --description securitygroup-example --rules 0.0.0.0/0:tcp:8080,9090;10.0.33.0/24:tcp:1234,1235  You can check whether the security group was created successfully by using the  securitygroup list  command. Check the security group and select it if you are happy with it:  securitygroup show --name secgroup_example\n\nsecuritygroup select --name secgroup_example  There are two default security groups defined:  all-services-port  and  only-ssh-and-ssl  only-ssh-and-ssl:  all ports are locked down (you can't access Hadoop services outside of the VPC)   SSH (22)  HTTPS (443)   all-services-port:  all Hadoop services + SSH/HTTP are accessible by default:   SSH (22)  HTTPS (443)  Ambari (8080)  Consul (8500)  NN (50070)  RM Web (8088)  Scheduler (8030RM)  IPC (8050RM)  Job history server (19888)  HBase master (60000)  HBase master web (60010)  HBase RS (16020)  HBase RS info (60030)  Falcon (15000)  Storm (8744)  Hive metastore (9083)  Hive server (10000)  Hive server HTTP (10001)  Accumulo master (9999)  Accumulo Tserver (9997)  Atlas (21000)  KNOX (8443)  Oozie (11000)  Spark HS (18080)  NM Web (8042)  Zeppelin WebSocket (9996)  Zeppelin UI (9995)  Kibana (3080)  Elasticsearch (9200)   Configure instance groups  You have to configure the instancegroups before the provisioning. An instancegroup is defining a group of your nodes with a specified template. Usually we create instancegroups for the hostgroups defined in the blueprints.  instancegroup configure --instanceGroup host_group_slave_1 --nodecount 3 --templateName minviable-azure  Other available options:  --templateId  Id of the template  Create a Hadoop cluster  You are almost done - two more command and this will create your Hadoop cluster on your favorite cloud provider. Same as the API, or UI this will use your  credential ,  instancegroups ,  network ,  securitygroup , and by using Azure ResourceManager will launch a cloud stack  stack create --name my-first-stack --region WEST_US  Once the  stack  is up and running (cloud provisioning is done) it will use your selected  blueprint  and install your custom Hadoop cluster with the selected components and services.  cluster create --description  my first cluster   You are done - you can check the progress through the Ambari UI. If you log back to Cloudbreak UI you can check the progress over there as well, and learn the IP address of Ambari.  Stop/Restart cluster and stack  You have the ability to  stop your existing stack then its cluster  if you want to suspend the work on it.  Select a stack for example with its name:  stack select --name my-stack  Other available option to define a stack is its  --id  (instead of the  --name ).  Apply the following commands to stop the previously selected stack:  cluster stop\nstack stop   Important!  The related cluster should be stopped before you can stop the stack.   Apply the following command to  restart the previously selected and stopped stack :  stack start  After the selected stack has restarted, you can  restart the related cluster as well :  cluster start  Upscale/Downscale cluster and stack  You can  upscale your selected stack  if you need more instances to your infrastructure:  stack node --ADD --instanceGroup host_group_slave_1 --adjustment 6  Other available options:  --withClusterUpScale  indicates cluster upscale after stack upscale\nor you can upscale the related cluster separately as well:  cluster node --ADD --hostgroup host_group_slave_1 --adjustment 6  Apply the following command to  downscale the previously selected stack :  stack node --REMOVE  --instanceGroup host_group_slave_1 --adjustment -2  and the related cluster separately:  cluster node --REMOVE  --hostgroup host_group_slave_1 --adjustment -2", 
            "title": "Interactive mode"
        }, 
        {
            "location": "/azure_cb_shell/#silent-mode", 
            "text": "With Cloudbreak shell you can execute script files as well. A script file contains cloudbreak shell commands and can be executed with the  script  cloudbreak shell command  script  your script file   or with the  cbd util cloudbreak-shell-quiet  cbd command:  cbd util cloudbreak-shell-quiet   example.sh", 
            "title": "Silent mode"
        }, 
        {
            "location": "/azure_cb_shell/#example", 
            "text": "The following example creates a hadoop cluster with  hdp-small-default  blueprint on STANDARD_D3 instances with 2X100G attached disks on  default-azure-network  network using  all-services-port  security group. You should copy your ssh public key file into your cbd working directory with name  id_rsa.pub  and change the  ...  parts with your azure credential details.  credential create --AZURE --description  credential description  --name myazurecredential --subscriptionId  your Azure subscription id  --appId  your Azure application id  --tenantId  your tenant id  --password  your Azure application password  --sshKeyPath id_rsa.pub\ncredential select --name myazurecredential\ntemplate create --AZURE --name azuretemplate --description azure-template --instanceType STANDARD_D3 --volumeSize 100 --volumeCount 2\nblueprint select --name hdp-small-default\ninstancegroup configure --instanceGroup cbgateway --nodecount 1 --templateName azuretemplate\ninstancegroup configure --instanceGroup host_group_master_1 --nodecount 1 --templateName azuretemplate\ninstancegroup configure --instanceGroup host_group_master_2 --nodecount 1 --templateName azuretemplate\ninstancegroup configure --instanceGroup host_group_master_3 --nodecount 1 --templateName azuretemplate\ninstancegroup configure --instanceGroup host_group_client_1  --nodecount 1 --templateName azuretemplate\ninstancegroup configure --instanceGroup host_group_slave_1 --nodecount 3 --templateName azuretemplate\nnetwork select --name default-azure-network\nsecuritygroup select --name all-services-port\nstack create --name my-first-stack --region WEST_US\ncluster create --description  My first cluster", 
            "title": "Example"
        }, 
        {
            "location": "/azure_cb_shell/#next-steps", 
            "text": "Congrats! Your cluster should now be up and running. To learn more about it we have some  interesting insights  about Cloudbreak clusters.", 
            "title": "Next steps"
        }, 
        {
            "location": "/gcp/", 
            "text": "Google Setup\n\n\nSetup Cloudbreak Deployer\n\n\nIf you already have Cloudbreak Deployer either by \nusing the GCP Cloud Images\n or by \ninstalling the Cloudbreak Deployer\n manually on your own VM,\nyou can start to setup the Cloudbreak Application with the deployer.\n\n\nCreate and open the \ncloudbreak-deployment\n directory:\n\n\ncd cloudbreak-deployment\n\n\n\n\nThis is the directory of the config files and the supporting binaries that will be downloaded by Cloudbreak deployer.\n\n\nInitialize your Profile\n\n\nFirst initialize cbd by creating a \nProfile\n file:\n\n\ncbd init\n\n\n\n\nIt will create a \nProfile\n file in the current directory. Please edit the file - the only required\nconfiguration is the \nPUBLIC_IP\n. This IP will be used to access the Cloudbreak UI\n(called Uluwatu). In some cases the \ncbd\n tool tries to guess it, if can't than will give a hint.\n\n\nGenerate your Profile\n\n\nYou are done with the configuration of Cloudbreak deployer. The last thing you have to do is to regenerate the configurations in order to take effect.\n\n\nrm *.yml\ncbd generate\n\n\n\n\nThis command applies the following steps: \n\n\n\n\ncreates the \ndocker-compose.yml\n file that describes the configuration of all the Docker containers needed for the Cloudbreak deployment.\n\n\ncreates the \nuaa.yml\n file that holds the configuration of the identity server used to authenticate users to Cloudbreak.\n\n\n\n\nStart Cloudbreak\n\n\nTo start the Cloudbreak application use the following command.\nThis will start all the Docker containers and initialize the application. It will take a few minutes until all the services start.\n\n\ncbd start\n\n\n\n\n\n\nLaunching it first will take more time as it downloads all the docker images needed by Cloudbreak.\n\n\n\n\nAfter the \ncbd start\n command finishes you can check the logs of the Cloudbreak server with this command:\n\n\ncbd logs cloudbreak\n\n\n\n\n\n\nCloudbreak server should start within a minute - you should see a line like this: \nStarted CloudbreakApplication in 36.823 seconds\n\n\n\n\nNext steps\n\n\nOnce Cloudbreak is up and running you should check out the \nProvisioning Prerequisites\n needed to create Google Cloud clusters with Cloudbreak.", 
            "title": "Setup"
        }, 
        {
            "location": "/gcp/#google-setup", 
            "text": "", 
            "title": "Google Setup"
        }, 
        {
            "location": "/gcp/#setup-cloudbreak-deployer", 
            "text": "If you already have Cloudbreak Deployer either by  using the GCP Cloud Images  or by  installing the Cloudbreak Deployer  manually on your own VM,\nyou can start to setup the Cloudbreak Application with the deployer.  Create and open the  cloudbreak-deployment  directory:  cd cloudbreak-deployment  This is the directory of the config files and the supporting binaries that will be downloaded by Cloudbreak deployer.  Initialize your Profile  First initialize cbd by creating a  Profile  file:  cbd init  It will create a  Profile  file in the current directory. Please edit the file - the only required\nconfiguration is the  PUBLIC_IP . This IP will be used to access the Cloudbreak UI\n(called Uluwatu). In some cases the  cbd  tool tries to guess it, if can't than will give a hint.  Generate your Profile  You are done with the configuration of Cloudbreak deployer. The last thing you have to do is to regenerate the configurations in order to take effect.  rm *.yml\ncbd generate  This command applies the following steps:    creates the  docker-compose.yml  file that describes the configuration of all the Docker containers needed for the Cloudbreak deployment.  creates the  uaa.yml  file that holds the configuration of the identity server used to authenticate users to Cloudbreak.   Start Cloudbreak  To start the Cloudbreak application use the following command.\nThis will start all the Docker containers and initialize the application. It will take a few minutes until all the services start.  cbd start   Launching it first will take more time as it downloads all the docker images needed by Cloudbreak.   After the  cbd start  command finishes you can check the logs of the Cloudbreak server with this command:  cbd logs cloudbreak   Cloudbreak server should start within a minute - you should see a line like this:  Started CloudbreakApplication in 36.823 seconds   Next steps  Once Cloudbreak is up and running you should check out the  Provisioning Prerequisites  needed to create Google Cloud clusters with Cloudbreak.", 
            "title": "Setup Cloudbreak Deployer"
        }, 
        {
            "location": "/gcp_pre_prov/", 
            "text": "Provisioning Prerequisites\n\n\nCreating a Google Cloud Service Account\n\n\nFollow the \ninstructions\n in Google Cloud's documentation to create a \nService account\n and \nGenerate a new P12 key\n.\n\n\nMake sure that at API level (\nAPIs and auth\n menu) you have enabled:\n\n\n\n\nGoogle Compute Engine\n\n\nGoogle Compute Engine Instance Group Manager API\n\n\nGoogle Compute Engine Instance Groups API\n\n\nBigQuery API\n\n\nGoogle Cloud Deployment Manager API\n\n\nGoogle Cloud DNS API\n\n\nGoogle Cloud SQL\n\n\nGoogle Cloud Storage\n\n\nGoogle Cloud Storage JSON API\n\n\n\n\n\n\nIf you enabled every API then you have to wait about \n10 minutes\n for the provider.\n\n\n\n\nWhen creating GCP credentials in Cloudbreak you will have to provide the email address of the Service Account and the project ID (from Google Developers Console - Projects) where the service account is created. You'll also have to upload the generated P12 file and provide an OpenSSH formatted public key that will be used as an SSH key.\n\n\nOnce your prerequisites created you can use the \nCloudbreak UI\n or use the \nCloudbreak shell\n.\n\n\nGenerate a new SSH key\n\n\nAll the instances created by Cloudbreak are configured to allow key-based SSH,\nso you'll need to provide an SSH public key that can be used later to SSH onto the instances in the clusters you'll create with Cloudbreak.\nYou can use one of your existing keys or you can generate a new one.\n\n\nTo generate a new SSH keypair:\n\n\nssh-keygen -t rsa -b 4096 -C \nyour_email@example.com\n\n# Creates a new ssh key, using the provided email as a label\n# Generating public/private rsa key pair.\n\n\n\n\n# Enter file in which to save the key (/Users/you/.ssh/id_rsa): [Press enter]\nYou'll be asked to enter a passphrase, but you can leave it empty.\n\n# Enter passphrase (empty for no passphrase): [Type a passphrase]\n# Enter same passphrase again: [Type passphrase again]\n\n\n\n\nAfter you enter a passphrase the keypair is generated. The output should look something like below.\n\n\n# Your identification has been saved in /Users/you/.ssh/id_rsa.\n# Your public key has been saved in /Users/you/.ssh/id_rsa.pub.\n# The key fingerprint is:\n# 01:0f:f4:3b:ca:85:sd:17:sd:7d:sd:68:9d:sd:a2:sd your_email@example.com\n\n\n\n\nLater you'll need to pass the \n.pub\n file's contents to Cloudbreak and use the private part to SSH to the instances.\n\n\nNext steps\n\n\nAfter these prerequisites are done you can move on to create clusters on the \nUI\n or with the \nShell\n.", 
            "title": "Provisioning Prerequisites"
        }, 
        {
            "location": "/gcp_pre_prov/#provisioning-prerequisites", 
            "text": "", 
            "title": "Provisioning Prerequisites"
        }, 
        {
            "location": "/gcp_pre_prov/#creating-a-google-cloud-service-account", 
            "text": "Follow the  instructions  in Google Cloud's documentation to create a  Service account  and  Generate a new P12 key .  Make sure that at API level ( APIs and auth  menu) you have enabled:   Google Compute Engine  Google Compute Engine Instance Group Manager API  Google Compute Engine Instance Groups API  BigQuery API  Google Cloud Deployment Manager API  Google Cloud DNS API  Google Cloud SQL  Google Cloud Storage  Google Cloud Storage JSON API    If you enabled every API then you have to wait about  10 minutes  for the provider.   When creating GCP credentials in Cloudbreak you will have to provide the email address of the Service Account and the project ID (from Google Developers Console - Projects) where the service account is created. You'll also have to upload the generated P12 file and provide an OpenSSH formatted public key that will be used as an SSH key.  Once your prerequisites created you can use the  Cloudbreak UI  or use the  Cloudbreak shell .", 
            "title": "Creating a Google Cloud Service Account"
        }, 
        {
            "location": "/gcp_pre_prov/#generate-a-new-ssh-key", 
            "text": "All the instances created by Cloudbreak are configured to allow key-based SSH,\nso you'll need to provide an SSH public key that can be used later to SSH onto the instances in the clusters you'll create with Cloudbreak.\nYou can use one of your existing keys or you can generate a new one.  To generate a new SSH keypair:  ssh-keygen -t rsa -b 4096 -C  your_email@example.com \n# Creates a new ssh key, using the provided email as a label\n# Generating public/private rsa key pair.  # Enter file in which to save the key (/Users/you/.ssh/id_rsa): [Press enter]\nYou'll be asked to enter a passphrase, but you can leave it empty.\n\n# Enter passphrase (empty for no passphrase): [Type a passphrase]\n# Enter same passphrase again: [Type passphrase again]  After you enter a passphrase the keypair is generated. The output should look something like below.  # Your identification has been saved in /Users/you/.ssh/id_rsa.\n# Your public key has been saved in /Users/you/.ssh/id_rsa.pub.\n# The key fingerprint is:\n# 01:0f:f4:3b:ca:85:sd:17:sd:7d:sd:68:9d:sd:a2:sd your_email@example.com  Later you'll need to pass the  .pub  file's contents to Cloudbreak and use the private part to SSH to the instances.", 
            "title": "Generate a new SSH key"
        }, 
        {
            "location": "/gcp_pre_prov/#next-steps", 
            "text": "After these prerequisites are done you can move on to create clusters on the  UI  or with the  Shell .", 
            "title": "Next steps"
        }, 
        {
            "location": "/gcp_cb_ui/", 
            "text": "Provisioning via Browser\n\n\nYou can log into the Cloudbreak application at http://PUBLIC_IP:3000.\n\n\nThe main goal of the Cloudbreak UI is to easily create clusters on your own cloud provider account.\nThis description details the GCP setup - if you'd like to use a different cloud provider check out its manual.\n\n\nThis document explains the four steps that need to be followed to create Cloudbreak clusters from the UI:\n\n\n\n\nconnect your GCP account with Cloudbreak\n\n\ncreate some template resources on the UI that describe the infrastructure of your clusters\n\n\ncreate a blueprint that describes the HDP services in your clusters and add some recipes for customization\n\n\nlaunch the cluster itself based on these template resources\n\n\n\n\nManage cloud credentials\n\n\nYou can now log into the Cloudbreak application at http://PUBLIC_IP:3000. Once logged in go to \nManage credentials\n. Using manage credentials will  link your cloud account with the Cloudbreak account.\n\n\nName:\n name of your credential\n\n\nDescription:\n short description of your linked credential\n\n\nProject Id:\n your GCP Project id - see Accounts\n\n\nService Account Email Address:\n your GCP service account mail address - see Accounts\n\n\nService Account private (p12) key:\n your GCP service account generated private key - see Accounts\n\n\nSSH public key:\n the SSH public key in OpenSSH format that's private keypair can be used to \nlog into the launched instances\n later\n\n\nPublic in account:\n share it with others in the account\n\n\nThe ssh username is \ncloudbreak\n.\n\n\nInfrastructure templates\n\n\nAfter your GCP account is linked to Cloudbreak you can start creating templates that describe your clusters' infrastructure:\n\n\n\n\nresources\n\n\nnetworks\n\n\nsecurity groups\n\n\n\n\nWhen you create a template, Cloudbreak \ndoesn't make any requests\n to GCP.\nResources are only created on GCP after the \nCreate cluster\n button is pushed.\nThese templates are saved to Cloudbreak's database and can be reused with multiple clusters to describe the infrastructure.\n\n\nManage resources\n\n\nUsing manage resources you can create infrastructure templates. Templates describes the infrastructure where the HDP cluster will be provisioned. We support heterogenous clusters - this means that one cluster can be built by combining different templates.\n\n\nName:\n name of your template\n\n\nDescription:\n short description of your template\n\n\nInstance type:\n the Google Cloud instance type to be used - we suggest to use at least n1-standard-4 instances\n\n\nVolume type:\n option to choose are SSD, regular Magnetic\n\n\nAttached volumes per instance:\n the number of disks to be attached\n\n\nVolume size (GB):\n the size of the attached disks (in GB)\n\n\nPublic in account:\n share it with others in the account\n\n\nManage blueprints\n\n\nBlueprints are your declarative definition of a Hadoop cluster.\n\n\nName:\n name of your blueprint\n\n\nDescription:\n short description of your blueprint\n\n\nSource URL:\n you can add a blueprint by pointing to a URL. As an example you can use this \nblueprint\n.\n\n\nManual copy:\n you can copy paste your blueprint in this text area\n\n\nPublic in account:\n share it with others in the account\n\n\nManage networks\n\n\nManage networks allows you to create or reuse existing networks and configure them.\n\n\nName:\n name of the network\n\n\nDescription:\n short description of your network\n\n\nSubnet (CIDR):\n a subnet in the VPC with CIDR block\n\n\nPublic in account:\n share it with others in the account\n\n\nSecurity groups\n\n\nThey describe the allowed inbound traffic to the instances in the cluster.\nCurrently only one security group template can be selected for a Cloudbreak cluster and all the instances have a public IP address so all the instances in the cluster will belong to the same security group.\nThis may change in a later release.\n\n\nYou can define your own security group by adding all the ports, protocols and CIDR range you'd like to use. 443 needs to be there in every security group otherwise Cloudbreak won't be able to communicate with the provisioned cluster. The rules defined here doesn't need to contain the internal rules, those are automatically added by Cloudbreak to the security group on GCP.\n\n\nYou can also use the two pre-defined security groups in Cloudbreak:\n\n\nonly-ssh-and-ssl:\n all ports are locked down except for SSH and gateway HTTPS (you can't access Hadoop services outside of the VPC):\n\n\n\n\nSSH (22)\n\n\nHTTPS (443)\n\n\n\n\nall-services-port:\n all Hadoop services and SSH/gateway HTTPS are accessible by default:\n\n\n\n\nSSH (22)\n\n\nHTTPS (443)\n\n\nAmbari (8080)\n\n\nConsul (8500)\n\n\nNN (50070)\n\n\nRM Web (8088)\n\n\nScheduler (8030RM)\n\n\nIPC (8050RM)\n\n\nJob history server (19888)\n\n\nHBase master (60000)\n\n\nHBase master web (60010)\n\n\nHBase RS (16020)\n\n\nHBase RS info (60030)\n\n\nFalcon (15000)\n\n\nStorm (8744)\n\n\nHive metastore (9083)\n\n\nHive server (10000)\n\n\nHive server HTTP (10001)\n\n\nAccumulo master (9999)\n\n\nAccumulo Tserver (9997)\n\n\nAtlas (21000)\n\n\nKNOX (8443)\n\n\nOozie (11000)\n\n\nSpark HS (18080)\n\n\nNM Web (8042)\n\n\nZeppelin WebSocket (9996)\n\n\nZeppelin UI (9995)\n\n\nKibana (3080)\n\n\nElasticsearch (9200)\n\n\n\n\nIf \nPublic in account\n is checked all the users belonging to your account will be able to use this security group template to create clusters, but cannot delete or modify it.\n\n\n\n\nNote\n that the security groups are \nnot created\n on GCP after the \nCreate Security Group\n button is pushed, only \nafter the cluster provisioning starts with the selected security group template.\n\n\n\n\nCluster installation\n\n\nThis section describes\n\n\nBlueprints\n\n\nBlueprints are your declarative definition of a Hadoop cluster. These are the same blueprints that are \nused by Ambari\n.\n\n\nYou can use the 3 default blueprints pre-defined in Cloudbreak or you can create your own.\nBlueprints can be added from an URL or the whole JSON can be copied to the \nManual copy\n field.\n\n\nThe hostgroups added in the JSON will be mapped to a set of instances when starting the cluster and the services and components defined in the hostgroup will be installed on the corresponding nodes.\nIt is not necessary to define all the configuration fields in the blueprints - if a configuration is missing, Ambari will fill that with a default value.\nThe configurations defined in the blueprint can also be modified later from the Ambari UI.\n\n\nIf \nPublic in account\n is checked all the users belonging to your account will be able to use this blueprint to create clusters, but cannot delete or modify it.\n\n\nA blueprint can be exported from a running Ambari cluster that can be reused in Cloudbreak with slight modifications.\nThere is no automatic way to modify an exported blueprint and make it instantly usable in Cloudbreak, the modifications have to be done manually.\nWhen the blueprint is exported some configurations will have for example hardcoded domain names, or memory configurations that won't be applicable to the Cloudbreak cluster.\n\n\nCluster customization\n\n\nSometimes it can be useful to define some custom scripts that run during cluster creation and add some additional functionality.\nFor example it can be a service you'd like to install but it's not supported by Ambari or some script that automatically downloads some data to the necessary nodes.\nThe most notable example is Ranger setup: it has a prerequisite of a running database when Ranger Admin is installing.\nA PostgreSQL database can be easily started and configured with a recipe before the blueprint installation starts.\n\n\nTo learn more about these so called \nRecipes\n, and to check out the Ranger database recipe, take a look at the \nCluster customization\n part of the documentation.\n\n\nCluster deployment\n\n\nAfter all the templates are configured you can deploy a new HDP cluster. Start by selecting a previously created credential in the header.\nClick on \ncreate cluster\n, give it a \nName\n, select a \nRegion\n where the cluster infrastructure will be provisioned and select one of the \nNetworks\n and \nSecurity Groups\n created earlier.\nAfter you've selected a \nBlueprint\n as well you should be able to configure the \nTemplate resources\n and the number of nodes for all of the hostgroups in the blueprint.\n\n\nIf \nPublic in account\n is checked all the users belonging to your account will be able to see the newly created cluster on the UI, but cannot delete or modify it.\n\n\nIf \nEnable security\n is checked as well, Cloudbreak will install KDC and the cluster will be Kerberized. See more about it in the \nKerberos\n section of this documentation.\n\n\nAfter the \ncreate and start cluster\n button is pushed Cloudbreak will start to create resources on your GCP account.\n\n\n\n\nImportant\n Always use Cloudbreak to delete the cluster, or if that fails for some reason always try to delete \nthe Google Cloud first.\n\n\n\n\nAdvanced options\n\n\nThere are some advanced features when deploying a new cluster, these are the following:\n\n\nMinimum cluster size:\n the provisioning strategy in case of the cloud provider can't allocate all the requested nodes\n\n\nValidate blueprint:\n feature to validate or not the Ambari blueprint. By default is switched on.\n\n\nNext steps\n\n\nCongrats! Your cluster should now be up and running. To learn more about it we have some \ninteresting insights\n about Cloudbreak clusters.", 
            "title": "Provisioning - UI"
        }, 
        {
            "location": "/gcp_cb_ui/#provisioning-via-browser", 
            "text": "You can log into the Cloudbreak application at http://PUBLIC_IP:3000.  The main goal of the Cloudbreak UI is to easily create clusters on your own cloud provider account.\nThis description details the GCP setup - if you'd like to use a different cloud provider check out its manual.  This document explains the four steps that need to be followed to create Cloudbreak clusters from the UI:   connect your GCP account with Cloudbreak  create some template resources on the UI that describe the infrastructure of your clusters  create a blueprint that describes the HDP services in your clusters and add some recipes for customization  launch the cluster itself based on these template resources", 
            "title": "Provisioning via Browser"
        }, 
        {
            "location": "/gcp_cb_ui/#manage-cloud-credentials", 
            "text": "You can now log into the Cloudbreak application at http://PUBLIC_IP:3000. Once logged in go to  Manage credentials . Using manage credentials will  link your cloud account with the Cloudbreak account.  Name:  name of your credential  Description:  short description of your linked credential  Project Id:  your GCP Project id - see Accounts  Service Account Email Address:  your GCP service account mail address - see Accounts  Service Account private (p12) key:  your GCP service account generated private key - see Accounts  SSH public key:  the SSH public key in OpenSSH format that's private keypair can be used to  log into the launched instances  later  Public in account:  share it with others in the account  The ssh username is  cloudbreak .", 
            "title": "Manage cloud credentials"
        }, 
        {
            "location": "/gcp_cb_ui/#infrastructure-templates", 
            "text": "After your GCP account is linked to Cloudbreak you can start creating templates that describe your clusters' infrastructure:   resources  networks  security groups   When you create a template, Cloudbreak  doesn't make any requests  to GCP.\nResources are only created on GCP after the  Create cluster  button is pushed.\nThese templates are saved to Cloudbreak's database and can be reused with multiple clusters to describe the infrastructure.  Manage resources  Using manage resources you can create infrastructure templates. Templates describes the infrastructure where the HDP cluster will be provisioned. We support heterogenous clusters - this means that one cluster can be built by combining different templates.  Name:  name of your template  Description:  short description of your template  Instance type:  the Google Cloud instance type to be used - we suggest to use at least n1-standard-4 instances  Volume type:  option to choose are SSD, regular Magnetic  Attached volumes per instance:  the number of disks to be attached  Volume size (GB):  the size of the attached disks (in GB)  Public in account:  share it with others in the account  Manage blueprints  Blueprints are your declarative definition of a Hadoop cluster.  Name:  name of your blueprint  Description:  short description of your blueprint  Source URL:  you can add a blueprint by pointing to a URL. As an example you can use this  blueprint .  Manual copy:  you can copy paste your blueprint in this text area  Public in account:  share it with others in the account  Manage networks  Manage networks allows you to create or reuse existing networks and configure them.  Name:  name of the network  Description:  short description of your network  Subnet (CIDR):  a subnet in the VPC with CIDR block  Public in account:  share it with others in the account  Security groups  They describe the allowed inbound traffic to the instances in the cluster.\nCurrently only one security group template can be selected for a Cloudbreak cluster and all the instances have a public IP address so all the instances in the cluster will belong to the same security group.\nThis may change in a later release.  You can define your own security group by adding all the ports, protocols and CIDR range you'd like to use. 443 needs to be there in every security group otherwise Cloudbreak won't be able to communicate with the provisioned cluster. The rules defined here doesn't need to contain the internal rules, those are automatically added by Cloudbreak to the security group on GCP.  You can also use the two pre-defined security groups in Cloudbreak:  only-ssh-and-ssl:  all ports are locked down except for SSH and gateway HTTPS (you can't access Hadoop services outside of the VPC):   SSH (22)  HTTPS (443)   all-services-port:  all Hadoop services and SSH/gateway HTTPS are accessible by default:   SSH (22)  HTTPS (443)  Ambari (8080)  Consul (8500)  NN (50070)  RM Web (8088)  Scheduler (8030RM)  IPC (8050RM)  Job history server (19888)  HBase master (60000)  HBase master web (60010)  HBase RS (16020)  HBase RS info (60030)  Falcon (15000)  Storm (8744)  Hive metastore (9083)  Hive server (10000)  Hive server HTTP (10001)  Accumulo master (9999)  Accumulo Tserver (9997)  Atlas (21000)  KNOX (8443)  Oozie (11000)  Spark HS (18080)  NM Web (8042)  Zeppelin WebSocket (9996)  Zeppelin UI (9995)  Kibana (3080)  Elasticsearch (9200)   If  Public in account  is checked all the users belonging to your account will be able to use this security group template to create clusters, but cannot delete or modify it.   Note  that the security groups are  not created  on GCP after the  Create Security Group  button is pushed, only \nafter the cluster provisioning starts with the selected security group template.", 
            "title": "Infrastructure templates"
        }, 
        {
            "location": "/gcp_cb_ui/#cluster-installation", 
            "text": "This section describes  Blueprints  Blueprints are your declarative definition of a Hadoop cluster. These are the same blueprints that are  used by Ambari .  You can use the 3 default blueprints pre-defined in Cloudbreak or you can create your own.\nBlueprints can be added from an URL or the whole JSON can be copied to the  Manual copy  field.  The hostgroups added in the JSON will be mapped to a set of instances when starting the cluster and the services and components defined in the hostgroup will be installed on the corresponding nodes.\nIt is not necessary to define all the configuration fields in the blueprints - if a configuration is missing, Ambari will fill that with a default value.\nThe configurations defined in the blueprint can also be modified later from the Ambari UI.  If  Public in account  is checked all the users belonging to your account will be able to use this blueprint to create clusters, but cannot delete or modify it.  A blueprint can be exported from a running Ambari cluster that can be reused in Cloudbreak with slight modifications.\nThere is no automatic way to modify an exported blueprint and make it instantly usable in Cloudbreak, the modifications have to be done manually.\nWhen the blueprint is exported some configurations will have for example hardcoded domain names, or memory configurations that won't be applicable to the Cloudbreak cluster.  Cluster customization  Sometimes it can be useful to define some custom scripts that run during cluster creation and add some additional functionality.\nFor example it can be a service you'd like to install but it's not supported by Ambari or some script that automatically downloads some data to the necessary nodes.\nThe most notable example is Ranger setup: it has a prerequisite of a running database when Ranger Admin is installing.\nA PostgreSQL database can be easily started and configured with a recipe before the blueprint installation starts.  To learn more about these so called  Recipes , and to check out the Ranger database recipe, take a look at the  Cluster customization  part of the documentation.", 
            "title": "Cluster installation"
        }, 
        {
            "location": "/gcp_cb_ui/#cluster-deployment", 
            "text": "After all the templates are configured you can deploy a new HDP cluster. Start by selecting a previously created credential in the header.\nClick on  create cluster , give it a  Name , select a  Region  where the cluster infrastructure will be provisioned and select one of the  Networks  and  Security Groups  created earlier.\nAfter you've selected a  Blueprint  as well you should be able to configure the  Template resources  and the number of nodes for all of the hostgroups in the blueprint.  If  Public in account  is checked all the users belonging to your account will be able to see the newly created cluster on the UI, but cannot delete or modify it.  If  Enable security  is checked as well, Cloudbreak will install KDC and the cluster will be Kerberized. See more about it in the  Kerberos  section of this documentation.  After the  create and start cluster  button is pushed Cloudbreak will start to create resources on your GCP account.   Important  Always use Cloudbreak to delete the cluster, or if that fails for some reason always try to delete \nthe Google Cloud first.   Advanced options  There are some advanced features when deploying a new cluster, these are the following:  Minimum cluster size:  the provisioning strategy in case of the cloud provider can't allocate all the requested nodes  Validate blueprint:  feature to validate or not the Ambari blueprint. By default is switched on.", 
            "title": "Cluster deployment"
        }, 
        {
            "location": "/gcp_cb_ui/#next-steps", 
            "text": "Congrats! Your cluster should now be up and running. To learn more about it we have some  interesting insights  about Cloudbreak clusters.", 
            "title": "Next steps"
        }, 
        {
            "location": "/gcp_cb_shell/", 
            "text": "Interactive mode\n\n\nStart the shell with \ncbd util cloudbreak-shell\n. This will launch the Cloudbreak shell inside a Docker container and you are ready to start using it.\n\n\nYou have to copy files into the cbd working directory, which you would like to use from shell. For example if your \ncbd\n working directory is \n~/prj/cbd\n then copy your blueprint and public ssh key file into this directory. You can refer to these files with their names from the shell.\n\n\nCreate a cloud credential\n\n\nIn order to start using Cloudbreak to provision a cluster in Google Cloud you will need to have a GCP credential. If you do not want to Cloubreak to reach your Google Cloud resources then you have to delete the service account.\n\n\ncredential create --GCP --description \nshort description of your linked credential\n --name my-gcp-credential --projectId \nyour gcp projectid\n --serviceAccountId \nyour GCP service account mail address\n --serviceAccountPrivateKeyPath \npath of your GCP service account generated private key\n --sshKeyPath \npath of your GCP public key\n\n\n\n\n\nAlternatively you can upload your public key from an url as well, by using the \n\u2014sshKeyUrl\n switch. You can check whether the credential was creates successfully by using the \ncredential list\n command.\nYou can switch between your cloud credential - when you\u2019d like to use one and act with that you will have to use:\n\n\ncredential select --name my-gcp-credential\n\n\n\n\nYou can delete your cloud credential - when you\u2019d like to delete one you will have to use:\n\n\ncredential delete --name my-gcp-credential\n\n\n\n\nYou can show your cloud credential - when you\u2019d like to show one you will have to use:\n\n\ncredential show --name my-gcp-credential\n\n\n\n\nCreate a template\n\n\nA template gives developers and systems administrators an easy way to create and manage a collection of cloud infrastructure related resources, maintaining and updating them in an orderly and predictable fashion. A template can be used repeatedly to create identical copies of the same stack (or to use as a foundation to start a new stack).\n\n\ntemplate create --GCP --name gcptemplate --description gcp-template --instanceType N1_STANDARD_4 --volumeSize 100 --volumeCount 2\n\n\n\n\nOther available options:\n\n\n--volumeType\n defaults to \"HDD\", other allowed value: \"SSD\"\n\n\n--publicInAccount\n flags if the template is public in the account\n\n\nYou can check whether the template was created successfully by using the \ntemplate list\n or \ntemplate show\n command.\n\n\nYou can delete your cloud template - when you\u2019d like to delete one you will have to use:\n\n\ntemplate delete --name gcptemplate\n\n\n\n\nCreate or select a blueprint\n\n\nYou can define Ambari blueprints with cloudbreak-shell:\n\n\nblueprint add --name myblueprint --description myblueprint-description --file \nthe path of the blueprint\n\n\n\n\n\nOther available options:\n\n\n--url\n the url of the blueprint\n\n\n--publicInAccount\n flags if the network is public in the account\n\n\nWe ship default Ambari blueprints with Cloudbreak. You can use these blueprints or add yours. To see the available blueprints and use one of them please use:\n\n\nblueprint list\n\nblueprint select --name hdp-small-default\n\n\n\n\nCreate a network\n\n\nA network gives developers and systems administrators an easy way to create and manage a collection of cloud infrastructure related networking, maintaining and updating them in an orderly and predictable fashion. A network can be used repeatedly to create identical copies of the same stack (or to use as a foundation to start a new stack).\n\n\nnetwork create --GCP --name gcpnetwork --description \ngcp network\n--subnet 10.0.0.0/16\n\n\n\n\nOther available options:\n\n\n--publicInAccount\n flags if the network is public in the account\n\n\nThere is a default network with name \ndefault-gcp-network\n. If we use this for cluster creation, Cloudbreak will create the cluster within the 10.0.0.0/16 subnet.\n\n\nYou can check whether the network was created successfully by using the \nnetwork list\n command. Check the network and select it if you are happy with it:\n\n\nnetwork show --name gcpnetwork\n\nnetwork select --name gcpnetwork\n\n\n\n\nCreate a security group\n\n\nA security group gives developers and systems administrators an easy way to create and manage a collection of cloud infrastructure related security rules.\n\n\nsecuritygroup create --name secgroup_example --description securitygroup-example --rules 0.0.0.0/0:tcp:8080,9090;10.0.33.0/24:tcp:1234,1235\n\n\n\n\nYou can check whether the security group was created successfully by using the \nsecuritygroup list\n command. Check the security group and select it if you are happy with it:\n\n\nsecuritygroup show --name secgroup_example\n\nsecuritygroup select --name secgroup_example\n\n\n\n\nThere are two default security groups defined: \nall-services-port\n and \nonly-ssh-and-ssl\n\n\nonly-ssh-and-ssl:\n all ports are locked down (you can't access Hadoop services outside of the VPC)\n\n\n\n\nSSH (22)\n\n\nHTTPS (443)\n\n\n\n\nall-services-port:\n all Hadoop services + SSH/HTTP are accessible by default:\n\n\n\n\nSSH (22)\n\n\nHTTPS (443)\n\n\nAmbari (8080)\n\n\nConsul (8500)\n\n\nNN (50070)\n\n\nRM Web (8088)\n\n\nScheduler (8030RM)\n\n\nIPC (8050RM)\n\n\nJob history server (19888)\n\n\nHBase master (60000)\n\n\nHBase master web (60010)\n\n\nHBase RS (16020)\n\n\nHBase RS info (60030)\n\n\nFalcon (15000)\n\n\nStorm (8744)\n\n\nHive metastore (9083)\n\n\nHive server (10000)\n\n\nHive server HTTP (10001)\n\n\nAccumulo master (9999)\n\n\nAccumulo Tserver (9997)\n\n\nAtlas (21000)\n\n\nKNOX (8443)\n\n\nOozie (11000)\n\n\nSpark HS (18080)\n\n\nNM Web (8042)\n\n\nZeppelin WebSocket (9996)\n\n\nZeppelin UI (9995)\n\n\nKibana (3080)\n\n\nElasticsearch (9200)\n\n\n\n\nConfigure instance groups\n\n\nYou have to configure the instancegroups before the provisioning. An instancegroup is defining a group of your nodes with a specified template. Usually we create instancegroups for the hostgroups defined in the blueprints.\n\n\ninstancegroup configure --instanceGroup host_group_slave_1 --nodecount 3 --templateName minviable-gcp\n\n\n\n\nOther available options:\n\n\n--templateId\n Id of the template\n\n\nCreate a Hadoop cluster\n\n\nYou are almost done - two more command and this will create your Hadoop cluster on your favorite cloud provider. Same as the API, or UI this will use your \ncredential\n, \ninstancegroups\n, \nnetwork\n, \nsecuritygroup\n, and by using Google Cloud Platform will launch a cloud stack\n\n\nstack create --name my-first-stack --region US_CENTRAL1_A\n\n\n\n\nOnce the \nstack\n is up and running (cloud provisioning is done) it will use your selected \nblueprint\n and install your custom Hadoop cluster with the selected components and services.\n\n\ncluster create --description \nmy first cluster\n\n\n\n\n\nYou are done - you can check the progress through the Ambari UI. If you log back to Cloudbreak UI you can check the progress over there as well, and learn the IP address of Ambari.\n\n\nStop/Restart cluster and stack\n\n\nYou have the ability to \nstop your existing stack then its cluster\n if you want to suspend the work on it.\n\n\nSelect a stack for example with its name:\n\n\nstack select --name my-stack\n\n\n\n\nOther available option to define a stack is its \n--id\n (instead of the \n--name\n).\n\n\nApply the following commands to stop the previously selected stack:\n\n\ncluster stop\nstack stop\n\n\n\n\n\n\nImportant!\n The related cluster should be stopped before you can stop the stack.\n\n\n\n\nApply the following command to \nrestart the previously selected and stopped stack\n:\n\n\nstack start\n\n\n\n\nAfter the selected stack has restarted, you can \nrestart the related cluster as well\n:\n\n\ncluster start\n\n\n\n\nUpscale/Downscale cluster and stack\n\n\nYou can \nupscale your selected stack\n if you need more instances to your infrastructure:\n\n\nstack node --ADD --instanceGroup host_group_slave_1 --adjustment 6\n\n\n\n\nOther available options:\n\n\n--withClusterUpScale\n indicates cluster upscale after stack upscale\nor you can upscale the related cluster separately as well:\n\n\ncluster node --ADD --hostgroup host_group_slave_1 --adjustment 6\n\n\n\n\nApply the following command to \ndownscale the previously selected stack\n:\n\n\nstack node --REMOVE  --instanceGroup host_group_slave_1 --adjustment -2\n\n\n\n\nand the related cluster separately:\n\n\ncluster node --REMOVE  --hostgroup host_group_slave_1 --adjustment -2\n\n\n\n\nSilent mode\n\n\nWith Cloudbreak shell you can execute script files as well. A script file contains cloudbreak shell commands and can be executed with the \nscript\n cloudbreak shell command\n\n\nscript \nyour script file\n\n\n\n\n\nor with the \ncbd util cloudbreak-shell-quiet\n cbd command:\n\n\ncbd util cloudbreak-shell-quiet \n example.sh\n\n\n\n\nExample\n\n\nThe following example creates a hadoop cluster with \nhdp-small-default\n blueprint on M3Xlarge instances with 2X100G attached disks on \ndefault-gcp-network\n network using \nall-services-port\n security group. You should copy your ssh public key file and your GCP service account generated private key into your cbd working directory with name \nid_rsa.pub\n and \ngcp.p12\n and change the \n...\n parts with your gcp credential details.\n\n\ncredential create --GCP --description \nmy credential\n --name my-gcp-credential --projectId \nyour gcp projectid\n --serviceAccountId \nyour GCP service account mail address\n --serviceAccountPrivateKeyPath gcp.p12 --sshKeyFile id_rsa.pub\ncredential select --name my-gcp-credential\ntemplate create --GCP --name gcptemplate --description gcp-template --instanceType N1_STANDARD_4 --volumeSize 100 --volumeCount 2\nblueprint select --name hdp-small-default\ninstancegroup configure --instanceGroup cbgateway --nodecount 1 --templateName gcptemplate\ninstancegroup configure --instanceGroup host_group_master_1 --nodecount 1 --templateName gcptemplate\ninstancegroup configure --instanceGroup host_group_master_2 --nodecount 1 --templateName gcptemplate\ninstancegroup configure --instanceGroup host_group_master_3 --nodecount 1 --templateName gcptemplate\ninstancegroup configure --instanceGroup host_group_client_1  --nodecount 1 --templateName gcptemplate\ninstancegroup configure --instanceGroup host_group_slave_1 --nodecount 3 --templateName gcptemplate\nnetwork select --name default-gcp-network\nsecuritygroup select --name all-services-port\nstack create --name my-first-stack --region US_CENTRAL1_A\ncluster create --description \nMy first cluster\n\n\n\n\n\nNext steps\n\n\nCongrats! Your cluster should now be up and running. To learn more about it we have some \ninteresting insights\n about Cloudbreak clusters.", 
            "title": "Provisioning - CLI"
        }, 
        {
            "location": "/gcp_cb_shell/#interactive-mode", 
            "text": "Start the shell with  cbd util cloudbreak-shell . This will launch the Cloudbreak shell inside a Docker container and you are ready to start using it.  You have to copy files into the cbd working directory, which you would like to use from shell. For example if your  cbd  working directory is  ~/prj/cbd  then copy your blueprint and public ssh key file into this directory. You can refer to these files with their names from the shell.  Create a cloud credential  In order to start using Cloudbreak to provision a cluster in Google Cloud you will need to have a GCP credential. If you do not want to Cloubreak to reach your Google Cloud resources then you have to delete the service account.  credential create --GCP --description  short description of your linked credential  --name my-gcp-credential --projectId  your gcp projectid  --serviceAccountId  your GCP service account mail address  --serviceAccountPrivateKeyPath  path of your GCP service account generated private key  --sshKeyPath  path of your GCP public key   Alternatively you can upload your public key from an url as well, by using the  \u2014sshKeyUrl  switch. You can check whether the credential was creates successfully by using the  credential list  command.\nYou can switch between your cloud credential - when you\u2019d like to use one and act with that you will have to use:  credential select --name my-gcp-credential  You can delete your cloud credential - when you\u2019d like to delete one you will have to use:  credential delete --name my-gcp-credential  You can show your cloud credential - when you\u2019d like to show one you will have to use:  credential show --name my-gcp-credential  Create a template  A template gives developers and systems administrators an easy way to create and manage a collection of cloud infrastructure related resources, maintaining and updating them in an orderly and predictable fashion. A template can be used repeatedly to create identical copies of the same stack (or to use as a foundation to start a new stack).  template create --GCP --name gcptemplate --description gcp-template --instanceType N1_STANDARD_4 --volumeSize 100 --volumeCount 2  Other available options:  --volumeType  defaults to \"HDD\", other allowed value: \"SSD\"  --publicInAccount  flags if the template is public in the account  You can check whether the template was created successfully by using the  template list  or  template show  command.  You can delete your cloud template - when you\u2019d like to delete one you will have to use:  template delete --name gcptemplate  Create or select a blueprint  You can define Ambari blueprints with cloudbreak-shell:  blueprint add --name myblueprint --description myblueprint-description --file  the path of the blueprint   Other available options:  --url  the url of the blueprint  --publicInAccount  flags if the network is public in the account  We ship default Ambari blueprints with Cloudbreak. You can use these blueprints or add yours. To see the available blueprints and use one of them please use:  blueprint list\n\nblueprint select --name hdp-small-default  Create a network  A network gives developers and systems administrators an easy way to create and manage a collection of cloud infrastructure related networking, maintaining and updating them in an orderly and predictable fashion. A network can be used repeatedly to create identical copies of the same stack (or to use as a foundation to start a new stack).  network create --GCP --name gcpnetwork --description  gcp network --subnet 10.0.0.0/16  Other available options:  --publicInAccount  flags if the network is public in the account  There is a default network with name  default-gcp-network . If we use this for cluster creation, Cloudbreak will create the cluster within the 10.0.0.0/16 subnet.  You can check whether the network was created successfully by using the  network list  command. Check the network and select it if you are happy with it:  network show --name gcpnetwork\n\nnetwork select --name gcpnetwork  Create a security group  A security group gives developers and systems administrators an easy way to create and manage a collection of cloud infrastructure related security rules.  securitygroup create --name secgroup_example --description securitygroup-example --rules 0.0.0.0/0:tcp:8080,9090;10.0.33.0/24:tcp:1234,1235  You can check whether the security group was created successfully by using the  securitygroup list  command. Check the security group and select it if you are happy with it:  securitygroup show --name secgroup_example\n\nsecuritygroup select --name secgroup_example  There are two default security groups defined:  all-services-port  and  only-ssh-and-ssl  only-ssh-and-ssl:  all ports are locked down (you can't access Hadoop services outside of the VPC)   SSH (22)  HTTPS (443)   all-services-port:  all Hadoop services + SSH/HTTP are accessible by default:   SSH (22)  HTTPS (443)  Ambari (8080)  Consul (8500)  NN (50070)  RM Web (8088)  Scheduler (8030RM)  IPC (8050RM)  Job history server (19888)  HBase master (60000)  HBase master web (60010)  HBase RS (16020)  HBase RS info (60030)  Falcon (15000)  Storm (8744)  Hive metastore (9083)  Hive server (10000)  Hive server HTTP (10001)  Accumulo master (9999)  Accumulo Tserver (9997)  Atlas (21000)  KNOX (8443)  Oozie (11000)  Spark HS (18080)  NM Web (8042)  Zeppelin WebSocket (9996)  Zeppelin UI (9995)  Kibana (3080)  Elasticsearch (9200)   Configure instance groups  You have to configure the instancegroups before the provisioning. An instancegroup is defining a group of your nodes with a specified template. Usually we create instancegroups for the hostgroups defined in the blueprints.  instancegroup configure --instanceGroup host_group_slave_1 --nodecount 3 --templateName minviable-gcp  Other available options:  --templateId  Id of the template  Create a Hadoop cluster  You are almost done - two more command and this will create your Hadoop cluster on your favorite cloud provider. Same as the API, or UI this will use your  credential ,  instancegroups ,  network ,  securitygroup , and by using Google Cloud Platform will launch a cloud stack  stack create --name my-first-stack --region US_CENTRAL1_A  Once the  stack  is up and running (cloud provisioning is done) it will use your selected  blueprint  and install your custom Hadoop cluster with the selected components and services.  cluster create --description  my first cluster   You are done - you can check the progress through the Ambari UI. If you log back to Cloudbreak UI you can check the progress over there as well, and learn the IP address of Ambari.  Stop/Restart cluster and stack  You have the ability to  stop your existing stack then its cluster  if you want to suspend the work on it.  Select a stack for example with its name:  stack select --name my-stack  Other available option to define a stack is its  --id  (instead of the  --name ).  Apply the following commands to stop the previously selected stack:  cluster stop\nstack stop   Important!  The related cluster should be stopped before you can stop the stack.   Apply the following command to  restart the previously selected and stopped stack :  stack start  After the selected stack has restarted, you can  restart the related cluster as well :  cluster start  Upscale/Downscale cluster and stack  You can  upscale your selected stack  if you need more instances to your infrastructure:  stack node --ADD --instanceGroup host_group_slave_1 --adjustment 6  Other available options:  --withClusterUpScale  indicates cluster upscale after stack upscale\nor you can upscale the related cluster separately as well:  cluster node --ADD --hostgroup host_group_slave_1 --adjustment 6  Apply the following command to  downscale the previously selected stack :  stack node --REMOVE  --instanceGroup host_group_slave_1 --adjustment -2  and the related cluster separately:  cluster node --REMOVE  --hostgroup host_group_slave_1 --adjustment -2", 
            "title": "Interactive mode"
        }, 
        {
            "location": "/gcp_cb_shell/#silent-mode", 
            "text": "With Cloudbreak shell you can execute script files as well. A script file contains cloudbreak shell commands and can be executed with the  script  cloudbreak shell command  script  your script file   or with the  cbd util cloudbreak-shell-quiet  cbd command:  cbd util cloudbreak-shell-quiet   example.sh", 
            "title": "Silent mode"
        }, 
        {
            "location": "/gcp_cb_shell/#example", 
            "text": "The following example creates a hadoop cluster with  hdp-small-default  blueprint on M3Xlarge instances with 2X100G attached disks on  default-gcp-network  network using  all-services-port  security group. You should copy your ssh public key file and your GCP service account generated private key into your cbd working directory with name  id_rsa.pub  and  gcp.p12  and change the  ...  parts with your gcp credential details.  credential create --GCP --description  my credential  --name my-gcp-credential --projectId  your gcp projectid  --serviceAccountId  your GCP service account mail address  --serviceAccountPrivateKeyPath gcp.p12 --sshKeyFile id_rsa.pub\ncredential select --name my-gcp-credential\ntemplate create --GCP --name gcptemplate --description gcp-template --instanceType N1_STANDARD_4 --volumeSize 100 --volumeCount 2\nblueprint select --name hdp-small-default\ninstancegroup configure --instanceGroup cbgateway --nodecount 1 --templateName gcptemplate\ninstancegroup configure --instanceGroup host_group_master_1 --nodecount 1 --templateName gcptemplate\ninstancegroup configure --instanceGroup host_group_master_2 --nodecount 1 --templateName gcptemplate\ninstancegroup configure --instanceGroup host_group_master_3 --nodecount 1 --templateName gcptemplate\ninstancegroup configure --instanceGroup host_group_client_1  --nodecount 1 --templateName gcptemplate\ninstancegroup configure --instanceGroup host_group_slave_1 --nodecount 3 --templateName gcptemplate\nnetwork select --name default-gcp-network\nsecuritygroup select --name all-services-port\nstack create --name my-first-stack --region US_CENTRAL1_A\ncluster create --description  My first cluster", 
            "title": "Example"
        }, 
        {
            "location": "/gcp_cb_shell/#next-steps", 
            "text": "Congrats! Your cluster should now be up and running. To learn more about it we have some  interesting insights  about Cloudbreak clusters.", 
            "title": "Next steps"
        }, 
        {
            "location": "/openstack/", 
            "text": "OpenStack Setup\n\n\nSetup Cloudbreak Deployer\n\n\nIf you already have Cloudbreak Deployer either by \nusing the OpenStack Cloud Images\n or by \ninstalling the Cloudbreak Deployer\n manually on your own VM,\nyou can start to setup the Cloudbreak Application with the deployer.\n\n\n\n\nCloudbreak currently only supports the \nOpenStack Juno\n release.\n\n\n\n\nCreate and open the \ncloudbreak-deployment\n directory:\n\n\ncd cloudbreak-deployment\n\n\n\n\nThis is the directory of the config files and the supporting binaries that will be downloaded by Cloudbreak deployer.\n\n\nInitialize your Profile\n\n\nFirst initialize your directory by creating a \nProfile\n file:\n\n\ncbd init\n\n\n\n\nIt will create a \nProfile\n file in the current directory. Please edit the file - one of the required configurations is the \nPUBLIC_IP\n.\nThis IP will be used to access the Cloudbreak UI (called Uluwatu). In some cases the \ncbd\n tool tries to guess it, if can't than will give a hint.\n\n\nThe other required configuration in the \nProfile\n is the name of the Cloudbreak image you uploaded to your OpenStack cloud.\n\n\nexport CB_OPENSTACK_IMAGE=\n$OS_IMAGE_NAME\n\n\n\n\n\nGenerate your Profile\n\n\nYou are done with the configuration of Cloudbreak deployer. The last thing you have to do is to regenerate the configurations in order to take effect.\n\n\nrm *.yml\ncbd generate\n\n\n\n\nThis command applies the following steps: \n\n\n\n\ncreates the \ndocker-compose.yml\n file that describes the configuration of all the Docker containers needed for the Cloudbreak deployment.\n\n\ncreates the \nuaa.yml\n file that holds the configuration of the identity server used to authenticate users to Cloudbreak.\n\n\n\n\nStart Cloudbreak\n\n\nTo start the Cloudbreak application use the following command.\nThis will start all the Docker containers and initialize the application. It will take a few minutes until all the services start.\n\n\ncbd start\n\n\n\n\n\n\nLaunching it first will take more time as it downloads all the docker images needed by Cloudbreak.\n\n\n\n\nAfter the \ncbd start\n command finishes you can check the logs of the Cloudbreak server with this command:\n\n\ncbd logs cloudbreak\n\n\n\n\n\n\nCloudbreak server should start within a minute - you should see a line like this: \nStarted CloudbreakApplication in 36.823 seconds\n\n\n\n\nNext steps\n\n\nOnce Cloudbreak is up and running you should check out the \nProvisioning Prerequisites\n needed to create OpenStack clusters with Cloudbreak.", 
            "title": "Setup"
        }, 
        {
            "location": "/openstack/#openstack-setup", 
            "text": "", 
            "title": "OpenStack Setup"
        }, 
        {
            "location": "/openstack/#setup-cloudbreak-deployer", 
            "text": "If you already have Cloudbreak Deployer either by  using the OpenStack Cloud Images  or by  installing the Cloudbreak Deployer  manually on your own VM,\nyou can start to setup the Cloudbreak Application with the deployer.   Cloudbreak currently only supports the  OpenStack Juno  release.   Create and open the  cloudbreak-deployment  directory:  cd cloudbreak-deployment  This is the directory of the config files and the supporting binaries that will be downloaded by Cloudbreak deployer.  Initialize your Profile  First initialize your directory by creating a  Profile  file:  cbd init  It will create a  Profile  file in the current directory. Please edit the file - one of the required configurations is the  PUBLIC_IP .\nThis IP will be used to access the Cloudbreak UI (called Uluwatu). In some cases the  cbd  tool tries to guess it, if can't than will give a hint.  The other required configuration in the  Profile  is the name of the Cloudbreak image you uploaded to your OpenStack cloud.  export CB_OPENSTACK_IMAGE= $OS_IMAGE_NAME   Generate your Profile  You are done with the configuration of Cloudbreak deployer. The last thing you have to do is to regenerate the configurations in order to take effect.  rm *.yml\ncbd generate  This command applies the following steps:    creates the  docker-compose.yml  file that describes the configuration of all the Docker containers needed for the Cloudbreak deployment.  creates the  uaa.yml  file that holds the configuration of the identity server used to authenticate users to Cloudbreak.   Start Cloudbreak  To start the Cloudbreak application use the following command.\nThis will start all the Docker containers and initialize the application. It will take a few minutes until all the services start.  cbd start   Launching it first will take more time as it downloads all the docker images needed by Cloudbreak.   After the  cbd start  command finishes you can check the logs of the Cloudbreak server with this command:  cbd logs cloudbreak   Cloudbreak server should start within a minute - you should see a line like this:  Started CloudbreakApplication in 36.823 seconds   Next steps  Once Cloudbreak is up and running you should check out the  Provisioning Prerequisites  needed to create OpenStack clusters with Cloudbreak.", 
            "title": "Setup Cloudbreak Deployer"
        }, 
        {
            "location": "/openstack_pre_prov/", 
            "text": "Provisioning Prerequisites\n\n\nGenerate a new SSH key\n\n\nAll the instances created by Cloudbreak are configured to allow key-based SSH,\nso you'll need to provide an SSH public key that can be used later to SSH onto the instances in the clusters you'll create with Cloudbreak.\nYou can use one of your existing keys or you can generate a new one.\n\n\nTo generate a new SSH keypair:\n\n\nssh-keygen -t rsa -b 4096 -C \nyour_email@example.com\n\n# Creates a new ssh key, using the provided email as a label\n# Generating public/private rsa key pair.\n\n\n\n\n# Enter file in which to save the key (/Users/you/.ssh/id_rsa): [Press enter]\nYou'll be asked to enter a passphrase, but you can leave it empty.\n\n# Enter passphrase (empty for no passphrase): [Type a passphrase]\n# Enter same passphrase again: [Type passphrase again]\n\n\n\n\nAfter you enter a passphrase the keypair is generated. The output should look something like below.\n\n\n# Your identification has been saved in /Users/you/.ssh/id_rsa.\n# Your public key has been saved in /Users/you/.ssh/id_rsa.pub.\n# The key fingerprint is:\n# 01:0f:f4:3b:ca:85:sd:17:sd:7d:sd:68:9d:sd:a2:sd your_email@example.com\n\n\n\n\nLater you'll need to pass the \n.pub\n file's contents to Cloudbreak and use the private part to SSH to the instances.\n\n\nNext steps\n\n\nAfter these prerequisites are done you can move on to create clusters on the \nUI\n or with the \nShell\n.", 
            "title": "Provisioning Prerequisites"
        }, 
        {
            "location": "/openstack_pre_prov/#provisioning-prerequisites", 
            "text": "", 
            "title": "Provisioning Prerequisites"
        }, 
        {
            "location": "/openstack_pre_prov/#generate-a-new-ssh-key", 
            "text": "All the instances created by Cloudbreak are configured to allow key-based SSH,\nso you'll need to provide an SSH public key that can be used later to SSH onto the instances in the clusters you'll create with Cloudbreak.\nYou can use one of your existing keys or you can generate a new one.  To generate a new SSH keypair:  ssh-keygen -t rsa -b 4096 -C  your_email@example.com \n# Creates a new ssh key, using the provided email as a label\n# Generating public/private rsa key pair.  # Enter file in which to save the key (/Users/you/.ssh/id_rsa): [Press enter]\nYou'll be asked to enter a passphrase, but you can leave it empty.\n\n# Enter passphrase (empty for no passphrase): [Type a passphrase]\n# Enter same passphrase again: [Type passphrase again]  After you enter a passphrase the keypair is generated. The output should look something like below.  # Your identification has been saved in /Users/you/.ssh/id_rsa.\n# Your public key has been saved in /Users/you/.ssh/id_rsa.pub.\n# The key fingerprint is:\n# 01:0f:f4:3b:ca:85:sd:17:sd:7d:sd:68:9d:sd:a2:sd your_email@example.com  Later you'll need to pass the  .pub  file's contents to Cloudbreak and use the private part to SSH to the instances.", 
            "title": "Generate a new SSH key"
        }, 
        {
            "location": "/openstack_pre_prov/#next-steps", 
            "text": "After these prerequisites are done you can move on to create clusters on the  UI  or with the  Shell .", 
            "title": "Next steps"
        }, 
        {
            "location": "/openstack_cb_ui/", 
            "text": "Provisioning via Browser\n\n\nYou can log into the Cloudbreak application at http://PUBLIC_IP:3000.\n\n\nThe main goal of the Cloudbreak UI is to easily create clusters on your own cloud provider account.\nThis description details the OpenStack setup - if you'd like to use a different cloud provider check out its manual.\n\n\nThis document explains the four steps that need to be followed to create Cloudbreak clusters from the UI:\n\n\n\n\nconnect your OpenStack with Cloudbreak\n\n\ncreate some template resources on the UI that describe the infrastructure of your clusters\n\n\ncreate a blueprint that describes the HDP services in your clusters and add some recipes for customization\n\n\nlaunch the cluster itself based on these template resources\n\n\n\n\nManage cloud credentials\n\n\nYou can now log into the Cloudbreak application at http://PUBLIC_IP:3000. Once logged in go to \nManage credentials\n. Using manage credentials will  link your cloud account with the Cloudbreak account.\n\n\nName:\n name of your credential\n\n\nDescription:\n short description of your linked credential\n\n\nUser:\n your OpenStack user\n\n\nPassword:\n your password\n\n\nTenant Name:\n OpenStack tenant name\n\n\nEndpoint:\n Openstack Identity Service (Keystone) endpont (e.g. http://PUBLIC_IP:5000/v2.0)\n\n\nSSH public key:\n the SSH public certificate in OpenSSH format that's private keypair can be used to \nlog into the launched instances\n later with the \nssh username: centos\n\n\nPublic in account:\n share it with others in the account\n\n\nInfrastructure templates\n\n\nAfter your OpenStack is linked to Cloudbreak you can start creating templates that describe your clusters' infrastructure:\n\n\n\n\nresources\n\n\nnetworks\n\n\nsecurity groups\n\n\n\n\nWhen you create a template, Cloudbreak \ndoesn't make any requests\n to OpenStack.\nResources are only created on OpenStack after the \ncreate cluster\n button is pushed.\nThese templates are saved to Cloudbreak's database and can be reused with multiple clusters to describe the infrastructure.\n\n\nManage resources\n\n\nUsing manage resources you can create infrastructure templates. Templates describes the infrastructure where the HDP cluster will be provisioned. We support heterogenous clusters - this means that one cluster can be built by combining different templates.\n\n\nName:\n name of your template\n\n\nDescription:\n short description of your template\n\n\nInstance type:\n the OpenStack instance type to be used\n\n\nAttached volumes per instance:\n the number of disks to be attached\n\n\nVolume size (GB):\n the size of the attached disks (in GB)\n\n\nPublic in account:\n share it with others in the account\n\n\nManage blueprints\n\n\nBlueprints are your declarative definition of a Hadoop cluster.\n\n\nName:\n name of your blueprint\n\n\nDescription:\n short description of your blueprint\n\n\nSource URL:\n you can add a blueprint by pointing to a URL. As an example you can use this \nblueprint\n.\n\n\nManual copy:\n you can copy paste your blueprint in this text area\n\n\nPublic in account:\n share it with others in the account\n\n\nManage networks\n\n\nManage networks allows you to create or reuse existing networks and configure them.\n\n\nName:\n name of the network\n\n\nDescription:\n short description of your network\n\n\nSubnet (CIDR):\n a subnet with CIDR block under the given \npublic network\n\n\nPublic network ID:\n id of an OpenStack public network\n\n\nPublic in account:\n share it with others in the account\n\n\nSecurity groups\n\n\nThey describe the allowed inbound traffic to the instances in the cluster.\nCurrently only one security group template can be selected for a Cloudbreak cluster and all the instances have a public IP address so all the instances in the cluster will belong to the same security group.\nThis may change in a later release.\n\n\nYou can define your own security group by adding all the ports, protocols and CIDR range you'd like to use. 443 needs to be there in every security group otherwise Cloudbreak won't be able to communicate with the provisioned cluster. The rules defined here doesn't need to contain the internal rules, those are automatically added by Cloudbreak to the security group on OpenStack.\n\n\nYou can also use the two pre-defined security groups in Cloudbreak:\n\n\nonly-ssh-and-ssl:\n all ports are locked down (you can't access Hadoop services outside of the Virtual Private Cloud) but\n\n\n\n\nSSH (22)\n\n\nHTTPS (443)\n\n\n\n\nall-services-port:\n all Hadoop services + SSH/gateway HTTPS are accessible by default:\n\n\n\n\nSSH (22)\n\n\nHTTPS (443)\n\n\nAmbari (8080)\n\n\nConsul (8500)\n\n\nNN (50070)\n\n\nRM Web (8088)\n\n\nScheduler (8030RM)\n\n\nIPC (8050RM)\n\n\nJob history server (19888)\n\n\nHBase master (60000)\n\n\nHBase master web (60010)\n\n\nHBase RS (16020)\n\n\nHBase RS info (60030)\n\n\nFalcon (15000)\n\n\nStorm (8744)\n\n\nHive metastore (9083)\n\n\nHive server (10000)\n\n\nHive server HTTP (10001)\n\n\nAccumulo master (9999)\n\n\nAccumulo Tserver (9997)\n\n\nAtlas (21000)\n\n\nKNOX (8443)\n\n\nOozie (11000)\n\n\nSpark HS (18080)\n\n\nNM Web (8042)\n\n\nZeppelin WebSocket (9996)\n\n\nZeppelin UI (9995)\n\n\nKibana (3080)\n\n\nElasticsearch (9200)\n\n\n\n\nIf \nPublic in account\n is checked all the users belonging to your account will be able to use this security group template to create clusters, but cannot delete or modify it.\n\n\nNote\n that the security groups are \nnot created\n on OpenStack after the \nCreate Security Group\n button is pushed, only after the cluster provisioning starts with the selected security group template.\n\n\nCluster installation\n\n\nThis section describes\n\n\nBlueprints\n\n\nBlueprints are your declarative definition of a Hadoop cluster. These are the same blueprints that are \nused by Ambari\n.\n\n\nYou can use the 3 default blueprints pre-defined in Cloudbreak or you can create your own.\nBlueprints can be added from an URL or the whole JSON can be copied to the \nManual copy\n field.\n\n\nThe hostgroups added in the JSON will be mapped to a set of instances when starting the cluster and the services and components defined in the hostgroup will be installed on the corresponding nodes.\nIt is not necessary to define all the configuration fields in the blueprints - if a configuration is missing, Ambari will fill that with a default value.\nThe configurations defined in the blueprint can also be modified later from the Ambari UI.\n\n\nIf \nPublic in account\n is checked all the users belonging to your account will be able to use this blueprint to create clusters, but cannot delete or modify it.\n\n\nA blueprint can be exported from a running Ambari cluster that can be reused in Cloudbreak with slight modifications.\nThere is no automatic way to modify an exported blueprint and make it instantly usable in Cloudbreak, the modifications have to be done manually.\nWhen the blueprint is exported some configurations will have for example hardcoded domain names, or memory configurations that won't be applicable to the Cloudbreak cluster.\n\n\nCluster customization\n\n\nSometimes it can be useful to define some custom scripts that run during cluster creation and add some additional functionality.\nFor example it can be a service you'd like to install but it's not supported by Ambari or some script that automatically downloads some data to the necessary nodes.\nThe most notable example is Ranger setup: it has a prerequisite of a running database when Ranger Admin is installing.\nA PostgreSQL database can be easily started and configured with a recipe before the blueprint installation starts.\n\n\nTo learn more about these so called \nRecipes\n, and to check out the Ranger database recipe, take a look at the \nCluster customization\n part of the documentation.\n\n\nCluster deployment\n\n\nAfter all the templates are configured you can deploy a new HDP cluster. Start by selecting a previously created credential in the header.\nClick on \ncreate cluster\n, give it a \nName\n, select a \nRegion\n where the cluster infrastructure will be provisioned and select one of the \nNetworks\n and \nSecurity Groups\n created earlier.\nAfter you've selected a \nBlueprint\n as well you should be able to configure the \nTemplate resources\n and the number of nodes for all of the hostgroups in the blueprint.\n\n\nIf \nPublic in account\n is checked all the users belonging to your account will be able to see the newly created cluster on the UI, but cannot delete or modify it.\n\n\nIf \nEnable security\n is checked as well, Cloudbreak will install Key Distribution Center (KDC) and the cluster will be Kerberized. See more about it in the \nKerberos\n section of this documentation.\n\n\nAfter the \ncreate and start cluster\n button is pushed Cloudbreak will start to create resources on your OpenStack.\n\n\n\n\nImportant\n Always use Cloudbreak to delete the cluster. If that fails for some reason, always try to delete via \nOpenStack Dashboard.\n\n\n\n\nAdvanced options\n:\n\n\nConsul server count:\n the number of Consul servers (add number), by default is 3. It varies with the cluster size.\n\n\nPlatform variant:\n Cloudbreak provides two implementation for creating OpenStack cluster\n\n\n\n\nHEAT:\n using heat template to create the resources\n\n\nNATIVE:\n using API calls to create the resources\n\n\n\n\nMinimum cluster size:\n the provisioning strategy in case of the cloud provider can't allocate all the requested nodes\n\n\nValidate blueprint:\n feature to validate or not the Ambari blueprint. By default is switched on.\n\n\nOnce you have launched the cluster creation you can track the progress either on Cloudbreak UI or your cloud provider management UI.\n\n\nNext steps\n\n\nCongrats! Your cluster should now be up and running. To learn more about it we have some \ninteresting insights\n about Cloudbreak clusters.", 
            "title": "Provisioning - UI"
        }, 
        {
            "location": "/openstack_cb_ui/#provisioning-via-browser", 
            "text": "You can log into the Cloudbreak application at http://PUBLIC_IP:3000.  The main goal of the Cloudbreak UI is to easily create clusters on your own cloud provider account.\nThis description details the OpenStack setup - if you'd like to use a different cloud provider check out its manual.  This document explains the four steps that need to be followed to create Cloudbreak clusters from the UI:   connect your OpenStack with Cloudbreak  create some template resources on the UI that describe the infrastructure of your clusters  create a blueprint that describes the HDP services in your clusters and add some recipes for customization  launch the cluster itself based on these template resources", 
            "title": "Provisioning via Browser"
        }, 
        {
            "location": "/openstack_cb_ui/#manage-cloud-credentials", 
            "text": "You can now log into the Cloudbreak application at http://PUBLIC_IP:3000. Once logged in go to  Manage credentials . Using manage credentials will  link your cloud account with the Cloudbreak account.  Name:  name of your credential  Description:  short description of your linked credential  User:  your OpenStack user  Password:  your password  Tenant Name:  OpenStack tenant name  Endpoint:  Openstack Identity Service (Keystone) endpont (e.g. http://PUBLIC_IP:5000/v2.0)  SSH public key:  the SSH public certificate in OpenSSH format that's private keypair can be used to  log into the launched instances  later with the  ssh username: centos  Public in account:  share it with others in the account", 
            "title": "Manage cloud credentials"
        }, 
        {
            "location": "/openstack_cb_ui/#infrastructure-templates", 
            "text": "After your OpenStack is linked to Cloudbreak you can start creating templates that describe your clusters' infrastructure:   resources  networks  security groups   When you create a template, Cloudbreak  doesn't make any requests  to OpenStack.\nResources are only created on OpenStack after the  create cluster  button is pushed.\nThese templates are saved to Cloudbreak's database and can be reused with multiple clusters to describe the infrastructure.  Manage resources  Using manage resources you can create infrastructure templates. Templates describes the infrastructure where the HDP cluster will be provisioned. We support heterogenous clusters - this means that one cluster can be built by combining different templates.  Name:  name of your template  Description:  short description of your template  Instance type:  the OpenStack instance type to be used  Attached volumes per instance:  the number of disks to be attached  Volume size (GB):  the size of the attached disks (in GB)  Public in account:  share it with others in the account  Manage blueprints  Blueprints are your declarative definition of a Hadoop cluster.  Name:  name of your blueprint  Description:  short description of your blueprint  Source URL:  you can add a blueprint by pointing to a URL. As an example you can use this  blueprint .  Manual copy:  you can copy paste your blueprint in this text area  Public in account:  share it with others in the account  Manage networks  Manage networks allows you to create or reuse existing networks and configure them.  Name:  name of the network  Description:  short description of your network  Subnet (CIDR):  a subnet with CIDR block under the given  public network  Public network ID:  id of an OpenStack public network  Public in account:  share it with others in the account  Security groups  They describe the allowed inbound traffic to the instances in the cluster.\nCurrently only one security group template can be selected for a Cloudbreak cluster and all the instances have a public IP address so all the instances in the cluster will belong to the same security group.\nThis may change in a later release.  You can define your own security group by adding all the ports, protocols and CIDR range you'd like to use. 443 needs to be there in every security group otherwise Cloudbreak won't be able to communicate with the provisioned cluster. The rules defined here doesn't need to contain the internal rules, those are automatically added by Cloudbreak to the security group on OpenStack.  You can also use the two pre-defined security groups in Cloudbreak:  only-ssh-and-ssl:  all ports are locked down (you can't access Hadoop services outside of the Virtual Private Cloud) but   SSH (22)  HTTPS (443)   all-services-port:  all Hadoop services + SSH/gateway HTTPS are accessible by default:   SSH (22)  HTTPS (443)  Ambari (8080)  Consul (8500)  NN (50070)  RM Web (8088)  Scheduler (8030RM)  IPC (8050RM)  Job history server (19888)  HBase master (60000)  HBase master web (60010)  HBase RS (16020)  HBase RS info (60030)  Falcon (15000)  Storm (8744)  Hive metastore (9083)  Hive server (10000)  Hive server HTTP (10001)  Accumulo master (9999)  Accumulo Tserver (9997)  Atlas (21000)  KNOX (8443)  Oozie (11000)  Spark HS (18080)  NM Web (8042)  Zeppelin WebSocket (9996)  Zeppelin UI (9995)  Kibana (3080)  Elasticsearch (9200)   If  Public in account  is checked all the users belonging to your account will be able to use this security group template to create clusters, but cannot delete or modify it.  Note  that the security groups are  not created  on OpenStack after the  Create Security Group  button is pushed, only after the cluster provisioning starts with the selected security group template.", 
            "title": "Infrastructure templates"
        }, 
        {
            "location": "/openstack_cb_ui/#cluster-installation", 
            "text": "This section describes  Blueprints  Blueprints are your declarative definition of a Hadoop cluster. These are the same blueprints that are  used by Ambari .  You can use the 3 default blueprints pre-defined in Cloudbreak or you can create your own.\nBlueprints can be added from an URL or the whole JSON can be copied to the  Manual copy  field.  The hostgroups added in the JSON will be mapped to a set of instances when starting the cluster and the services and components defined in the hostgroup will be installed on the corresponding nodes.\nIt is not necessary to define all the configuration fields in the blueprints - if a configuration is missing, Ambari will fill that with a default value.\nThe configurations defined in the blueprint can also be modified later from the Ambari UI.  If  Public in account  is checked all the users belonging to your account will be able to use this blueprint to create clusters, but cannot delete or modify it.  A blueprint can be exported from a running Ambari cluster that can be reused in Cloudbreak with slight modifications.\nThere is no automatic way to modify an exported blueprint and make it instantly usable in Cloudbreak, the modifications have to be done manually.\nWhen the blueprint is exported some configurations will have for example hardcoded domain names, or memory configurations that won't be applicable to the Cloudbreak cluster.  Cluster customization  Sometimes it can be useful to define some custom scripts that run during cluster creation and add some additional functionality.\nFor example it can be a service you'd like to install but it's not supported by Ambari or some script that automatically downloads some data to the necessary nodes.\nThe most notable example is Ranger setup: it has a prerequisite of a running database when Ranger Admin is installing.\nA PostgreSQL database can be easily started and configured with a recipe before the blueprint installation starts.  To learn more about these so called  Recipes , and to check out the Ranger database recipe, take a look at the  Cluster customization  part of the documentation.", 
            "title": "Cluster installation"
        }, 
        {
            "location": "/openstack_cb_ui/#cluster-deployment", 
            "text": "After all the templates are configured you can deploy a new HDP cluster. Start by selecting a previously created credential in the header.\nClick on  create cluster , give it a  Name , select a  Region  where the cluster infrastructure will be provisioned and select one of the  Networks  and  Security Groups  created earlier.\nAfter you've selected a  Blueprint  as well you should be able to configure the  Template resources  and the number of nodes for all of the hostgroups in the blueprint.  If  Public in account  is checked all the users belonging to your account will be able to see the newly created cluster on the UI, but cannot delete or modify it.  If  Enable security  is checked as well, Cloudbreak will install Key Distribution Center (KDC) and the cluster will be Kerberized. See more about it in the  Kerberos  section of this documentation.  After the  create and start cluster  button is pushed Cloudbreak will start to create resources on your OpenStack.   Important  Always use Cloudbreak to delete the cluster. If that fails for some reason, always try to delete via \nOpenStack Dashboard.   Advanced options :  Consul server count:  the number of Consul servers (add number), by default is 3. It varies with the cluster size.  Platform variant:  Cloudbreak provides two implementation for creating OpenStack cluster   HEAT:  using heat template to create the resources  NATIVE:  using API calls to create the resources   Minimum cluster size:  the provisioning strategy in case of the cloud provider can't allocate all the requested nodes  Validate blueprint:  feature to validate or not the Ambari blueprint. By default is switched on.  Once you have launched the cluster creation you can track the progress either on Cloudbreak UI or your cloud provider management UI.", 
            "title": "Cluster deployment"
        }, 
        {
            "location": "/openstack_cb_ui/#next-steps", 
            "text": "Congrats! Your cluster should now be up and running. To learn more about it we have some  interesting insights  about Cloudbreak clusters.", 
            "title": "Next steps"
        }, 
        {
            "location": "/openstack_cb_shell/", 
            "text": "Interactive mode\n\n\nStart the shell with \ncbd util cloudbreak-shell\n. This will launch the Cloudbreak shell inside a Docker container and you are ready to start using it.\n\n\nYou have to copy files into the cbd working directory, which you would like to use from shell. For example if your \ncbd\n working directory is \n~/prj/cbd\n then copy your blueprint and public ssh key file into this directory. You can refer to these files with their names from the shell.\n\n\nCreate a cloud credential\n\n\ncredential create --OPENSTACK --name my-os-credential --description \ncredentail description\n --userName \nOpenStack username\n --password \nOpenStack password\n --tenantName \nOpenStack tenant name\n --endPoint \nOpenStack Identity Service (Keystone) endpoint\n --sshKeyPath \npath of your public SSH key file\n\n\n\n\n\nAlternatively you can upload your public key from an url as well, by using the \n\u2014sshKeyUrl\n switch. You can check whether the credential was created successfully by using the \ncredential list\n command. You can switch between your cloud credentials - when you\u2019d like to use one and act with that you will have to use:\n\n\ncredential select --name my-openstack-credential\n\n\n\n\nCreate a template\n\n\nA template gives developers and systems administrators an easy way to create and manage a collection of cloud infrastructure related resources, maintaining and updating them in an orderly and predictable fashion. A template can be used repeatedly to create identical copies of the same stack (or to use as a foundation to start a new stack).\n\n\ntemplate create --OPENSTACK --name ostemplate --description openstack-template --instanceType m1.large --volumeSize 100 --volumeCount 2\n\n\n\n\nYou can check whether the template was created successfully by using the \ntemplate list\n or \ntemplate show\n command.\n\n\nYou can delete your cloud template - when you\u2019d like to delete one you will have to use:\n\n\ntemplate delete --name ostemplate\n\n\n\n\nCreate or select a blueprint\n\n\nYou can define Ambari blueprints with cloudbreak-shell:\n\n\nblueprint add --name myblueprint --description myblueprint-description --file \nthe path of the blueprint\n\n\n\n\n\nOther available options:\n\n\n--url\n the url of the blueprint\n\n\n--publicInAccount\n flags if the network is public in the account\n\n\nWe ship default Ambari blueprints with Cloudbreak. You can use these blueprints or add yours. To see the available blueprints and use one of them please use:\n\n\nblueprint list\n\nblueprint select --name hdp-small-default\n\n\n\n\nCreate a network\n\n\nA network gives developers and systems administrators an easy way to create and manage a collection of cloud infrastructure related networking, maintaining and updating them in an orderly and predictable fashion. A network can be used repeatedly to create identical copies of the same stack (or to use as a foundation to start a new stack).\n\n\nnetwork create --OPENSTACK --name osnetwork --description openstack-network --publicNetID \nid of an OpenStack public network\n --subnet 10.0.0.0/16\n\n\n\n\nOther available options:\n\n\n--publicInAccount\n flags if the network is public in the account\n\n\nYou can check whether the network was created successfully by using the \nnetwork list\n command. Check the network and select it if you are happy with it:\n\n\nnetwork show --name osnetwork\n\nnetwork select --name osnetwork\n\n\n\n\nCreate a security group\n\n\nA security group gives developers and systems administrators an easy way to create and manage a collection of cloud infrastructure related security rules.\n\n\nsecuritygroup create --name secgroup_example --description securitygroup-example --rules 0.0.0.0/0:tcp:8080,9090;10.0.33.0/24:tcp:1234,1235\n\n\n\n\nYou can check whether the security group was created successfully by using the \nsecuritygroup list\n command. Check the security group and select it if you are happy with it:\n\n\nsecuritygroup show --name secgroup_example\n\nsecuritygroup select --name secgroup_example\n\n\n\n\nThere are two default security groups defined: \nall-services-port\n and \nonly-ssh-and-ssl\n\n\nonly-ssh-and-ssl:\n all ports are locked down (you can't access Hadoop services outside of the Virtual Private Cloud)\n\n\n\n\nSSH (22)\n\n\nHTTPS (443)\n\n\n\n\nall-services-port:\n all Hadoop services + SSH/gateway HTTPS are accessible by default:\n\n\n\n\nSSH (22)\n\n\nHTTPS (443)\n\n\nAmbari (8080)\n\n\nConsul (8500)\n\n\nNN (50070)\n\n\nRM Web (8088)\n\n\nScheduler (8030RM)\n\n\nIPC (8050RM)\n\n\nJob history server (19888)\n\n\nHBase master (60000)\n\n\nHBase master web (60010)\n\n\nHBase RS (16020)\n\n\nHBase RS info (60030)\n\n\nFalcon (15000)\n\n\nStorm (8744)\n\n\nHive metastore (9083)\n\n\nHive server (10000)\n\n\nHive server HTTP (10001)\n\n\nAccumulo master (9999)\n\n\nAccumulo Tserver (9997)\n\n\nAtlas (21000)\n\n\nKNOX (8443)\n\n\nOozie (11000)\n\n\nSpark HS (18080)\n\n\nNM Web (8042)\n\n\nZeppelin WebSocket (9996)\n\n\nZeppelin UI (9995)\n\n\nKibana (3080)\n\n\nElasticsearch (9200)\n\n\n\n\nConfigure instance groups\n\n\nYou have to configure the instancegroups before the provisioning. An instancegroup is defining a group of your nodes with a specified template. Usually we create instancegroups for the hostgroups defined in the blueprints.\n\n\ninstancegroup configure --instanceGroup host_group_slave_1 --nodecount 3 --templateName ostemplate\n\n\n\n\nOther available options:\n\n\n--templateId\n Id of the template\n\n\nCreate a Hadoop cluster\n\n\nYou are almost done - two more command and this will create your Hadoop cluster on your favorite cloud provider. Same as the API, or UI this will use your \ncredential\n, \ninstancegroups\n, \nnetwork\n, \nsecuritygroup\n, and by using OpenStack Heat will launch a cloud stack\n\n\nstack create --name my-first-stack --region local\n\n\n\n\nOnce the \nstack\n is up and running (cloud provisioning is done) it will use your selected \nblueprint\n and install your custom Hadoop cluster with the selected components and services.\n\n\ncluster create --description \nmy first cluster\n\n\n\n\n\nYou are done - you can check the progress through the Ambari UI. If you log back to Cloudbreak UI you can check the progress over there as well, and learn the IP address of Ambari.\n\n\nStop/Restart cluster and stack\n\n\nYou have the ability to \nstop your existing stack then its cluster\n if you want to suspend the work on it.\n\n\nSelect a stack for example with its name:\n\n\nstack select --name my-stack\n\n\n\n\nOther available option to define a stack is its \n--id\n (instead of the \n--name\n).\n\n\nApply the following commands to stop the previously selected stack:\n\n\ncluster stop\nstack stop\n\n\n\n\n\n\nImportant!\n The related cluster should be stopped before you can stop the stack.\n\n\n\n\nApply the following command to \nrestart the previously selected and stopped stack\n:\n\n\nstack start\n\n\n\n\nAfter the selected stack has restarted, you can \nrestart the related cluster as well\n:\n\n\ncluster start\n\n\n\n\nUpscale/Downscale cluster and stack\n\n\nYou can \nupscale your selected stack\n if you need more instances to your infrastructure:\n\n\nstack node --ADD --instanceGroup host_group_slave_1 --adjustment 6\n\n\n\n\nOther available options:\n\n\n--withClusterUpScale\n indicates cluster upscale after stack upscale\nor you can upscale the related cluster separately as well:\n\n\ncluster node --ADD --hostgroup host_group_slave_1 --adjustment 6\n\n\n\n\nApply the following command to \ndownscale the previously selected stack\n:\n\n\nstack node --REMOVE  --instanceGroup host_group_slave_1 --adjustment -2\n\n\n\n\nand the related cluster separately:\n\n\ncluster node --REMOVE  --hostgroup host_group_slave_1 --adjustment -2\n\n\n\n\nSilent mode\n\n\nWith Cloudbreak shell you can execute script files as well. A script file contains cloudbreak shell commands and can be executed with the \nscript\n cloudbreak shell command\n\n\nscript \nyour script file\n\n\n\n\n\nor with the \ncbd util cloudbreak-shell-quiet\n cbd command:\n\n\ncbd util cloudbreak-shell-quiet \n example.sh\n\n\n\n\nExample\n\n\nThe following example creates a hadoop cluster with \nhdp-small-default\n blueprint on \nm1.large\n instances with 2X100G attached disks on \nosnetwork\n network using \nall-services-port\n security group. You should copy your ssh public key file into your cbd working directory with name \nid_rsa.pub\n and change the \n...\n parts with your openstack credential and network details.\n\n\ncredential create --OPENSTACK --name my-os-credential --description \ncredentail description\n --userName \nOpenStack username\n --password \nOpenStack password\n --tenantName \nOpenStack tenant name\n --endPoint \nOpenStack Identity Service (Keystone) endpoint\n --sshKeyPath \npath of your public SSH key file\n\ncredential select --name my-os-credential\ntemplate create --OPENSTACK --name ostemplate --description openstack-template --instanceType m1.large --volumeSize 100 --volumeCount 2\nblueprint select --name hdp-small-default\ninstancegroup configure --instanceGroup cbgateway --nodecount 1 --templateName ostemplate\ninstancegroup configure --instanceGroup host_group_master_1 --nodecount 1 --templateName ostemplate\ninstancegroup configure --instanceGroup host_group_master_2 --nodecount 1 --templateName ostemplate\ninstancegroup configure --instanceGroup host_group_master_3 --nodecount 1 --templateName ostemplate\ninstancegroup configure --instanceGroup host_group_client_1  --nodecount 1 --templateName ostemplate\ninstancegroup configure --instanceGroup host_group_slave_1 --nodecount 3 --templateName ostemplate\nnetwork create --OPENSTACK --name osnetwork --description openstack-network --publicNetID \nid of an OpenStack public network\n --subnet 10.0.0.0/16\nnetwork select --name osnetwork\nsecuritygroup select --name all-services-port\nstack create --name my-first-stack --region local\ncluster create --description \nMy first cluster\n\n\n\n\n\nNext steps\n\n\nCongrats! Your cluster should now be up and running. To learn more about it we have some \ninteresting insights\n about Cloudbreak clusters.", 
            "title": "Provisioning - CLI"
        }, 
        {
            "location": "/openstack_cb_shell/#interactive-mode", 
            "text": "Start the shell with  cbd util cloudbreak-shell . This will launch the Cloudbreak shell inside a Docker container and you are ready to start using it.  You have to copy files into the cbd working directory, which you would like to use from shell. For example if your  cbd  working directory is  ~/prj/cbd  then copy your blueprint and public ssh key file into this directory. You can refer to these files with their names from the shell.  Create a cloud credential  credential create --OPENSTACK --name my-os-credential --description  credentail description  --userName  OpenStack username  --password  OpenStack password  --tenantName  OpenStack tenant name  --endPoint  OpenStack Identity Service (Keystone) endpoint  --sshKeyPath  path of your public SSH key file   Alternatively you can upload your public key from an url as well, by using the  \u2014sshKeyUrl  switch. You can check whether the credential was created successfully by using the  credential list  command. You can switch between your cloud credentials - when you\u2019d like to use one and act with that you will have to use:  credential select --name my-openstack-credential  Create a template  A template gives developers and systems administrators an easy way to create and manage a collection of cloud infrastructure related resources, maintaining and updating them in an orderly and predictable fashion. A template can be used repeatedly to create identical copies of the same stack (or to use as a foundation to start a new stack).  template create --OPENSTACK --name ostemplate --description openstack-template --instanceType m1.large --volumeSize 100 --volumeCount 2  You can check whether the template was created successfully by using the  template list  or  template show  command.  You can delete your cloud template - when you\u2019d like to delete one you will have to use:  template delete --name ostemplate  Create or select a blueprint  You can define Ambari blueprints with cloudbreak-shell:  blueprint add --name myblueprint --description myblueprint-description --file  the path of the blueprint   Other available options:  --url  the url of the blueprint  --publicInAccount  flags if the network is public in the account  We ship default Ambari blueprints with Cloudbreak. You can use these blueprints or add yours. To see the available blueprints and use one of them please use:  blueprint list\n\nblueprint select --name hdp-small-default  Create a network  A network gives developers and systems administrators an easy way to create and manage a collection of cloud infrastructure related networking, maintaining and updating them in an orderly and predictable fashion. A network can be used repeatedly to create identical copies of the same stack (or to use as a foundation to start a new stack).  network create --OPENSTACK --name osnetwork --description openstack-network --publicNetID  id of an OpenStack public network  --subnet 10.0.0.0/16  Other available options:  --publicInAccount  flags if the network is public in the account  You can check whether the network was created successfully by using the  network list  command. Check the network and select it if you are happy with it:  network show --name osnetwork\n\nnetwork select --name osnetwork  Create a security group  A security group gives developers and systems administrators an easy way to create and manage a collection of cloud infrastructure related security rules.  securitygroup create --name secgroup_example --description securitygroup-example --rules 0.0.0.0/0:tcp:8080,9090;10.0.33.0/24:tcp:1234,1235  You can check whether the security group was created successfully by using the  securitygroup list  command. Check the security group and select it if you are happy with it:  securitygroup show --name secgroup_example\n\nsecuritygroup select --name secgroup_example  There are two default security groups defined:  all-services-port  and  only-ssh-and-ssl  only-ssh-and-ssl:  all ports are locked down (you can't access Hadoop services outside of the Virtual Private Cloud)   SSH (22)  HTTPS (443)   all-services-port:  all Hadoop services + SSH/gateway HTTPS are accessible by default:   SSH (22)  HTTPS (443)  Ambari (8080)  Consul (8500)  NN (50070)  RM Web (8088)  Scheduler (8030RM)  IPC (8050RM)  Job history server (19888)  HBase master (60000)  HBase master web (60010)  HBase RS (16020)  HBase RS info (60030)  Falcon (15000)  Storm (8744)  Hive metastore (9083)  Hive server (10000)  Hive server HTTP (10001)  Accumulo master (9999)  Accumulo Tserver (9997)  Atlas (21000)  KNOX (8443)  Oozie (11000)  Spark HS (18080)  NM Web (8042)  Zeppelin WebSocket (9996)  Zeppelin UI (9995)  Kibana (3080)  Elasticsearch (9200)   Configure instance groups  You have to configure the instancegroups before the provisioning. An instancegroup is defining a group of your nodes with a specified template. Usually we create instancegroups for the hostgroups defined in the blueprints.  instancegroup configure --instanceGroup host_group_slave_1 --nodecount 3 --templateName ostemplate  Other available options:  --templateId  Id of the template  Create a Hadoop cluster  You are almost done - two more command and this will create your Hadoop cluster on your favorite cloud provider. Same as the API, or UI this will use your  credential ,  instancegroups ,  network ,  securitygroup , and by using OpenStack Heat will launch a cloud stack  stack create --name my-first-stack --region local  Once the  stack  is up and running (cloud provisioning is done) it will use your selected  blueprint  and install your custom Hadoop cluster with the selected components and services.  cluster create --description  my first cluster   You are done - you can check the progress through the Ambari UI. If you log back to Cloudbreak UI you can check the progress over there as well, and learn the IP address of Ambari.  Stop/Restart cluster and stack  You have the ability to  stop your existing stack then its cluster  if you want to suspend the work on it.  Select a stack for example with its name:  stack select --name my-stack  Other available option to define a stack is its  --id  (instead of the  --name ).  Apply the following commands to stop the previously selected stack:  cluster stop\nstack stop   Important!  The related cluster should be stopped before you can stop the stack.   Apply the following command to  restart the previously selected and stopped stack :  stack start  After the selected stack has restarted, you can  restart the related cluster as well :  cluster start  Upscale/Downscale cluster and stack  You can  upscale your selected stack  if you need more instances to your infrastructure:  stack node --ADD --instanceGroup host_group_slave_1 --adjustment 6  Other available options:  --withClusterUpScale  indicates cluster upscale after stack upscale\nor you can upscale the related cluster separately as well:  cluster node --ADD --hostgroup host_group_slave_1 --adjustment 6  Apply the following command to  downscale the previously selected stack :  stack node --REMOVE  --instanceGroup host_group_slave_1 --adjustment -2  and the related cluster separately:  cluster node --REMOVE  --hostgroup host_group_slave_1 --adjustment -2", 
            "title": "Interactive mode"
        }, 
        {
            "location": "/openstack_cb_shell/#silent-mode", 
            "text": "With Cloudbreak shell you can execute script files as well. A script file contains cloudbreak shell commands and can be executed with the  script  cloudbreak shell command  script  your script file   or with the  cbd util cloudbreak-shell-quiet  cbd command:  cbd util cloudbreak-shell-quiet   example.sh", 
            "title": "Silent mode"
        }, 
        {
            "location": "/openstack_cb_shell/#example", 
            "text": "The following example creates a hadoop cluster with  hdp-small-default  blueprint on  m1.large  instances with 2X100G attached disks on  osnetwork  network using  all-services-port  security group. You should copy your ssh public key file into your cbd working directory with name  id_rsa.pub  and change the  ...  parts with your openstack credential and network details.  credential create --OPENSTACK --name my-os-credential --description  credentail description  --userName  OpenStack username  --password  OpenStack password  --tenantName  OpenStack tenant name  --endPoint  OpenStack Identity Service (Keystone) endpoint  --sshKeyPath  path of your public SSH key file \ncredential select --name my-os-credential\ntemplate create --OPENSTACK --name ostemplate --description openstack-template --instanceType m1.large --volumeSize 100 --volumeCount 2\nblueprint select --name hdp-small-default\ninstancegroup configure --instanceGroup cbgateway --nodecount 1 --templateName ostemplate\ninstancegroup configure --instanceGroup host_group_master_1 --nodecount 1 --templateName ostemplate\ninstancegroup configure --instanceGroup host_group_master_2 --nodecount 1 --templateName ostemplate\ninstancegroup configure --instanceGroup host_group_master_3 --nodecount 1 --templateName ostemplate\ninstancegroup configure --instanceGroup host_group_client_1  --nodecount 1 --templateName ostemplate\ninstancegroup configure --instanceGroup host_group_slave_1 --nodecount 3 --templateName ostemplate\nnetwork create --OPENSTACK --name osnetwork --description openstack-network --publicNetID  id of an OpenStack public network  --subnet 10.0.0.0/16\nnetwork select --name osnetwork\nsecuritygroup select --name all-services-port\nstack create --name my-first-stack --region local\ncluster create --description  My first cluster", 
            "title": "Example"
        }, 
        {
            "location": "/openstack_cb_shell/#next-steps", 
            "text": "Congrats! Your cluster should now be up and running. To learn more about it we have some  interesting insights  about Cloudbreak clusters.", 
            "title": "Next steps"
        }, 
        {
            "location": "/api/", 
            "text": "API documentation\n\n\nCloudbreak is a RESTful application development platform with the goal of helping developers to build solutions for deploying HDP clusters in different environments. Once it is deployed in your favourite servlet container it exposes a REST API allowing to span up Hadoop clusters of arbitary sizes and cloud providers.\n\n\nThe \nAPI documentation\n is generated from the code using \nSwagger\n.", 
            "title": "API"
        }, 
        {
            "location": "/api/#api-documentation", 
            "text": "Cloudbreak is a RESTful application development platform with the goal of helping developers to build solutions for deploying HDP clusters in different environments. Once it is deployed in your favourite servlet container it exposes a REST API allowing to span up Hadoop clusters of arbitary sizes and cloud providers.  The  API documentation  is generated from the code using  Swagger .", 
            "title": "API documentation"
        }, 
        {
            "location": "/shell/", 
            "text": "Cloudbreak Shell\n\n\nThe goal with the CLI was to provide an interactive command line tool which supports:\n\n\n\n\nall functionality available through the REST API or Cloudbreak web UI\n\n\nmakes possible complete automation of management task via \nscripts\n\n\ncontext aware command availability\n\n\ntab completion\n\n\nrequired/optional parameter support\n\n\nhint\n command to guide you on the usual path\n\n\n\n\nInstall and start Cloudbreak shell\n\n\nYou have a few options to give it a try:\n\n\n\n\nuse Cloudreak deployer - \nrecommended\n\n\nuse our prepared docker image\n\n\nbuild it from source\n\n\n\n\n\n\nStarting cloudbreak shell using cloudbreak deployer\n\n\nStart the shell with \ncbd util cloudbreak-shell\n. This will launch the Cloudbreak shell inside a Docker container and you are ready to start using it.\n\n\n\n\nStarting Cloudbreak shell with our prepared docker image\n\n\nYou can find the docker image and its documentation \nhere\n.\n\n\n\n\nBuild from source\n\n\nIf want to use the code or extend it with new commands follow the steps below. You will need:\n- jdk 1.7\n\n\ngit clone https://github.com/sequenceiq/cloudbreak-shell.git\ncd cloudbreak-shell\n./gradlew clean build\n\n\n\n\n\n\nNote\n\nIn case you use the hosted version of Cloudbreak you should use the \nlatest-release.sh\n to get the right version of the CLI.\n\n\n\n\nStart Cloudbreak-shell from the built source\n\n\nUsage:\n  java -jar cloudbreak-shell-0.5-SNAPSHOT.jar                  : Starts Cloudbreak Shell in interactive mode.\n  java -jar cloudbreak-shell-0.5-SNAPSHOT.jar --cmdfile=\nFILE\n : Cloudbreak executes commands read from the file.\n\nOptions:\n  --cloudbreak.address=\nhttp[s]://HOSTNAME:PORT\n  Address of the Cloudbreak Server [default: https://cloudbreak-api.sequenceiq.com].\n  --identity.address=\nhttp[s]://HOSTNAME:PORT\n    Address of the SequenceIQ identity server [default: https://identity.sequenceiq.com].\n  --sequenceiq.user=\nUSER\n                        Username of the SequenceIQ user [default: user@sequenceiq.com].\n  --sequenceiq.password=\nPASSWORD\n                Password of the SequenceIQ user [default: password].\n\nNote:\n  You should specify at least your username and password.\n\n\n\n\nOnce you are connected you can start to create a cluster. If you are lost and need guidance through the process you can use \nhint\n. You can always use \nTAB\n for completion.\n\n\n\n\nNote\n\nAll commands are \ncontext aware\n - they are available only when it makes sense - this way you are never confused and guided by the system on the right path.\n\n\n\n\nProvider specific documentations\n\n\n\n\nAWS\n\n\nAzure\n\n\nGCP\n\n\nOpenStack\n\n\n\n\nor you can find a more detailed documentation about Cloudbreak-shell in its \nGithub repositiry\n.", 
            "title": "CLI/Shell"
        }, 
        {
            "location": "/shell/#cloudbreak-shell", 
            "text": "The goal with the CLI was to provide an interactive command line tool which supports:   all functionality available through the REST API or Cloudbreak web UI  makes possible complete automation of management task via  scripts  context aware command availability  tab completion  required/optional parameter support  hint  command to guide you on the usual path", 
            "title": "Cloudbreak Shell"
        }, 
        {
            "location": "/shell/#install-and-start-cloudbreak-shell", 
            "text": "You have a few options to give it a try:   use Cloudreak deployer -  recommended  use our prepared docker image  build it from source    Starting cloudbreak shell using cloudbreak deployer  Start the shell with  cbd util cloudbreak-shell . This will launch the Cloudbreak shell inside a Docker container and you are ready to start using it.   Starting Cloudbreak shell with our prepared docker image  You can find the docker image and its documentation  here .   Build from source  If want to use the code or extend it with new commands follow the steps below. You will need:\n- jdk 1.7  git clone https://github.com/sequenceiq/cloudbreak-shell.git\ncd cloudbreak-shell\n./gradlew clean build   Note \nIn case you use the hosted version of Cloudbreak you should use the  latest-release.sh  to get the right version of the CLI.   Start Cloudbreak-shell from the built source  Usage:\n  java -jar cloudbreak-shell-0.5-SNAPSHOT.jar                  : Starts Cloudbreak Shell in interactive mode.\n  java -jar cloudbreak-shell-0.5-SNAPSHOT.jar --cmdfile= FILE  : Cloudbreak executes commands read from the file.\n\nOptions:\n  --cloudbreak.address= http[s]://HOSTNAME:PORT   Address of the Cloudbreak Server [default: https://cloudbreak-api.sequenceiq.com].\n  --identity.address= http[s]://HOSTNAME:PORT     Address of the SequenceIQ identity server [default: https://identity.sequenceiq.com].\n  --sequenceiq.user= USER                         Username of the SequenceIQ user [default: user@sequenceiq.com].\n  --sequenceiq.password= PASSWORD                 Password of the SequenceIQ user [default: password].\n\nNote:\n  You should specify at least your username and password.  Once you are connected you can start to create a cluster. If you are lost and need guidance through the process you can use  hint . You can always use  TAB  for completion.   Note \nAll commands are  context aware  - they are available only when it makes sense - this way you are never confused and guided by the system on the right path.   Provider specific documentations   AWS  Azure  GCP  OpenStack   or you can find a more detailed documentation about Cloudbreak-shell in its  Github repositiry .", 
            "title": "Install and start Cloudbreak shell"
        }, 
        {
            "location": "/periscope/", 
            "text": "Auto-Scaling\n\n\nThe purpose of \nauto-scaling\n is to apply SLA scaling policies to a Cloudbreak-managed Hadoop cluster.\n\n\n\n\nThis feature is currently \nTECHNICAL PREVIEW\n.\n\n\n\n\nHow It Works\n\n\nThe auto-scaling capabilities is based on \nAmbari Metrics\n - and \nAmbari Alerts\n. Based on the Blueprint\nused and the running services, Cloudbreak can access all the available metrics from the subsystem and define \nalerts\n based on this information.\n\n\nBeside the default Ambari Metrics, Cloudbreak includes two custom metrics: \nPending YARN containers\n and \nPending applications\n. These two custom metrics works with the YARN subsystem in order to bring \napplication\n level QoS to the cluster.\n\n\n\n\nIn order to use the \nautoscaling\n feature with Cloudbreak you will have to enable from the UI or shell.\n\n\n\n\n\n\nAlerts\n\n\nAuto-scaling supports two \nAlert\n types: \nmetric\n and \ntime\n based.\n\n\nMetric-based Alerts\n\n\nMetric based alerts are using the default (or custom) Ambari metrics. These metrics have a default \nThreshold\n value configured in Ambari - nevertheless these thresholds can be configured, changed or altered in Ambari. In order to change the default threshold for a metric please go to Ambari UI and select the \nAlerts\n tab and the metric. The values can be changed in the \nThreshold\n section.\n\n\n\n\nMetric alerts have a few configurable fields.\n\n\n\n\nalert name\n - name of the alert\n\n\ndescription\n - description of the alert\n\n\nmetric - desired state\n - the Ambari metrics based on the installed services and their \nstate\n (OK, WARN, CRITICAL), based on the \nthreshold\n value\n\n\nperiod\n - for how many \nminutes\n the metric state has to be sustained in order for an alert to be triggered\n\n\n\n\n\n\nTime-based Alerts\n\n\nTime based alerts are based on \ncron\n expressions and allow alerts to be triggered based on time.\n\n\nTime alerts have a few configurable fields.\n\n\n\n\nalert name\n - name of the alert\n\n\ndescription\n - description of the alert\n\n\ntime zone\n - the time zone\n\n\ncrom expression\n - the \ncron\n expression to be used for the alert\n\n\n\n\n\n\nScaling Policies\n\n\nScaling is the ability to increase or decrease the capacity of the Hadoop cluster or application based on an alert.\nWhen scaling policies are used, the capacity is automatically increased or decreased according to the conditions defined.\nCloudbreak will do the heavy lifting and based on the alerts and the scaling policy linked to them it executes the associated policy. We scaling granularity is at the \nhostgroup\n level - thus you have the option to scale services or components only, not the whole cluster.\n\n\nScaling policies have a few configurable fields.\n\n\n\n\npolicy name\n - name of the scaling policy\n\n\nscaling adjustment\n - the number of added or removed noded based on \nnode count\n (the number of nodes), \npercentage\n (computed percentage adjustment based on the cluster size) and \nexact\n (a given exact size of the cluster)\n\n\nhost group\n - the Ambari hostgroup to be scaled\n\n\nalert\n - the triggered alert based on that the scaling policy applies\n\n\n\n\n\n\nCluster Scaling Configuration\n\n\nAn SLA scaling policy can contain multiple alerts. When an alert is triggered a \nscaling adjustment\n is applied, however to keep the cluster size within boundaries a \ncluster size min.\n and \ncluster size max.\n is attached to the cluster - thus a scaling policy can never over or undersize a cluster. Also in order to avoid stressing the cluster we have introduced a \ncooldown time\n period (minutes) - though an alert is raised and there is an associated scaling policy, the system will not apply the policy within the configured timeframe. In an SLA scaling policy the triggered rules are applied in order.\n\n\n\n\ncooldown time\n - period (minutes) between two scaling events while the cluster is locked from adjustments\n\n\ncluster size min.\n - size will never go under the minimum value, despite scaling adjustments\n\n\ncluster size max.\n - size will never go above the maximum value, despite scaling adjustments\n\n\n\n\n\n\nDownscale Scaling Considerations\n\n\nCloudbreak auto-scaling will try to keep a healthy cluster, thus does several background checks during \ndownscale\n.\n\n\n\n\nWe never remove \nApplication master nodes\n from a cluster. In order to make sure that a node running AM is not removed, Cloudbreak has to be able to access the YARN Resource Manager - when creating a cluster using the \ndefault\n secure network template please make sure that the RM's port is open on the node\n\n\nIn order to keep a healthy HDFS during downscale we always keep the configured \nreplication\n factor and make sure there is enough \nspace\n on HDFS to rebalance data. Also during downscale in order to minimize the rebalancing, replication and HDFS storms we check block locations and compute the least costly operations.", 
            "title": "Auto-Scaling"
        }, 
        {
            "location": "/periscope/#auto-scaling", 
            "text": "The purpose of  auto-scaling  is to apply SLA scaling policies to a Cloudbreak-managed Hadoop cluster.   This feature is currently  TECHNICAL PREVIEW .", 
            "title": "Auto-Scaling"
        }, 
        {
            "location": "/periscope/#how-it-works", 
            "text": "The auto-scaling capabilities is based on  Ambari Metrics  - and  Ambari Alerts . Based on the Blueprint\nused and the running services, Cloudbreak can access all the available metrics from the subsystem and define  alerts  based on this information.  Beside the default Ambari Metrics, Cloudbreak includes two custom metrics:  Pending YARN containers  and  Pending applications . These two custom metrics works with the YARN subsystem in order to bring  application  level QoS to the cluster.   In order to use the  autoscaling  feature with Cloudbreak you will have to enable from the UI or shell.    Alerts  Auto-scaling supports two  Alert  types:  metric  and  time  based.  Metric-based Alerts  Metric based alerts are using the default (or custom) Ambari metrics. These metrics have a default  Threshold  value configured in Ambari - nevertheless these thresholds can be configured, changed or altered in Ambari. In order to change the default threshold for a metric please go to Ambari UI and select the  Alerts  tab and the metric. The values can be changed in the  Threshold  section.   Metric alerts have a few configurable fields.   alert name  - name of the alert  description  - description of the alert  metric - desired state  - the Ambari metrics based on the installed services and their  state  (OK, WARN, CRITICAL), based on the  threshold  value  period  - for how many  minutes  the metric state has to be sustained in order for an alert to be triggered    Time-based Alerts  Time based alerts are based on  cron  expressions and allow alerts to be triggered based on time.  Time alerts have a few configurable fields.   alert name  - name of the alert  description  - description of the alert  time zone  - the time zone  crom expression  - the  cron  expression to be used for the alert    Scaling Policies  Scaling is the ability to increase or decrease the capacity of the Hadoop cluster or application based on an alert.\nWhen scaling policies are used, the capacity is automatically increased or decreased according to the conditions defined.\nCloudbreak will do the heavy lifting and based on the alerts and the scaling policy linked to them it executes the associated policy. We scaling granularity is at the  hostgroup  level - thus you have the option to scale services or components only, not the whole cluster.  Scaling policies have a few configurable fields.   policy name  - name of the scaling policy  scaling adjustment  - the number of added or removed noded based on  node count  (the number of nodes),  percentage  (computed percentage adjustment based on the cluster size) and  exact  (a given exact size of the cluster)  host group  - the Ambari hostgroup to be scaled  alert  - the triggered alert based on that the scaling policy applies    Cluster Scaling Configuration  An SLA scaling policy can contain multiple alerts. When an alert is triggered a  scaling adjustment  is applied, however to keep the cluster size within boundaries a  cluster size min.  and  cluster size max.  is attached to the cluster - thus a scaling policy can never over or undersize a cluster. Also in order to avoid stressing the cluster we have introduced a  cooldown time  period (minutes) - though an alert is raised and there is an associated scaling policy, the system will not apply the policy within the configured timeframe. In an SLA scaling policy the triggered rules are applied in order.   cooldown time  - period (minutes) between two scaling events while the cluster is locked from adjustments  cluster size min.  - size will never go under the minimum value, despite scaling adjustments  cluster size max.  - size will never go above the maximum value, despite scaling adjustments    Downscale Scaling Considerations  Cloudbreak auto-scaling will try to keep a healthy cluster, thus does several background checks during  downscale .   We never remove  Application master nodes  from a cluster. In order to make sure that a node running AM is not removed, Cloudbreak has to be able to access the YARN Resource Manager - when creating a cluster using the  default  secure network template please make sure that the RM's port is open on the node  In order to keep a healthy HDFS during downscale we always keep the configured  replication  factor and make sure there is enough  space  on HDFS to rebalance data. Also during downscale in order to minimize the rebalancing, replication and HDFS storms we check block locations and compute the least costly operations.", 
            "title": "How It Works"
        }, 
        {
            "location": "/recipes/", 
            "text": "Recipes\n\n\nWith the help of Cloudbreak it is very easy to provision Hadoop clusters in the cloud from an Apache Ambari blueprint. Cloudbreak built in provisioning doesn't contain every use case, so we are introducing the concept of recipes.\n\n\nRecipes are basically script extensions to a cluster that run on a set of nodes before or after the Ambari cluster installation. With recipes it's quite easy for example to put a JAR file on the Hadoop classpath or run some custom scripts.\n\n\nIn Cloudbreak we supports two ways to configure recipe, we have downloadable and stored recipes.\n\n\nStored recipes\n\n\nAs the name mentions stored recipes are uploaded and stored in Cloudbreak via web interface or shell.\n\n\nThe easiest way to create a custom recipe:\n\n\n\n\ncreate your own pre and/or post scripts\n\n\nupload them on shell or web interface\n\n\n\n\nAdd recipe\n\n\nOn the web interface under \"manage recipes\" section you should create new recipe. Please select SCRIPT or FILE type plugin, and fill other required fields.\n\n\nTo add recipe via shell use the following command:\n\n\nrecipe store --name [recipe-name] --executionType [ONE_NODE|ALL_NODES] --preInstallScriptFile /path/of/the/pre-install-script --postInstallScriptFile /path/of/the/post-install-script\n\n\n\n\nThis command has optional parameters:\n\n\n--description\n \"string\" description of the recipe\n\n\n--timeout\n \"integer\" timeout of the script execution\n\n\n--publicInAccount\n \"flag\" flags if the template is public in the account\n\n\nIn the background Cloudbreak pushes recipe to Consul key/value store during cluster creation.\n\n\nNote\n Stored recipes has limitation on size, because they are stored in Consul key/value store, the base64 encoded content of the scripts must be less than 512kB.\n\n\nDownloadable recipes\n\n\nA downloadable recipe should be available on HTTP, HTTPS protocols optionally with basic authentication, or any kind of public Git repository.\n\n\nThis kind of recipe must contain a plugin.toml file, with some basic information about the recipe. Besides this at least a recipe-pre-install or a recipe-post-install script.\n\n\nContent of plugin.toml:\n\n\n[plugin]\nname = \n[recipe-name]\n\ndescription = \n[description-of-the-recipe]\n\nversion = \n1.0\n\nmaintainer_name = \n[maintainer-name]\n\nmaintainer_email = \n[maintainer-email]\n\nwebsite_url = \n[website-url]\n\n\n\n\n\nPre- and post scripts are regular shell scripts, and must be executable.\n\n\nTo configure recipe or recipe groups in Cloudbreak you have to create a descriptive JSON file and send it to Cloudbreak via our shell. On web interface you don't need to take care of this file.\n\n\n{\n  \nname\n: \n[recipe-name]\n,\n  \ndescription\n: \n[description-of-the-recipe]\n,\n  \nproperties\n: {\n    \n[key]\n: \n[value]\n\n  },\n  \nplugins\n: {\n      \ngit://github.com/account/recipe.git\n: \nONE_NODE\n\n      \nhttp://user:password@mydomain.com/my-recipe.tar\n: \nALL_NODES\n\n      \nhttps://mydomain.com/my-recipe.zip\n: \nALL_NODES\n\n  }\n}\n\n\n\n\nAt this point we need to understand some element of the JSON above.\n\n\nFirst of all \nproperties\n. Properties are saved to Consul key/value store, and they are available from the pre or post script by fetching http://localhost:8500/v1/kv/[key]?raw. The limitation of the value's base64 representation is 512kB. This option is a good choice if you want to write reusable recipes.\n\n\nThe next one is \nplugins\n. As you read before we support a few kind of protocols, and each of them has their own limitations:\n\n\n\n\n\n\nGit\n\n\n\n\ngit repository must be public (or available from the cluster)\n\n\nthe recipe files must be on the root\n\n\nonly repository default branch supported, there is no opportunity to check out different branch\n\n\n\n\n\n\n\n\nHTTP(S)\n\n\n\n\non this kind of protocols you have to bundle your recipe into a tar or zip file\n\n\nbasic authentication is the only way to protect recipe from public\n\n\n\n\n\n\n\n\nLast one is the execution type of the recipe. We supports two options:\n\n\n\n\nONE_NODE means the recipe will execute only one node in the hostgroup\n\n\nAll_NODES runs every single instance in the hostgroup.\n\n\n\n\nAdd recipe\n\n\nOn the web interface please select URL type plugin, and fill other required fields.\n\n\nTo add recipe via shell use the command(s) below:\n\n\nrecipe add --file /path/of/the/recipe/json\n\n\n\n\nor\n\n\nrecipe add --url http(s)://mydomain.com/my-recipe.json\n\n\n\n\nAdd command has an optional parameter\n\n\n--publicInAccount\n is checked all the users belonging to your account will be able to use this recipe for create clusters, but cannot delete it.\n\n\nSample recipe for Ranger\n\n\nTo be able to install Ranger from a blueprint, a database must be running when Ambari starts to install Ranger Admin. With Cloudbreak a database can be configured and started from a recipe. We've created a sample recipe that can be used to initialize and start a PostgreSQL database that will be able to accept connections from Ranger and store its data. Add the \nONE_NODE\n recipe from \nthis URL\n on the Cloudbreak UI:\n\n\n\n\nAnd add this recipe to the same hostgroup where Ranger Admin is installed on the 'Choose Blueprint' when creating a new cluster:\n\n\n\n\nRanger installation also has some required properties that must be added to the blueprint. We've created a sample one-node blueprint with the necessary configurations to install Ranger Admin and Ranger Usersync. The configuration values in this blueprint match the sample recipe above - they are set to use a PostgreSQL database on the same host where Ranger Admin is installed. Usersync is configured to use UNIX as the authentication method and it should also be installed on the same host where Ranger Admin is installed.\n\n\n{\n  \nconfigurations\n: [\n    {\n      \nranger-site\n: {\n        \nproperties_attributes\n: {},\n        \nproperties\n: {}\n      }\n    },\n    {\n      \nranger-hdfs-policymgr-ssl\n: {\n        \nproperties_attributes\n: {},\n        \nproperties\n: {\n          \nxasecure.policymgr.clientssl.keystore\n: \n/etc/hadoop/conf/ranger-plugin-keystore.jks\n,\n          \nxasecure.policymgr.clientssl.keystore.credential.file\n: \njceks://file{{credential_file}}\n,\n          \nxasecure.policymgr.clientssl.truststore\n: \n/etc/hadoop/conf/ranger-plugin-truststore.jks\n,\n          \nxasecure.policymgr.clientssl.truststore.credential.file\n: \njceks://file{{credential_file}}\n\n        }\n      }\n    },\n    {\n      \nranger-ugsync-site\n: {\n        \nproperties_attributes\n: {},\n        \nproperties\n: {\n          \nranger.usersync.enabled\n: \ntrue\n,\n          \nranger.usersync.filesource.file\n: \n/tmp/usergroup.txt\n,\n          \nranger.usersync.filesource.text.delimiter\n: \n,\n,\n          \nranger.usersync.group.memberattributename\n: \nmember\n,\n          \nranger.usersync.group.nameattribute\n: \ncn\n,\n          \nranger.usersync.group.objectclass\n: \ngroupofnames\n,\n          \nranger.usersync.group.searchbase\n: \nou=groups,dc=hadoop,dc=apache,dc=org\n,\n          \nranger.usersync.group.searchenabled\n: \nfalse\n,\n          \nranger.usersync.group.searchfilter\n: \nempty\n,\n          \nranger.usersync.group.searchscope\n: \nsub\n,\n          \nranger.usersync.group.usermapsyncenabled\n: \nfalse\n,\n          \nranger.usersync.ldap.bindalias\n: \ntestldapalias\n,\n          \nranger.usersync.ldap.binddn\n: \ncn=admin,dc=xasecure,dc=net\n,\n          \nranger.usersync.ldap.bindkeystore\n: \n-\n,\n          \nranger.usersync.ldap.groupname.caseconversion\n: \nlower\n,\n          \nranger.usersync.ldap.searchBase\n: \ndc=hadoop,dc=apache,dc=org\n,\n          \nranger.usersync.ldap.url\n: \nldap://localhost:389\n,\n          \nranger.usersync.ldap.user.groupnameattribute\n: \nmemberof, ismemberof\n,\n          \nranger.usersync.ldap.user.nameattribute\n: \ncn\n,\n          \nranger.usersync.ldap.user.objectclass\n: \nperson\n,\n          \nranger.usersync.ldap.user.searchbase\n: \nou=users,dc=xasecure,dc=net\n,\n          \nranger.usersync.ldap.user.searchfilter\n: \nempty\n,\n          \nranger.usersync.ldap.user.searchscope\n: \nsub\n,\n          \nranger.usersync.ldap.username.caseconversion\n: \nlower\n,\n          \nranger.usersync.logdir\n: \n/var/log/ranger/usersync\n,\n          \nranger.usersync.pagedresultsenabled\n: \ntrue\n,\n          \nranger.usersync.pagedresultssize\n: \n500\n,\n          \nranger.usersync.policymanager.baseURL\n: \n{{ranger_external_url}}\n,\n          \nranger.usersync.policymanager.maxrecordsperapicall\n: \n1000\n,\n          \nranger.usersync.policymanager.mockrun\n: \nfalse\n,\n          \nranger.usersync.port\n: \n5151\n,\n          \nranger.usersync.sink.impl.class\n: \norg.apache.ranger.unixusersync.process.PolicyMgrUserGroupBuilder\n,\n          \nranger.usersync.sleeptimeinmillisbetweensynccycle\n: \n5\n,\n          \nranger.usersync.source.impl.class\n: \norg.apache.ranger.unixusersync.process.UnixUserGroupBuilder\n,\n          \nranger.usersync.ssl\n: \ntrue\n,\n          \nranger.usersync.unix.minUserId\n: \n500\n\n        }\n      }\n    },\n    {\n      \nadmin-properties\n: {\n        \nproperties_attributes\n: {},\n        \nproperties\n: {\n          \nDB_FLAVOR\n: \nPOSTGRES\n,\n          \nSQL_COMMAND_INVOKER\n: \npsql\n,\n          \nSQL_CONNECTOR_JAR\n: \n/var/lib/ambari-agent/tmp/postgres-jdbc-driver.jar\n,\n          \naudit_db_name\n: \nranger_audit\n,\n          \naudit_db_user\n: \nrangerlogger\n,\n          \ndb_host\n: \nlocalhost:5432\n,\n          \ndb_name\n: \nranger\n,\n          \ndb_root_user\n: \npostgres\n,\n          \ndb_root_password\n: \nadmin\n,\n          \ndb_user\n: \nrangeradmin\n,\n          \npolicymgr_external_url\n: \nhttp://localhost:6080\n,\n          \nranger_jdbc_connection_url\n: \njdbc:postgresql://{db_host}/ranger\n,\n          \nranger_jdbc_driver\n: \norg.postgresql.Driver\n\n        }\n      }\n    },\n    {\n      \nranger-admin-site\n: {\n        \nproperties_attributes\n: {},\n        \nproperties\n: {\n          \nranger.audit.source.type\n: \ndb\n,\n          \nranger.authentication.method\n: \nUNIX\n,\n          \nranger.credential.provider.path\n: \n/etc/ranger/admin/rangeradmin.jceks\n,\n          \nranger.externalurl\n: \n{{ranger_external_url}}\n,\n          \nranger.https.attrib.keystore.file\n: \n/etc/ranger/admin/keys/server.jks\n,\n          \nranger.jpa.audit.jdbc.credential.alias\n: \nrangeraudit\n,\n          \nranger.jpa.audit.jdbc.dialect\n: \n{{jdbc_dialect}}\n,\n          \nranger.jpa.audit.jdbc.driver\n: \n{{jdbc_driver}}\n,\n          \nranger.jpa.audit.jdbc.url\n: \n{{audit_jdbc_url}}\n,\n          \nranger.jpa.audit.jdbc.user\n: \n{{ranger_audit_db_user}}\n,\n          \nranger.jpa.jdbc.credential.alias\n: \nrangeradmin\n,\n          \nranger.jpa.jdbc.dialect\n: \n{{jdbc_dialect}}\n,\n          \nranger.jpa.jdbc.driver\n: \norg.postgresql.Driver\n,\n          \nranger.jpa.jdbc.url\n: \njdbc:postgresql://localhost:5432/ranger\n,\n          \nranger.jpa.jdbc.user\n: \n{{ranger_db_user}}\n,\n          \nranger.jpa.jdbc.password\n: \n{{ranger_db_password}}\n,\n          \nranger.ldap.ad.domain\n: \nlocalhost\n,\n          \nranger.ldap.ad.url\n: \nldap://ad.xasecure.net:389\n,\n          \nranger.ldap.group.roleattribute\n: \ncn\n,\n          \nranger.ldap.group.searchbase\n: \nou=groups,dc=xasecure,dc=net\n,\n          \nranger.ldap.group.searchfilter\n: \n(member=uid={0},ou=users,dc=xasecure,dc=net)\n,\n          \nranger.ldap.url\n: \nldap://localhost:389\n,\n          \nranger.ldap.user.dnpattern\n: \nuid={0},ou=users,dc=xasecure,dc=net\n,\n          \nranger.service.host\n: \n{{ranger_host}}\n,\n          \nranger.service.http.enabled\n: \ntrue\n,\n          \nranger.service.http.port\n: \n6080\n,\n          \nranger.service.https.attrib.clientAuth\n: \nfalse\n,\n          \nranger.service.https.attrib.keystore.keyalias\n: \nmkey\n,\n          \nranger.service.https.attrib.keystore.pass\n: \nranger\n,\n          \nranger.service.https.attrib.ssl.enabled\n: \nfalse\n,\n          \nranger.service.https.port\n: \n6182\n,\n          \nranger.unixauth.remote.login.enabled\n: \ntrue\n,\n          \nranger.unixauth.service.hostname\n: \nlocalhost\n,\n          \nranger.unixauth.service.port\n: \n5151\n\n        }\n      }\n    },\n    {\n      \nranger-env\n: {\n        \nproperties_attributes\n: {},\n        \nproperties\n: {\n          \nadmin_username\n: \nadmin\n,\n          \ncreate_db_dbuser\n: \ntrue\n,\n          \nranger_admin_log_dir\n: \n/var/log/ranger/admin\n,\n          \nranger_admin_username\n: \namb_ranger_admin\n,\n          \nranger_admin_password\n: \namb_ranger_pw\n,\n          \nranger_group\n: \nranger\n,\n          \nranger_jdbc_connection_url\n: \n{{ranger_jdbc_connection_url}}\n,\n          \nranger_jdbc_driver\n: \norg.postgresql.Driver\n,\n          \nranger_pid_dir\n: \n/var/run/ranger\n,\n          \nranger_user\n: \nranger\n,\n          \nranger_usersync_log_dir\n: \n/var/log/ranger/usersync\n,\n          \nxml_configurations_supported\n: \ntrue\n\n        }\n      }\n    },\n    {\n      \nranger-yarn-security\n: {\n        \nproperties_attributes\n: {},\n        \nproperties\n: {\n          \nranger.plugin.yarn.policy.cache.dir\n: \n/etc/ranger/{{repo_name}}/policycache\n,\n          \nranger.plugin.yarn.policy.pollIntervalMs\n: \n30000\n,\n          \nranger.plugin.yarn.policy.rest.ssl.config.file\n: \n/etc/yarn/conf/ranger-policymgr-ssl.xml\n,\n          \nranger.plugin.yarn.policy.rest.url\n: \n{{policymgr_mgr_url}}\n,\n          \nranger.plugin.yarn.policy.source.impl\n: \norg.apache.ranger.admin.client.RangerAdminRESTClient\n,\n          \nranger.plugin.yarn.service.name\n: \n{{repo_name}}\n\n        }\n      }\n    },\n    {\n      \nranger-yarn-audit\n: {\n        \nproperties_attributes\n: {},\n        \nproperties\n: {\n          \nxasecure.audit.credential.provider.file\n: \njceks://file{{credential_file}}\n,\n          \nxasecure.audit.db.async.max.flush.interval.ms\n: \n30000\n,\n          \nxasecure.audit.db.async.max.queue.size\n: \n10240\n,\n          \nxasecure.audit.db.batch.size\n: \n100\n,\n          \nxasecure.audit.db.is.async\n: \ntrue\n,\n          \nxasecure.audit.destination.db\n: \ntrue\n,\n          \nxasecure.audit.hdfs.async.max.flush.interval.ms\n: \n30000\n,\n          \nxasecure.audit.hdfs.async.max.queue.size\n: \n1048576\n,\n          \nxasecure.audit.destination.hdfs.dir\n: \n/ranger/audit/%app-type%/%time:yyyyMMdd%\n,\n          \nxasecure.audit.hdfs.config.destination.file\n: \n%hostname%-audit.log\n,\n          \nxasecure.audit.hdfs.config.destination.flush.interval.seconds\n: \n900\n,\n          \nxasecure.audit.hdfs.config.destination.open.retry.interval.seconds\n: \n60\n,\n          \nxasecure.audit.hdfs.config.destination.rollover.interval.seconds\n: \n86400\n,\n          \nxasecure.audit.hdfs.config.encoding\n: \n,\n          \nxasecure.audit.hdfs.config.local.archive.directory\n: \n/var/log/yarn/audit/archive\n,\n          \nxasecure.audit.hdfs.config.local.archive.max.file.count\n: \n10\n,\n          \nxasecure.audit.hdfs.config.local.buffer.directory\n: \n/var/log/yarn/audit\n,\n          \nxasecure.audit.hdfs.config.local.buffer.file\n: \n%time:yyyyMMdd-HHmm.ss%.log\n,\n          \nxasecure.audit.hdfs.config.local.buffer.file.buffer.size.bytes\n: \n8192\n,\n          \nxasecure.audit.hdfs.config.local.buffer.flush.interval.seconds\n: \n60\n,\n          \nxasecure.audit.hdfs.config.local.buffer.rollover.interval.seconds\n: \n600\n,\n          \nxasecure.audit.hdfs.is.async\n: \ntrue\n,\n          \nxasecure.audit.is.enabled\n: \ntrue\n,\n          \nxasecure.audit.jpa.javax.persistence.jdbc.driver\n: \n{{jdbc_driver}}\n,\n          \nxasecure.audit.jpa.javax.persistence.jdbc.url\n: \n{{audit_jdbc_url}}\n,\n          \nxasecure.audit.jpa.javax.persistence.jdbc.user\n: \n{{xa_audit_db_user}}\n,\n          \nxasecure.audit.kafka.async.max.flush.interval.ms\n: \n1000\n,\n          \nxasecure.audit.kafka.async.max.queue.size\n: \n1\n,\n          \nxasecure.audit.kafka.broker_list\n: \nlocalhost:9092\n,\n          \nxasecure.audit.kafka.is.enabled\n: \nfalse\n,\n          \nxasecure.audit.kafka.topic_name\n: \nranger_audits\n,\n          \nxasecure.audit.log4j.async.max.flush.interval.ms\n: \n30000\n,\n          \nxasecure.audit.log4j.async.max.queue.size\n: \n10240\n,\n          \nxasecure.audit.log4j.is.async\n: \nfalse\n,\n          \nxasecure.audit.log4j.is.enabled\n: \nfalse\n\n        }\n      }\n    },\n    {\n      \nranger-hdfs-security\n: {\n        \nproperties_attributes\n: {},\n        \nproperties\n: {\n          \nranger.plugin.hdfs.policy.cache.dir\n: \n/etc/ranger/{{repo_name}}/policycache\n,\n          \nranger.plugin.hdfs.policy.pollIntervalMs\n: \n30000\n,\n          \nranger.plugin.hdfs.policy.rest.ssl.config.file\n: \n/etc/hadoop/conf/ranger-policymgr-ssl.xml\n,\n          \nranger.plugin.hdfs.policy.rest.url\n: \n{{policymgr_mgr_url}}\n,\n          \nranger.plugin.hdfs.policy.source.impl\n: \norg.apache.ranger.admin.client.RangerAdminRESTClient\n,\n          \nranger.plugin.hdfs.service.name\n: \n{{repo_name}}\n,\n          \nxasecure.add-hadoop-authorization\n: \ntrue\n\n        }\n      }\n    },\n    {\n      \nranger-yarn-plugin-properties\n: {\n        \nproperties_attributes\n: {},\n        \nproperties\n: {\n          \nREPOSITORY_CONFIG_USERNAME\n: \nyarn\n,\n          \ncommon.name.for.certificate\n: \n-\n,\n          \nhadoop.rpc.protection\n: \n-\n,\n          \npolicy_user\n: \nambari-qa\n,\n          \nranger-yarn-plugin-enabled\n: \nNo\n\n        }\n      }\n    },\n    {\n      \nranger-hdfs-audit\n: {\n        \nproperties_attributes\n: {},\n        \nproperties\n: {\n          \nxasecure.audit.credential.provider.file\n: \njceks://file{{credential_file}}\n,\n          \nxasecure.audit.db.async.max.flush.interval.ms\n: \n30000\n,\n          \nxasecure.audit.db.async.max.queue.size\n: \n10240\n,\n          \nxasecure.audit.db.batch.size\n: \n100\n,\n          \nxasecure.audit.db.is.async\n: \ntrue\n,\n          \nxasecure.audit.destination.db\n: \ntrue\n,\n          \nxasecure.audit.destination.hdfs.dir\n: \n/ranger/audit/%app-type%/%time:yyyyMMdd%\n,\n          \nxasecure.audit.hdfs.async.max.flush.interval.ms\n: \n30000\n,\n          \nxasecure.audit.hdfs.async.max.queue.size\n: \n1048576\n,\n          \nxasecure.audit.hdfs.config.destination.file\n: \n%hostname%-audit.log\n,\n          \nxasecure.audit.hdfs.config.destination.flush.interval.seconds\n: \n900\n,\n          \nxasecure.audit.hdfs.config.destination.open.retry.interval.seconds\n: \n60\n,\n          \nxasecure.audit.hdfs.config.destination.rollover.interval.seconds\n: \n86400\n,\n          \nxasecure.audit.hdfs.config.encoding\n: \n,\n          \nxasecure.audit.hdfs.config.local.archive.directory\n: \n/var/log/hadoop/audit/archive/%app-type%\n,\n          \nxasecure.audit.hdfs.config.local.archive.max.file.count\n: \n10\n,\n          \nxasecure.audit.hdfs.config.local.buffer.directory\n: \n/var/log/hadoop/audit/%app-type%\n,\n          \nxasecure.audit.hdfs.config.local.buffer.file\n: \n%time:yyyyMMdd-HHmm.ss%.log\n,\n          \nxasecure.audit.hdfs.config.local.buffer.file.buffer.size.bytes\n: \n8192\n,\n          \nxasecure.audit.hdfs.config.local.buffer.flush.interval.seconds\n: \n60\n,\n          \nxasecure.audit.hdfs.config.local.buffer.rollover.interval.seconds\n: \n600\n,\n          \nxasecure.audit.hdfs.is.async\n: \ntrue\n,\n          \nxasecure.audit.is.enabled\n: \ntrue\n,\n          \nxasecure.audit.jpa.javax.persistence.jdbc.driver\n: \n{{jdbc_driver}}\n,\n          \nxasecure.audit.jpa.javax.persistence.jdbc.url\n: \n{{audit_jdbc_url}}\n,\n          \nxasecure.audit.jpa.javax.persistence.jdbc.user\n: \n{{xa_audit_db_user}}\n,\n          \nxasecure.audit.kafka.async.max.flush.interval.ms\n: \n1000\n,\n          \nxasecure.audit.kafka.async.max.queue.size\n: \n1\n,\n          \nxasecure.audit.kafka.broker_list\n: \nlocalhost:9092\n,\n          \nxasecure.audit.kafka.is.enabled\n: \nfalse\n,\n          \nxasecure.audit.kafka.topic_name\n: \nranger_audits\n,\n          \nxasecure.audit.log4j.async.max.flush.interval.ms\n: \n30000\n,\n          \nxasecure.audit.log4j.async.max.queue.size\n: \n10240\n,\n          \nxasecure.audit.log4j.is.async\n: \nfalse\n,\n          \nxasecure.audit.log4j.is.enabled\n: \nfalse\n\n        }\n      }\n    },\n    {\n      \nranger-hdfs-plugin-properties\n: {\n        \nproperties_attributes\n: {},\n        \nproperties\n: {\n          \nREPOSITORY_CONFIG_USERNAME\n: \nhadoop\n,\n          \ncommon.name.for.certificate\n: \n-\n,\n          \nhadoop.rpc.protection\n: \n-\n,\n          \npolicy_user\n: \nambari-qa\n,\n          \nranger-hdfs-plugin-enabled\n: \nNo\n\n        }\n      }\n    },\n    {\n      \nusersync-properties\n: {\n        \nproperties_attributes\n: {},\n        \nproperties\n: {}\n      }\n    }\n  ],\n  \nhost_groups\n: [\n    {\n      \ncomponents\n: [\n        {\n          \nname\n: \nNODEMANAGER\n\n        },\n        {\n          \nname\n: \nYARN_CLIENT\n\n        },\n        {\n          \nname\n: \nHDFS_CLIENT\n\n        },\n        {\n          \nname\n: \nHISTORYSERVER\n\n        },\n        {\n          \nname\n: \nMETRICS_MONITOR\n\n        },\n        {\n          \nname\n: \nNAMENODE\n\n        },\n        {\n          \nname\n: \nZOOKEEPER_CLIENT\n\n        },\n        {\n          \nname\n: \nRANGER_ADMIN\n\n        },\n        {\n          \nname\n: \nSECONDARY_NAMENODE\n\n        },\n        {\n          \nname\n: \nMAPREDUCE2_CLIENT\n\n        },\n        {\n          \nname\n: \nZOOKEEPER_SERVER\n\n        },\n        {\n          \nname\n: \nAMBARI_SERVER\n\n        },\n        {\n          \nname\n: \nDATANODE\n\n        },\n        {\n          \nname\n: \nRANGER_USERSYNC\n\n        },\n        {\n          \nname\n: \nAPP_TIMELINE_SERVER\n\n        },\n        {\n          \nname\n: \nMETRICS_COLLECTOR\n\n        },\n        {\n          \nname\n: \nRESOURCEMANAGER\n\n        }\n      ],\n      \nconfigurations\n: [],\n      \nname\n: \nhost_group_1\n,\n      \ncardinality\n: \n1\n\n    }\n  ],\n  \nBlueprints\n: {\n    \nstack_name\n: \nHDP\n,\n    \nstack_version\n: \n2.3\n,\n    \nblueprint_name\n: \nranger-psql-onenode-sample\n\n  }\n}\n\n\n\n\nNotes\n\n\n\n\nRanger plugins cannot be enabled by default in a blueprint due to some Ambari restrictions, so properties like \nranger-hdfs-plugin-enabled\n must be set to \nNo\n and the plugins must be enabled from the Ambari UI with the checkboxes and by restarting the necessary services.\n\n\nIf using the UNIX user sync, it may be necessary in some cases to restart the Ranger Usersync Services after the blueprint installation finished if the UNIX users cannot be seen on the Ranger Admin UI.", 
            "title": "Recipes"
        }, 
        {
            "location": "/recipes/#recipes", 
            "text": "With the help of Cloudbreak it is very easy to provision Hadoop clusters in the cloud from an Apache Ambari blueprint. Cloudbreak built in provisioning doesn't contain every use case, so we are introducing the concept of recipes.  Recipes are basically script extensions to a cluster that run on a set of nodes before or after the Ambari cluster installation. With recipes it's quite easy for example to put a JAR file on the Hadoop classpath or run some custom scripts.  In Cloudbreak we supports two ways to configure recipe, we have downloadable and stored recipes.", 
            "title": "Recipes"
        }, 
        {
            "location": "/recipes/#stored-recipes", 
            "text": "As the name mentions stored recipes are uploaded and stored in Cloudbreak via web interface or shell.  The easiest way to create a custom recipe:   create your own pre and/or post scripts  upload them on shell or web interface   Add recipe  On the web interface under \"manage recipes\" section you should create new recipe. Please select SCRIPT or FILE type plugin, and fill other required fields.  To add recipe via shell use the following command:  recipe store --name [recipe-name] --executionType [ONE_NODE|ALL_NODES] --preInstallScriptFile /path/of/the/pre-install-script --postInstallScriptFile /path/of/the/post-install-script  This command has optional parameters:  --description  \"string\" description of the recipe  --timeout  \"integer\" timeout of the script execution  --publicInAccount  \"flag\" flags if the template is public in the account  In the background Cloudbreak pushes recipe to Consul key/value store during cluster creation.  Note  Stored recipes has limitation on size, because they are stored in Consul key/value store, the base64 encoded content of the scripts must be less than 512kB.", 
            "title": "Stored recipes"
        }, 
        {
            "location": "/recipes/#downloadable-recipes", 
            "text": "A downloadable recipe should be available on HTTP, HTTPS protocols optionally with basic authentication, or any kind of public Git repository.  This kind of recipe must contain a plugin.toml file, with some basic information about the recipe. Besides this at least a recipe-pre-install or a recipe-post-install script.  Content of plugin.toml:  [plugin]\nname =  [recipe-name] \ndescription =  [description-of-the-recipe] \nversion =  1.0 \nmaintainer_name =  [maintainer-name] \nmaintainer_email =  [maintainer-email] \nwebsite_url =  [website-url]   Pre- and post scripts are regular shell scripts, and must be executable.  To configure recipe or recipe groups in Cloudbreak you have to create a descriptive JSON file and send it to Cloudbreak via our shell. On web interface you don't need to take care of this file.  {\n   name :  [recipe-name] ,\n   description :  [description-of-the-recipe] ,\n   properties : {\n     [key] :  [value] \n  },\n   plugins : {\n       git://github.com/account/recipe.git :  ONE_NODE \n       http://user:password@mydomain.com/my-recipe.tar :  ALL_NODES \n       https://mydomain.com/my-recipe.zip :  ALL_NODES \n  }\n}  At this point we need to understand some element of the JSON above.  First of all  properties . Properties are saved to Consul key/value store, and they are available from the pre or post script by fetching http://localhost:8500/v1/kv/[key]?raw. The limitation of the value's base64 representation is 512kB. This option is a good choice if you want to write reusable recipes.  The next one is  plugins . As you read before we support a few kind of protocols, and each of them has their own limitations:    Git   git repository must be public (or available from the cluster)  the recipe files must be on the root  only repository default branch supported, there is no opportunity to check out different branch     HTTP(S)   on this kind of protocols you have to bundle your recipe into a tar or zip file  basic authentication is the only way to protect recipe from public     Last one is the execution type of the recipe. We supports two options:   ONE_NODE means the recipe will execute only one node in the hostgroup  All_NODES runs every single instance in the hostgroup.   Add recipe  On the web interface please select URL type plugin, and fill other required fields.  To add recipe via shell use the command(s) below:  recipe add --file /path/of/the/recipe/json  or  recipe add --url http(s)://mydomain.com/my-recipe.json  Add command has an optional parameter  --publicInAccount  is checked all the users belonging to your account will be able to use this recipe for create clusters, but cannot delete it.", 
            "title": "Downloadable recipes"
        }, 
        {
            "location": "/recipes/#sample-recipe-for-ranger", 
            "text": "To be able to install Ranger from a blueprint, a database must be running when Ambari starts to install Ranger Admin. With Cloudbreak a database can be configured and started from a recipe. We've created a sample recipe that can be used to initialize and start a PostgreSQL database that will be able to accept connections from Ranger and store its data. Add the  ONE_NODE  recipe from  this URL  on the Cloudbreak UI:   And add this recipe to the same hostgroup where Ranger Admin is installed on the 'Choose Blueprint' when creating a new cluster:   Ranger installation also has some required properties that must be added to the blueprint. We've created a sample one-node blueprint with the necessary configurations to install Ranger Admin and Ranger Usersync. The configuration values in this blueprint match the sample recipe above - they are set to use a PostgreSQL database on the same host where Ranger Admin is installed. Usersync is configured to use UNIX as the authentication method and it should also be installed on the same host where Ranger Admin is installed.  {\n   configurations : [\n    {\n       ranger-site : {\n         properties_attributes : {},\n         properties : {}\n      }\n    },\n    {\n       ranger-hdfs-policymgr-ssl : {\n         properties_attributes : {},\n         properties : {\n           xasecure.policymgr.clientssl.keystore :  /etc/hadoop/conf/ranger-plugin-keystore.jks ,\n           xasecure.policymgr.clientssl.keystore.credential.file :  jceks://file{{credential_file}} ,\n           xasecure.policymgr.clientssl.truststore :  /etc/hadoop/conf/ranger-plugin-truststore.jks ,\n           xasecure.policymgr.clientssl.truststore.credential.file :  jceks://file{{credential_file}} \n        }\n      }\n    },\n    {\n       ranger-ugsync-site : {\n         properties_attributes : {},\n         properties : {\n           ranger.usersync.enabled :  true ,\n           ranger.usersync.filesource.file :  /tmp/usergroup.txt ,\n           ranger.usersync.filesource.text.delimiter :  , ,\n           ranger.usersync.group.memberattributename :  member ,\n           ranger.usersync.group.nameattribute :  cn ,\n           ranger.usersync.group.objectclass :  groupofnames ,\n           ranger.usersync.group.searchbase :  ou=groups,dc=hadoop,dc=apache,dc=org ,\n           ranger.usersync.group.searchenabled :  false ,\n           ranger.usersync.group.searchfilter :  empty ,\n           ranger.usersync.group.searchscope :  sub ,\n           ranger.usersync.group.usermapsyncenabled :  false ,\n           ranger.usersync.ldap.bindalias :  testldapalias ,\n           ranger.usersync.ldap.binddn :  cn=admin,dc=xasecure,dc=net ,\n           ranger.usersync.ldap.bindkeystore :  - ,\n           ranger.usersync.ldap.groupname.caseconversion :  lower ,\n           ranger.usersync.ldap.searchBase :  dc=hadoop,dc=apache,dc=org ,\n           ranger.usersync.ldap.url :  ldap://localhost:389 ,\n           ranger.usersync.ldap.user.groupnameattribute :  memberof, ismemberof ,\n           ranger.usersync.ldap.user.nameattribute :  cn ,\n           ranger.usersync.ldap.user.objectclass :  person ,\n           ranger.usersync.ldap.user.searchbase :  ou=users,dc=xasecure,dc=net ,\n           ranger.usersync.ldap.user.searchfilter :  empty ,\n           ranger.usersync.ldap.user.searchscope :  sub ,\n           ranger.usersync.ldap.username.caseconversion :  lower ,\n           ranger.usersync.logdir :  /var/log/ranger/usersync ,\n           ranger.usersync.pagedresultsenabled :  true ,\n           ranger.usersync.pagedresultssize :  500 ,\n           ranger.usersync.policymanager.baseURL :  {{ranger_external_url}} ,\n           ranger.usersync.policymanager.maxrecordsperapicall :  1000 ,\n           ranger.usersync.policymanager.mockrun :  false ,\n           ranger.usersync.port :  5151 ,\n           ranger.usersync.sink.impl.class :  org.apache.ranger.unixusersync.process.PolicyMgrUserGroupBuilder ,\n           ranger.usersync.sleeptimeinmillisbetweensynccycle :  5 ,\n           ranger.usersync.source.impl.class :  org.apache.ranger.unixusersync.process.UnixUserGroupBuilder ,\n           ranger.usersync.ssl :  true ,\n           ranger.usersync.unix.minUserId :  500 \n        }\n      }\n    },\n    {\n       admin-properties : {\n         properties_attributes : {},\n         properties : {\n           DB_FLAVOR :  POSTGRES ,\n           SQL_COMMAND_INVOKER :  psql ,\n           SQL_CONNECTOR_JAR :  /var/lib/ambari-agent/tmp/postgres-jdbc-driver.jar ,\n           audit_db_name :  ranger_audit ,\n           audit_db_user :  rangerlogger ,\n           db_host :  localhost:5432 ,\n           db_name :  ranger ,\n           db_root_user :  postgres ,\n           db_root_password :  admin ,\n           db_user :  rangeradmin ,\n           policymgr_external_url :  http://localhost:6080 ,\n           ranger_jdbc_connection_url :  jdbc:postgresql://{db_host}/ranger ,\n           ranger_jdbc_driver :  org.postgresql.Driver \n        }\n      }\n    },\n    {\n       ranger-admin-site : {\n         properties_attributes : {},\n         properties : {\n           ranger.audit.source.type :  db ,\n           ranger.authentication.method :  UNIX ,\n           ranger.credential.provider.path :  /etc/ranger/admin/rangeradmin.jceks ,\n           ranger.externalurl :  {{ranger_external_url}} ,\n           ranger.https.attrib.keystore.file :  /etc/ranger/admin/keys/server.jks ,\n           ranger.jpa.audit.jdbc.credential.alias :  rangeraudit ,\n           ranger.jpa.audit.jdbc.dialect :  {{jdbc_dialect}} ,\n           ranger.jpa.audit.jdbc.driver :  {{jdbc_driver}} ,\n           ranger.jpa.audit.jdbc.url :  {{audit_jdbc_url}} ,\n           ranger.jpa.audit.jdbc.user :  {{ranger_audit_db_user}} ,\n           ranger.jpa.jdbc.credential.alias :  rangeradmin ,\n           ranger.jpa.jdbc.dialect :  {{jdbc_dialect}} ,\n           ranger.jpa.jdbc.driver :  org.postgresql.Driver ,\n           ranger.jpa.jdbc.url :  jdbc:postgresql://localhost:5432/ranger ,\n           ranger.jpa.jdbc.user :  {{ranger_db_user}} ,\n           ranger.jpa.jdbc.password :  {{ranger_db_password}} ,\n           ranger.ldap.ad.domain :  localhost ,\n           ranger.ldap.ad.url :  ldap://ad.xasecure.net:389 ,\n           ranger.ldap.group.roleattribute :  cn ,\n           ranger.ldap.group.searchbase :  ou=groups,dc=xasecure,dc=net ,\n           ranger.ldap.group.searchfilter :  (member=uid={0},ou=users,dc=xasecure,dc=net) ,\n           ranger.ldap.url :  ldap://localhost:389 ,\n           ranger.ldap.user.dnpattern :  uid={0},ou=users,dc=xasecure,dc=net ,\n           ranger.service.host :  {{ranger_host}} ,\n           ranger.service.http.enabled :  true ,\n           ranger.service.http.port :  6080 ,\n           ranger.service.https.attrib.clientAuth :  false ,\n           ranger.service.https.attrib.keystore.keyalias :  mkey ,\n           ranger.service.https.attrib.keystore.pass :  ranger ,\n           ranger.service.https.attrib.ssl.enabled :  false ,\n           ranger.service.https.port :  6182 ,\n           ranger.unixauth.remote.login.enabled :  true ,\n           ranger.unixauth.service.hostname :  localhost ,\n           ranger.unixauth.service.port :  5151 \n        }\n      }\n    },\n    {\n       ranger-env : {\n         properties_attributes : {},\n         properties : {\n           admin_username :  admin ,\n           create_db_dbuser :  true ,\n           ranger_admin_log_dir :  /var/log/ranger/admin ,\n           ranger_admin_username :  amb_ranger_admin ,\n           ranger_admin_password :  amb_ranger_pw ,\n           ranger_group :  ranger ,\n           ranger_jdbc_connection_url :  {{ranger_jdbc_connection_url}} ,\n           ranger_jdbc_driver :  org.postgresql.Driver ,\n           ranger_pid_dir :  /var/run/ranger ,\n           ranger_user :  ranger ,\n           ranger_usersync_log_dir :  /var/log/ranger/usersync ,\n           xml_configurations_supported :  true \n        }\n      }\n    },\n    {\n       ranger-yarn-security : {\n         properties_attributes : {},\n         properties : {\n           ranger.plugin.yarn.policy.cache.dir :  /etc/ranger/{{repo_name}}/policycache ,\n           ranger.plugin.yarn.policy.pollIntervalMs :  30000 ,\n           ranger.plugin.yarn.policy.rest.ssl.config.file :  /etc/yarn/conf/ranger-policymgr-ssl.xml ,\n           ranger.plugin.yarn.policy.rest.url :  {{policymgr_mgr_url}} ,\n           ranger.plugin.yarn.policy.source.impl :  org.apache.ranger.admin.client.RangerAdminRESTClient ,\n           ranger.plugin.yarn.service.name :  {{repo_name}} \n        }\n      }\n    },\n    {\n       ranger-yarn-audit : {\n         properties_attributes : {},\n         properties : {\n           xasecure.audit.credential.provider.file :  jceks://file{{credential_file}} ,\n           xasecure.audit.db.async.max.flush.interval.ms :  30000 ,\n           xasecure.audit.db.async.max.queue.size :  10240 ,\n           xasecure.audit.db.batch.size :  100 ,\n           xasecure.audit.db.is.async :  true ,\n           xasecure.audit.destination.db :  true ,\n           xasecure.audit.hdfs.async.max.flush.interval.ms :  30000 ,\n           xasecure.audit.hdfs.async.max.queue.size :  1048576 ,\n           xasecure.audit.destination.hdfs.dir :  /ranger/audit/%app-type%/%time:yyyyMMdd% ,\n           xasecure.audit.hdfs.config.destination.file :  %hostname%-audit.log ,\n           xasecure.audit.hdfs.config.destination.flush.interval.seconds :  900 ,\n           xasecure.audit.hdfs.config.destination.open.retry.interval.seconds :  60 ,\n           xasecure.audit.hdfs.config.destination.rollover.interval.seconds :  86400 ,\n           xasecure.audit.hdfs.config.encoding :  ,\n           xasecure.audit.hdfs.config.local.archive.directory :  /var/log/yarn/audit/archive ,\n           xasecure.audit.hdfs.config.local.archive.max.file.count :  10 ,\n           xasecure.audit.hdfs.config.local.buffer.directory :  /var/log/yarn/audit ,\n           xasecure.audit.hdfs.config.local.buffer.file :  %time:yyyyMMdd-HHmm.ss%.log ,\n           xasecure.audit.hdfs.config.local.buffer.file.buffer.size.bytes :  8192 ,\n           xasecure.audit.hdfs.config.local.buffer.flush.interval.seconds :  60 ,\n           xasecure.audit.hdfs.config.local.buffer.rollover.interval.seconds :  600 ,\n           xasecure.audit.hdfs.is.async :  true ,\n           xasecure.audit.is.enabled :  true ,\n           xasecure.audit.jpa.javax.persistence.jdbc.driver :  {{jdbc_driver}} ,\n           xasecure.audit.jpa.javax.persistence.jdbc.url :  {{audit_jdbc_url}} ,\n           xasecure.audit.jpa.javax.persistence.jdbc.user :  {{xa_audit_db_user}} ,\n           xasecure.audit.kafka.async.max.flush.interval.ms :  1000 ,\n           xasecure.audit.kafka.async.max.queue.size :  1 ,\n           xasecure.audit.kafka.broker_list :  localhost:9092 ,\n           xasecure.audit.kafka.is.enabled :  false ,\n           xasecure.audit.kafka.topic_name :  ranger_audits ,\n           xasecure.audit.log4j.async.max.flush.interval.ms :  30000 ,\n           xasecure.audit.log4j.async.max.queue.size :  10240 ,\n           xasecure.audit.log4j.is.async :  false ,\n           xasecure.audit.log4j.is.enabled :  false \n        }\n      }\n    },\n    {\n       ranger-hdfs-security : {\n         properties_attributes : {},\n         properties : {\n           ranger.plugin.hdfs.policy.cache.dir :  /etc/ranger/{{repo_name}}/policycache ,\n           ranger.plugin.hdfs.policy.pollIntervalMs :  30000 ,\n           ranger.plugin.hdfs.policy.rest.ssl.config.file :  /etc/hadoop/conf/ranger-policymgr-ssl.xml ,\n           ranger.plugin.hdfs.policy.rest.url :  {{policymgr_mgr_url}} ,\n           ranger.plugin.hdfs.policy.source.impl :  org.apache.ranger.admin.client.RangerAdminRESTClient ,\n           ranger.plugin.hdfs.service.name :  {{repo_name}} ,\n           xasecure.add-hadoop-authorization :  true \n        }\n      }\n    },\n    {\n       ranger-yarn-plugin-properties : {\n         properties_attributes : {},\n         properties : {\n           REPOSITORY_CONFIG_USERNAME :  yarn ,\n           common.name.for.certificate :  - ,\n           hadoop.rpc.protection :  - ,\n           policy_user :  ambari-qa ,\n           ranger-yarn-plugin-enabled :  No \n        }\n      }\n    },\n    {\n       ranger-hdfs-audit : {\n         properties_attributes : {},\n         properties : {\n           xasecure.audit.credential.provider.file :  jceks://file{{credential_file}} ,\n           xasecure.audit.db.async.max.flush.interval.ms :  30000 ,\n           xasecure.audit.db.async.max.queue.size :  10240 ,\n           xasecure.audit.db.batch.size :  100 ,\n           xasecure.audit.db.is.async :  true ,\n           xasecure.audit.destination.db :  true ,\n           xasecure.audit.destination.hdfs.dir :  /ranger/audit/%app-type%/%time:yyyyMMdd% ,\n           xasecure.audit.hdfs.async.max.flush.interval.ms :  30000 ,\n           xasecure.audit.hdfs.async.max.queue.size :  1048576 ,\n           xasecure.audit.hdfs.config.destination.file :  %hostname%-audit.log ,\n           xasecure.audit.hdfs.config.destination.flush.interval.seconds :  900 ,\n           xasecure.audit.hdfs.config.destination.open.retry.interval.seconds :  60 ,\n           xasecure.audit.hdfs.config.destination.rollover.interval.seconds :  86400 ,\n           xasecure.audit.hdfs.config.encoding :  ,\n           xasecure.audit.hdfs.config.local.archive.directory :  /var/log/hadoop/audit/archive/%app-type% ,\n           xasecure.audit.hdfs.config.local.archive.max.file.count :  10 ,\n           xasecure.audit.hdfs.config.local.buffer.directory :  /var/log/hadoop/audit/%app-type% ,\n           xasecure.audit.hdfs.config.local.buffer.file :  %time:yyyyMMdd-HHmm.ss%.log ,\n           xasecure.audit.hdfs.config.local.buffer.file.buffer.size.bytes :  8192 ,\n           xasecure.audit.hdfs.config.local.buffer.flush.interval.seconds :  60 ,\n           xasecure.audit.hdfs.config.local.buffer.rollover.interval.seconds :  600 ,\n           xasecure.audit.hdfs.is.async :  true ,\n           xasecure.audit.is.enabled :  true ,\n           xasecure.audit.jpa.javax.persistence.jdbc.driver :  {{jdbc_driver}} ,\n           xasecure.audit.jpa.javax.persistence.jdbc.url :  {{audit_jdbc_url}} ,\n           xasecure.audit.jpa.javax.persistence.jdbc.user :  {{xa_audit_db_user}} ,\n           xasecure.audit.kafka.async.max.flush.interval.ms :  1000 ,\n           xasecure.audit.kafka.async.max.queue.size :  1 ,\n           xasecure.audit.kafka.broker_list :  localhost:9092 ,\n           xasecure.audit.kafka.is.enabled :  false ,\n           xasecure.audit.kafka.topic_name :  ranger_audits ,\n           xasecure.audit.log4j.async.max.flush.interval.ms :  30000 ,\n           xasecure.audit.log4j.async.max.queue.size :  10240 ,\n           xasecure.audit.log4j.is.async :  false ,\n           xasecure.audit.log4j.is.enabled :  false \n        }\n      }\n    },\n    {\n       ranger-hdfs-plugin-properties : {\n         properties_attributes : {},\n         properties : {\n           REPOSITORY_CONFIG_USERNAME :  hadoop ,\n           common.name.for.certificate :  - ,\n           hadoop.rpc.protection :  - ,\n           policy_user :  ambari-qa ,\n           ranger-hdfs-plugin-enabled :  No \n        }\n      }\n    },\n    {\n       usersync-properties : {\n         properties_attributes : {},\n         properties : {}\n      }\n    }\n  ],\n   host_groups : [\n    {\n       components : [\n        {\n           name :  NODEMANAGER \n        },\n        {\n           name :  YARN_CLIENT \n        },\n        {\n           name :  HDFS_CLIENT \n        },\n        {\n           name :  HISTORYSERVER \n        },\n        {\n           name :  METRICS_MONITOR \n        },\n        {\n           name :  NAMENODE \n        },\n        {\n           name :  ZOOKEEPER_CLIENT \n        },\n        {\n           name :  RANGER_ADMIN \n        },\n        {\n           name :  SECONDARY_NAMENODE \n        },\n        {\n           name :  MAPREDUCE2_CLIENT \n        },\n        {\n           name :  ZOOKEEPER_SERVER \n        },\n        {\n           name :  AMBARI_SERVER \n        },\n        {\n           name :  DATANODE \n        },\n        {\n           name :  RANGER_USERSYNC \n        },\n        {\n           name :  APP_TIMELINE_SERVER \n        },\n        {\n           name :  METRICS_COLLECTOR \n        },\n        {\n           name :  RESOURCEMANAGER \n        }\n      ],\n       configurations : [],\n       name :  host_group_1 ,\n       cardinality :  1 \n    }\n  ],\n   Blueprints : {\n     stack_name :  HDP ,\n     stack_version :  2.3 ,\n     blueprint_name :  ranger-psql-onenode-sample \n  }\n}  Notes   Ranger plugins cannot be enabled by default in a blueprint due to some Ambari restrictions, so properties like  ranger-hdfs-plugin-enabled  must be set to  No  and the plugins must be enabled from the Ambari UI with the checkboxes and by restarting the necessary services.  If using the UNIX user sync, it may be necessary in some cases to restart the Ranger Usersync Services after the blueprint installation finished if the UNIX users cannot be seen on the Ranger Admin UI.", 
            "title": "Sample recipe for Ranger"
        }, 
        {
            "location": "/blueprints/", 
            "text": "Blueprints\n\n\nWe provide a list of default Hadoop cluster Blueprints for your convenience, however you can always build and use your own Blueprint.\n\n\n\n\nhdp-small-default - HDP 2.3 blueprint\n\n\n\n\nThis is a complex \nBlueprint\n which allows you to launch a multi node, fully distributed HDP 2.3 Cluster in the cloud.\n\n\nIt allows you to use the following services: HDFS, YARN, MAPREDUCE2, KNOX, HBASE, HIVE, HCATALOG, WEBHCAT, SLIDER, OOZIE, PIG, SQOOP, METRICS, TEZ, FALCON, ZOOKEEPER.\n\n\n\n\nhdp-streaming-cluster - HDP 2.3 blueprint\n\n\n\n\nThis is a streaming \nBlueprint\n which allows you to launch a multi node, fully distributed HDP 2.3 Cluster in the cloud, optimized for streaming jobs.\n\n\nIt allows you to use the following services: HDFS, YARN, MAPREDUCE2, STORM, KNOX, HBASE, HIVE, HCATALOG, WEBHCAT, SLIDER, OOZIE, PIG, SQOOP, METRICS, TEZ, FALCON, ZOOKEEPER.\n\n\n\n\nhdp-spark-cluster - HDP 2.3 blueprint\n\n\n\n\nThis is an analytics \nBlueprint\n which allows you to launch a multi node, fully distributed HDP 2.3 Cluster in the cloud, optimized for analytic jobs.\n\n\nIt allows you to use the following services: HDFS, YARN, MAPREDUCE2, SPARK, ZEPPELIN, KNOX, HBASE, HIVE, HCATALOG, WEBHCAT, SLIDER, OOZIE, PIG, SQOOP, METRICS, TEZ, FALCON, ZOOKEEPER.\n\n\nComponents\n\n\nAmbari supports the concept of stacks and associated services in a stack definition. By leveraging the stack definition, Ambari has a consistent and defined interface to install, manage and monitor a set of services and provides extensibility model for new stacks and services to be introduced.\n\n\nAt high level the supported list of components can be grouped into main categories: Master and Slave - and bundling them together form a Hadoop Service.\n\n\n\n\n\n\n\n\nServices\n\n\nComponents\n\n\n\n\n\n\n\n\n\n\nHDFS\n\n\nDATANODE, HDFS_CLIENT, JOURNALNODE, NAMENODE, SECONDARY_NAMENODE, ZKFC\n\n\n\n\n\n\nYARN\n\n\nAPP_TIMELINE_SERVER, NODEMANAGER, RESOURCEMANAGER, YARN_CLIENT\n\n\n\n\n\n\nMAPREDUCE2\n\n\nHISTORYSERVER, MAPREDUCE2_CLIENT\n\n\n\n\n\n\nGANGLIA\n\n\nGANGLIA_MONITOR, GANGLIA_SERVER\n\n\n\n\n\n\nHBASE\n\n\nHBASE_CLIENT, HBASE_MASTER, HBASE_REGIONSERVER\n\n\n\n\n\n\nHIVE\n\n\nHIVE_CLIENT, HIVE_METASTORE, HIVE_SERVER, MYSQL_SERVER\n\n\n\n\n\n\nHCATALOG\n\n\nHCAT\n\n\n\n\n\n\nWEBHCAT\n\n\nWEBHCAT_SERVER\n\n\n\n\n\n\nOOZIE\n\n\nOOZIE_CLIENT, OOZIE_SERVER\n\n\n\n\n\n\nPIG\n\n\nPIG\n\n\n\n\n\n\nSQOOP\n\n\nSQOOP\n\n\n\n\n\n\nSTORM\n\n\nDRPC_SERVER, NIMBUS, STORM_REST_API, STORM_UI_SERVER, SUPERVISOR\n\n\n\n\n\n\nTEZ\n\n\nTEZ_CLIENT\n\n\n\n\n\n\nFALCON\n\n\nFALCON_CLIENT, FALCON_SERVER\n\n\n\n\n\n\nZOOKEEPER\n\n\nZOOKEEPER_CLIENT, ZOOKEEPER_SERVER\n\n\n\n\n\n\nSPARK\n\n\nSPARK_JOBHISTORYSERVER, SPARK_CLIENT\n\n\n\n\n\n\nRANGER\n\n\nRANGER_USERSYNC, RANGER_ADMIN\n\n\n\n\n\n\nAMBARI_METRICS\n\n\nAMBARI_METRICS, METRICS_COLLECTOR, METRICS_MONITOR\n\n\n\n\n\n\nKERBEROS\n\n\nKERBEROS_CLIENT\n\n\n\n\n\n\nFLUME\n\n\nFLUME_HANDLER\n\n\n\n\n\n\nKAFKA\n\n\nKAFKA_BROKER\n\n\n\n\n\n\nKNOX\n\n\nKNOX_GATEWAY\n\n\n\n\n\n\nNAGIOS\n\n\nNAGIOS_SERVER\n\n\n\n\n\n\nATLAS\n\n\nATLAS\n\n\n\n\n\n\nCLOUDBREAK\n\n\nCLOUDBREAK", 
            "title": "Blueprints"
        }, 
        {
            "location": "/blueprints/#blueprints", 
            "text": "We provide a list of default Hadoop cluster Blueprints for your convenience, however you can always build and use your own Blueprint.   hdp-small-default - HDP 2.3 blueprint   This is a complex  Blueprint  which allows you to launch a multi node, fully distributed HDP 2.3 Cluster in the cloud.  It allows you to use the following services: HDFS, YARN, MAPREDUCE2, KNOX, HBASE, HIVE, HCATALOG, WEBHCAT, SLIDER, OOZIE, PIG, SQOOP, METRICS, TEZ, FALCON, ZOOKEEPER.   hdp-streaming-cluster - HDP 2.3 blueprint   This is a streaming  Blueprint  which allows you to launch a multi node, fully distributed HDP 2.3 Cluster in the cloud, optimized for streaming jobs.  It allows you to use the following services: HDFS, YARN, MAPREDUCE2, STORM, KNOX, HBASE, HIVE, HCATALOG, WEBHCAT, SLIDER, OOZIE, PIG, SQOOP, METRICS, TEZ, FALCON, ZOOKEEPER.   hdp-spark-cluster - HDP 2.3 blueprint   This is an analytics  Blueprint  which allows you to launch a multi node, fully distributed HDP 2.3 Cluster in the cloud, optimized for analytic jobs.  It allows you to use the following services: HDFS, YARN, MAPREDUCE2, SPARK, ZEPPELIN, KNOX, HBASE, HIVE, HCATALOG, WEBHCAT, SLIDER, OOZIE, PIG, SQOOP, METRICS, TEZ, FALCON, ZOOKEEPER.", 
            "title": "Blueprints"
        }, 
        {
            "location": "/blueprints/#components", 
            "text": "Ambari supports the concept of stacks and associated services in a stack definition. By leveraging the stack definition, Ambari has a consistent and defined interface to install, manage and monitor a set of services and provides extensibility model for new stacks and services to be introduced.  At high level the supported list of components can be grouped into main categories: Master and Slave - and bundling them together form a Hadoop Service.     Services  Components      HDFS  DATANODE, HDFS_CLIENT, JOURNALNODE, NAMENODE, SECONDARY_NAMENODE, ZKFC    YARN  APP_TIMELINE_SERVER, NODEMANAGER, RESOURCEMANAGER, YARN_CLIENT    MAPREDUCE2  HISTORYSERVER, MAPREDUCE2_CLIENT    GANGLIA  GANGLIA_MONITOR, GANGLIA_SERVER    HBASE  HBASE_CLIENT, HBASE_MASTER, HBASE_REGIONSERVER    HIVE  HIVE_CLIENT, HIVE_METASTORE, HIVE_SERVER, MYSQL_SERVER    HCATALOG  HCAT    WEBHCAT  WEBHCAT_SERVER    OOZIE  OOZIE_CLIENT, OOZIE_SERVER    PIG  PIG    SQOOP  SQOOP    STORM  DRPC_SERVER, NIMBUS, STORM_REST_API, STORM_UI_SERVER, SUPERVISOR    TEZ  TEZ_CLIENT    FALCON  FALCON_CLIENT, FALCON_SERVER    ZOOKEEPER  ZOOKEEPER_CLIENT, ZOOKEEPER_SERVER    SPARK  SPARK_JOBHISTORYSERVER, SPARK_CLIENT    RANGER  RANGER_USERSYNC, RANGER_ADMIN    AMBARI_METRICS  AMBARI_METRICS, METRICS_COLLECTOR, METRICS_MONITOR    KERBEROS  KERBEROS_CLIENT    FLUME  FLUME_HANDLER    KAFKA  KAFKA_BROKER    KNOX  KNOX_GATEWAY    NAGIOS  NAGIOS_SERVER    ATLAS  ATLAS    CLOUDBREAK  CLOUDBREAK", 
            "title": "Components"
        }, 
        {
            "location": "/kerberos/", 
            "text": "Kerberos Security\n\n\nCloudbreak can enable Kerberos security on the cluster. When enabled, Cloudbreak will install an MIT KDC into the cluster\nand enable Kerberos on the cluster.\n\n\n\n\nThis feature is currently \nTECHNICAL PREVIEW\n.\n\n\n\n\nEnable Kerberos\n\n\nTo enable Kerberos in a cluster, when creating your cluster via the UI, do the following:\n\n\n\n\nWhen in the \nCreate cluster\n wizard, on the \nSetup Network and Security\n tab, check the \nEnable security\n option.\n\n\nFill in the following fields:\n\n\n\n\n\n\n\n\n\n\nField\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nKerberos master key\n\n\nThe master key to use for the KDC.\n\n\n\n\n\n\nKerberos admin\n\n\nThe KDC admin username to use for the KDC.\n\n\n\n\n\n\nKerberos password\n\n\nThe KDC admin password to use for the KDC.\n\n\n\n\n\n\n\n\n\n\nThe Cloudbreak Kerberos setup does not contain Active Directory support or any other third party user authentication method. If you\nwant to use custom Hadoop user, you have to create users manually with the same name on all Ambari containers on each node.\n\n\n\n\nTesting Kerberos\n\n\nTo run a job on the cluster, you can use one of the default Hadoop users, like \nambari-qa\n, as usual.\n\n\nOnce kerberos is enabled you need a \nticket\n to execute any job on the cluster. Here's an example to get a ticket:\n\n\nkinit -V -kt /etc/security/keytabs/smokeuser.headless.keytab ambari-qa-sparktest-rec@NODE.DC1.CONSUL\n\n\n\n\nExample job:\n\n\nexport HADOOP_LIBS=/usr/hdp/current/hadoop-mapreduce-client\nexport JAR_EXAMPLES=$HADOOP_LIBS/hadoop-mapreduce-examples.jar\nexport JAR_JOBCLIENT=$HADOOP_LIBS/hadoop-mapreduce-client-jobclient.jar\n\nhadoop jar $JAR_EXAMPLES teragen 10000000 /user/ambari-qa/terasort-input\n\nhadoop jar $JAR_JOBCLIENT mrbench -baseDir /user/ambari-qa/smallJobsBenchmark -numRuns 5 -maps 10 -reduces 5 -inputLines 10 -inputType ascending\n\n\n\n\nCreate Hadoop Users\n\n\nTo create Hadoop users please follow the steps below.\n\n\n\n\nLog in via SSH to the Cloudbreak gateway node (IP address is the same as the Ambari UI)\n\n\n\n\nsudo docker exec -i kerberos bash\nkadmin -p [admin_user]/[admin_user]@NODE.DC1.CONSUL (type admin password)\naddprinc custom-user (type user password twice)\n\n\n\n\n\n\nLog in via SSH to all other nodes\n\n\n\n\nsudo docker exec -i $(docker ps | grep ambari-warmup | cut -d\n \n -f 1) bash\nuseradd custom-user\n\n\n\n\n\n\nLog in via SSH to one of the nodes\n\n\n\n\n```\nsudo docker exec -i $(docker ps | grep ambari-warmup | cut -d\" \" -f 1) bash\nsu custom-user\nkinit -p custom-user (type user password)\nhdfs dfs -mkdir input\nhdfs dfs -put /tmp/wait-for-host-number.sh input\nyarn jar $(find /usr/hdp -name hadoop-mapreduce-examples.jar) wordcount input output\nhdfs dfs -cat output/*", 
            "title": "Kerberos"
        }, 
        {
            "location": "/kerberos/#kerberos-security", 
            "text": "Cloudbreak can enable Kerberos security on the cluster. When enabled, Cloudbreak will install an MIT KDC into the cluster\nand enable Kerberos on the cluster.   This feature is currently  TECHNICAL PREVIEW .", 
            "title": "Kerberos Security"
        }, 
        {
            "location": "/kerberos/#enable-kerberos", 
            "text": "To enable Kerberos in a cluster, when creating your cluster via the UI, do the following:   When in the  Create cluster  wizard, on the  Setup Network and Security  tab, check the  Enable security  option.  Fill in the following fields:      Field  Description      Kerberos master key  The master key to use for the KDC.    Kerberos admin  The KDC admin username to use for the KDC.    Kerberos password  The KDC admin password to use for the KDC.      The Cloudbreak Kerberos setup does not contain Active Directory support or any other third party user authentication method. If you\nwant to use custom Hadoop user, you have to create users manually with the same name on all Ambari containers on each node.   Testing Kerberos  To run a job on the cluster, you can use one of the default Hadoop users, like  ambari-qa , as usual.  Once kerberos is enabled you need a  ticket  to execute any job on the cluster. Here's an example to get a ticket:  kinit -V -kt /etc/security/keytabs/smokeuser.headless.keytab ambari-qa-sparktest-rec@NODE.DC1.CONSUL  Example job:  export HADOOP_LIBS=/usr/hdp/current/hadoop-mapreduce-client\nexport JAR_EXAMPLES=$HADOOP_LIBS/hadoop-mapreduce-examples.jar\nexport JAR_JOBCLIENT=$HADOOP_LIBS/hadoop-mapreduce-client-jobclient.jar\n\nhadoop jar $JAR_EXAMPLES teragen 10000000 /user/ambari-qa/terasort-input\n\nhadoop jar $JAR_JOBCLIENT mrbench -baseDir /user/ambari-qa/smallJobsBenchmark -numRuns 5 -maps 10 -reduces 5 -inputLines 10 -inputType ascending  Create Hadoop Users  To create Hadoop users please follow the steps below.   Log in via SSH to the Cloudbreak gateway node (IP address is the same as the Ambari UI)   sudo docker exec -i kerberos bash\nkadmin -p [admin_user]/[admin_user]@NODE.DC1.CONSUL (type admin password)\naddprinc custom-user (type user password twice)   Log in via SSH to all other nodes   sudo docker exec -i $(docker ps | grep ambari-warmup | cut -d    -f 1) bash\nuseradd custom-user   Log in via SSH to one of the nodes   ```\nsudo docker exec -i $(docker ps | grep ambari-warmup | cut -d\" \" -f 1) bash\nsu custom-user\nkinit -p custom-user (type user password)\nhdfs dfs -mkdir input\nhdfs dfs -put /tmp/wait-for-host-number.sh input\nyarn jar $(find /usr/hdp -name hadoop-mapreduce-examples.jar) wordcount input output\nhdfs dfs -cat output/*", 
            "title": "Enable Kerberos"
        }, 
        {
            "location": "/spi/", 
            "text": "Service Provider Interface (SPI)\n\n\nCloudbreak already supports multiple cloud platforms and provides an easy way to integrate a new provider trough \nCloudbreak's Service Provider Interface (SPI)\n which is a plugin mechanism to enable a seamless integration of any cloud provider.\n\n\nThe SPI plugin mechanism has been used to integrate all existing providers to Cloudbreak, therefore if a new provider is integrated it immediately becomes a first class citizen in Cloudbreak.\n\n\n\n\ncloud-aws\n module integrates Amazon Web Services\n\n\ncloud-gcp\n module integrates Google Cloud Platform\n\n\ncloud-arm\n module integrates Microsoft Azure\n\n\ncloud-openstack\n module integrates OpenStack\n\n\n\n\nThe SPI interface is event based, scales well and decoupled from Cloudbreak. The core of Cloudbreak is communicating trough \nEventBus\n with providers, but the complexity of Event handling is hidden from the provider implementation.\n\n\n\n\nUse of the SPI is currently \nTECHNICAL PREVIEW\n.\n\n\n\n\nResource management\n\n\nThere are two kind of deployment/resource management method is supported by cloud providers:\n\n\n\n\ntemplate based deployments\n\n\nindividual resource based deployments\n\n\n\n\nCloudbreak's SPI supports both way of resource management. It provides a well defined interfaces, abstract classes and helper classes like scheduling and polling of resources to aid the integration and to avoid any boilerplate code in the module of cloud provider.\n\n\nTemplate based deployments\n\n\nProviders with template based deployments like \nAWS CloudFormation\n, \nAzure ARM\n or \nOpenStack Heat\n have the ability to create and manage a collection of related cloud resources, provisioning and updating them in an orderly and predictable fashion. This means that Cloudbreak needs a reference to the template itself and every change in the infrastructure (e.g creating new instance or deleting one) is managed through this templating mechanism.\n\n\nIf the provider has templating support then the provider's \ngradle\n module shall depend on the \ncloud-api\n module.\n\n\napply plugin: 'java'\n\nsourceCompatibility = 1.7\n\nrepositories {\n    mavenCentral()\n}\n\njar {\n    baseName = 'cloud-new-provider'\n}\n\ndependencies {\n\n    compile project(':cloud-api')\n\n}\n\n\n\n\nThe entry point of the provider is the  \nCloudConnector\n interface and every interface that needs to be implemented is reachable trough this interface.\n\n\nIndividual resource based deployments\n\n\nProviders like GCP that does not support suitable templating mechanism or for customisable providers like OpenStack where the Heat Orchestration (templating) component optional the individual resources needs to be handlet separately. This means that resources like networks, discs and compute instances needs to be created and managed with an ordered sequence of API calls and Cloudbreak shall provide a solution to manage the collection of related cloud resources together.\n\n\nIf the provider has no templating support then the provider's \ngradle\n module shall depend on the \ncloud-template\n module, that includes Cloudbreak defined abstract template. This template is a set of abstract and utility classes to support provisioning and updating related resources in an orderly and predictable manner trough ordered sequences of cloud API calls.\n\n\napply plugin: 'java'\n\nsourceCompatibility = 1.7\n\nrepositories {\n    mavenCentral()\n}\n\njar {\n    baseName = 'cloud-new-provider'\n}\n\ndependencies {\n\n    compile project(':cloud-template')\n\n}\n\n\n\n\nVariants\n\n\nOpenStack is very modular and allows to install different components for e.g. volume storage or different components for networking (e.g. Nova networking or Neutron) or even you have a chance that some components like Heat are not installed at all.\n\n\nCloudbreak's SPI interface reflects this flexibility using so called variants. This means that if some part of cloud provider (typically OpenStack) is using different component you don't need re-implement the complete stack but just use a different variant and re-implement the part what is different.\n\n\nThe reference implementation for this feature can be found in  \ncloud-openstack\n module which support a HEAT and NATIVE variants. The HEAT variant utilizes the Heat templating to launch a stack, but the NATIVE variant starts the cluster by using a sequence of API calls without Heat to achieve the same result, although both of them are using the same authentication and credential management.\n\n\nDevelopment\n\n\nIn order to set up a development environment please take a look at \nLocal Development Setup\n documentation.", 
            "title": "SPI"
        }, 
        {
            "location": "/spi/#service-provider-interface-spi", 
            "text": "Cloudbreak already supports multiple cloud platforms and provides an easy way to integrate a new provider trough  Cloudbreak's Service Provider Interface (SPI)  which is a plugin mechanism to enable a seamless integration of any cloud provider.  The SPI plugin mechanism has been used to integrate all existing providers to Cloudbreak, therefore if a new provider is integrated it immediately becomes a first class citizen in Cloudbreak.   cloud-aws  module integrates Amazon Web Services  cloud-gcp  module integrates Google Cloud Platform  cloud-arm  module integrates Microsoft Azure  cloud-openstack  module integrates OpenStack   The SPI interface is event based, scales well and decoupled from Cloudbreak. The core of Cloudbreak is communicating trough  EventBus  with providers, but the complexity of Event handling is hidden from the provider implementation.   Use of the SPI is currently  TECHNICAL PREVIEW .", 
            "title": "Service Provider Interface (SPI)"
        }, 
        {
            "location": "/spi/#resource-management", 
            "text": "There are two kind of deployment/resource management method is supported by cloud providers:   template based deployments  individual resource based deployments   Cloudbreak's SPI supports both way of resource management. It provides a well defined interfaces, abstract classes and helper classes like scheduling and polling of resources to aid the integration and to avoid any boilerplate code in the module of cloud provider.", 
            "title": "Resource management"
        }, 
        {
            "location": "/spi/#template-based-deployments", 
            "text": "Providers with template based deployments like  AWS CloudFormation ,  Azure ARM  or  OpenStack Heat  have the ability to create and manage a collection of related cloud resources, provisioning and updating them in an orderly and predictable fashion. This means that Cloudbreak needs a reference to the template itself and every change in the infrastructure (e.g creating new instance or deleting one) is managed through this templating mechanism.  If the provider has templating support then the provider's  gradle  module shall depend on the  cloud-api  module.  apply plugin: 'java'\n\nsourceCompatibility = 1.7\n\nrepositories {\n    mavenCentral()\n}\n\njar {\n    baseName = 'cloud-new-provider'\n}\n\ndependencies {\n\n    compile project(':cloud-api')\n\n}  The entry point of the provider is the   CloudConnector  interface and every interface that needs to be implemented is reachable trough this interface.", 
            "title": "Template based deployments"
        }, 
        {
            "location": "/spi/#individual-resource-based-deployments", 
            "text": "Providers like GCP that does not support suitable templating mechanism or for customisable providers like OpenStack where the Heat Orchestration (templating) component optional the individual resources needs to be handlet separately. This means that resources like networks, discs and compute instances needs to be created and managed with an ordered sequence of API calls and Cloudbreak shall provide a solution to manage the collection of related cloud resources together.  If the provider has no templating support then the provider's  gradle  module shall depend on the  cloud-template  module, that includes Cloudbreak defined abstract template. This template is a set of abstract and utility classes to support provisioning and updating related resources in an orderly and predictable manner trough ordered sequences of cloud API calls.  apply plugin: 'java'\n\nsourceCompatibility = 1.7\n\nrepositories {\n    mavenCentral()\n}\n\njar {\n    baseName = 'cloud-new-provider'\n}\n\ndependencies {\n\n    compile project(':cloud-template')\n\n}", 
            "title": "Individual resource based deployments"
        }, 
        {
            "location": "/spi/#variants", 
            "text": "OpenStack is very modular and allows to install different components for e.g. volume storage or different components for networking (e.g. Nova networking or Neutron) or even you have a chance that some components like Heat are not installed at all.  Cloudbreak's SPI interface reflects this flexibility using so called variants. This means that if some part of cloud provider (typically OpenStack) is using different component you don't need re-implement the complete stack but just use a different variant and re-implement the part what is different.  The reference implementation for this feature can be found in   cloud-openstack  module which support a HEAT and NATIVE variants. The HEAT variant utilizes the Heat templating to launch a stack, but the NATIVE variant starts the cluster by using a sequence of API calls without Heat to achieve the same result, although both of them are using the same authentication and credential management.", 
            "title": "Variants"
        }, 
        {
            "location": "/spi/#development", 
            "text": "In order to set up a development environment please take a look at  Local Development Setup  documentation.", 
            "title": "Development"
        }, 
        {
            "location": "/operations/", 
            "text": "Operations\n\n\nCloudbreak deployer\n\n\nDebug\n\n\nIf you want to have more detailed output set the \nDEBUG\n env variable to non-zero:\n\n\nDEBUG=1 cbd some_command\n\n\n\n\nTroubleshoot\n\n\nYou can use the \ndoctor\n command to diagnose your environment.\nIt can reveal some common problems with your docker or boot2docker configuration and it also checks the cbd versions.\n\n\ncbd doctor\n\n\n\n\nLogs\n\n\nThe aggregated logs of all the Cloudbreak components can be checked with:\n\n\ncbd logs\n\n\n\n\nIt can also be used to check the logs of an individual docker container. To see only the logs of the Cloudbreak backend:\n\n\ncbd logs cloudbreak\n\n\n\n\nYou can also check the individual logs of \nuluwatu\n, \nperiscope\n, and \nidentity\n.\n\n\nUpdate\n\n\nThe cloudbreak-deployer tool is capable of upgrading itself to a newer version.\n\n\ncbd update\n\n\n\n\nCloudbreak application\n\n\nSSH to the hosts\n\n\nIn the current version of Cloudbreak all the nodes have a public IP address and all the nodes are accessible via SSH.\nThe public IP addresses of a running cluster can be checked on the Cloudbreak UI under the \nNodes\n tab.\nOnly key-based authentication is supported - the public key can be specified when creating a cloud credential.\n\n\nssh -i ~/.ssh/private-key.pem cloudbreak@\npublic-ip\n\n\n\n\n\nThe default user is \ncloudbreak\n except on EC2 where it is \nec2-user\n.\n\n\nAccessing HDP client services\n\n\nThe main difference between general HDP clusters and Cloudbreak-installed HDP clusters is that each host runs an Ambari server or agent Docker container and the HDP services will be installed in this container as well.\nIt means that after \nssh\n the client services won't be available instantly, first you'll have to \nenter\n the ambari-agent container.\nInside the container everything works the same way as expected. In order to do so there are a few options.\n\n\nHelper functions\n\n\nWe have created a few helper functions in order to help operations. Once yoy SSH into the host you should a message as such:\n\n\nYou are now logged in to the docker host ...\nGetting started with docker:\n\n  docker ps                : Listing running containers\n  docker logs -f \nID\n      : Star tail -f on container stdout/stderr\n  docker exec -it \nID\n sh  : Entering the container (instead of ssh)\n\nHelper commands:\n\n  h                        : prints this message\n  ambari-enter             : Enters the ambari container\n  ambari-log               : Watches the ambari logs\n\n\n\n\nYou can enter into the container where the HDP services are running using the \nambari-enter\n functions.\n\n\nDocker exec\n\n\nTo check the containers running on the host enter:\n\n\n[cloudbreak@vmhostgroupclient11 ~]$ sudo docker ps\nCONTAINER ID   IMAGE                                    COMMAND                CREATED      STATUS      PORTS     NAMES\n1098ca778176   sequenceiq/baywatch-client:v1.0.0        \n/etc/bootstrap.sh -d\n 4 hours ago  Up 4 hours            baywatch-client-14454170059514\nf4097c52fda5   sequenceiq/logrotate:v0.5.1              \n/start.sh\n            4 hours ago  Up 4 hours            logrotate-14454169954830\n7b94aedaab30   sequenceiq/docker-consul-watch-plugn:1.0 \n/start.sh consul://1\n 4 hours ago  Up 4 hours            consul-watch-14454169884044\nd8128b001427   sequenceiq/ambari:2.1.2-v2               \n/start-agent\n         4 hours ago  Up 4 hours            ambari-agent-14454169805924\na8ec90037aaf   swarm:0.4.0                              \n/swarm join --addr=1\n 4 hours ago  Up 4 hours  2375/tcp  vmhostgroupmaster12-swarm-agent\nef02b43eacee   sequenceiq/consul:v0.5.0-v5              \n/bin/start\n           4 hours ago  Up 4 hours            vmhostgroupmaster12-consul\n\n\n\n\nYou should see the \nambari-agent\n container running. Copy its id or name and \nexec\n into the container:\n\n\n[cloudbreak@vmhostgroupclient11 ~]$ sudo docker exec -it ambari-agent-14454169805924 bash\n[root@docker-ambari tmp]#\n\n\n\n\nOr you can use this one-step command as well:\n\n\n[cloudbreak@vmhostgroupclient11 ~]$ sudo docker exec -it $(sudo docker ps -f=name=ambari-agent -q) bash\n[root@docker-ambari tmp]#\n\n\n\n\nData volumes\n\n\nThe disks that are attached to the instances are automatically mounted to \n/hadoopfs/fs1\n, \n/hadoopfs/fs2\n, ... \n/hadoopfs/fsN\n respectively.\nThese directories are mounted from the host into the ambari-agent container under the same name so these can be accessed from inside.\nIt means that if you'd like to move some data to the instances you can use these volumes and the data will be available from the container instantly to work on it.\n\n\nAn \nscp\n Example:\n\n\n$ scp -qr -i ~/.ssh/private-key.pem ~/tmp/data cloudbreak@\nclient-node\n:/hadoopfs/fs1\n$ ssh -i ~/.ssh/private-key.pem cloudbreak@\nclient-node\n\n[cloudbreak@vmhostgroupclient11 ~]$ sudo docker exec -it $(sudo docker ps -f=name=ambari-agent -q) bash\n[root@docker-ambari tmp]# su hdfs\n[hdfs@docker-ambari tmp]# hadoop fs -put /hadoopfs/fs1/data /tmp\n[hdfs@docker-ambari tmp]# hadoop fs -ls /tmp\nFound 2 items\ndrwxr-xr-x   - hdfs supergroup          0 2015-10-21 13:46 /tmp/data\ndrwx-wx-wx   - hive supergroup          0 2015-10-21 08:51 /tmp/hive\n\n\n\n\nInternal hostnames\n\n\nAfter a cluster is created with Cloudbreak, the nodes will have internal hostnames like this:\n\n\nvmhostgroupclient11.node.dc1.consul\n\n\nThis is because Cloudbreak uses \nConsul\n to provide DNS services.\nIt means that you won't see entries to the other nodes inside the \n/etc/hosts\n file, because nodes are registered inside Consul and the hostnames are resolved by Consul as well.\n\n\nIn the current version the \nnode.dc1.consul\n domain is hardcoded and cannot be changed.\n\n\nAccessing Ambari server from the other nodes\n\n\nAmbari server is registered as a service in Consul, so it can always be accessed through its domain name \nambari-8080.service.consul\n from the other ambari containers.\nIt can be tried by pinging it from one of the \nambari-agent\n containers:\n\n\nping ambari-8080.service.consul\n\n\n\n\nCloudbreak gateway node\n\n\nWith every Cloudbreak cluster installation there is a special node called \ncbgateway\n started that won't run an ambari-agent container so it won't run HDP services either.\nIt can be seen on the Cloudbreak UI among the hostgroups when creating a cluster, but its node count cannot be changed from 1 and it shouldn't be there in the Ambari blueprint.\nIt is by design because this instance has some special tasks:\n\n\n\n\nit runs the Ambari server and its database inside Docker containers\n\n\nit runs an nginx proxy that is used by the Cloudbreak API to communicate with the cluster securely\n\n\nit runs the Swarm manager that orchestrates the Docker containers on the whole cluster\n\n\nit runs the Baywatch server that is responsible for collecting the operational logs from the cluster\n\n\nit runs a Kerberos KDC container if Kerberos is configured\n\n\n\n\nLogs\n\n\nHadoop logs\n\n\nHadoop logs are available from the host and from the container as well in the \n/hadoopfs/fs1/logs\n directory.\n\n\nAmbari logs\n\n\nFor Ambari logs you can use our helper function, \nambari-log\n. For further information please check the \nHelper functions\n section. Alternatively you watch the Ambari logs on the host instance as well under the \n`/hadoopfs/fs1/logs\n folder as well.\n\n\nAmbari database\n\n\nAmbari's database runs on the \ncbgateway\n node inside a PostgreSQL docker container. To access it SSH to the \ngateway\n node and run the following command:\n\n\n[cloudbreak@vmcbgateway0 ~]$ sudo docker exec -it ambari_db psql -U postgres", 
            "title": "Operations"
        }, 
        {
            "location": "/operations/#operations", 
            "text": "", 
            "title": "Operations"
        }, 
        {
            "location": "/operations/#cloudbreak-deployer", 
            "text": "Debug  If you want to have more detailed output set the  DEBUG  env variable to non-zero:  DEBUG=1 cbd some_command  Troubleshoot  You can use the  doctor  command to diagnose your environment.\nIt can reveal some common problems with your docker or boot2docker configuration and it also checks the cbd versions.  cbd doctor  Logs  The aggregated logs of all the Cloudbreak components can be checked with:  cbd logs  It can also be used to check the logs of an individual docker container. To see only the logs of the Cloudbreak backend:  cbd logs cloudbreak  You can also check the individual logs of  uluwatu ,  periscope , and  identity .  Update  The cloudbreak-deployer tool is capable of upgrading itself to a newer version.  cbd update", 
            "title": "Cloudbreak deployer"
        }, 
        {
            "location": "/operations/#cloudbreak-application", 
            "text": "SSH to the hosts  In the current version of Cloudbreak all the nodes have a public IP address and all the nodes are accessible via SSH.\nThe public IP addresses of a running cluster can be checked on the Cloudbreak UI under the  Nodes  tab.\nOnly key-based authentication is supported - the public key can be specified when creating a cloud credential.  ssh -i ~/.ssh/private-key.pem cloudbreak@ public-ip   The default user is  cloudbreak  except on EC2 where it is  ec2-user .  Accessing HDP client services  The main difference between general HDP clusters and Cloudbreak-installed HDP clusters is that each host runs an Ambari server or agent Docker container and the HDP services will be installed in this container as well.\nIt means that after  ssh  the client services won't be available instantly, first you'll have to  enter  the ambari-agent container.\nInside the container everything works the same way as expected. In order to do so there are a few options.  Helper functions  We have created a few helper functions in order to help operations. Once yoy SSH into the host you should a message as such:  You are now logged in to the docker host ...\nGetting started with docker:\n\n  docker ps                : Listing running containers\n  docker logs -f  ID       : Star tail -f on container stdout/stderr\n  docker exec -it  ID  sh  : Entering the container (instead of ssh)\n\nHelper commands:\n\n  h                        : prints this message\n  ambari-enter             : Enters the ambari container\n  ambari-log               : Watches the ambari logs  You can enter into the container where the HDP services are running using the  ambari-enter  functions.  Docker exec  To check the containers running on the host enter:  [cloudbreak@vmhostgroupclient11 ~]$ sudo docker ps\nCONTAINER ID   IMAGE                                    COMMAND                CREATED      STATUS      PORTS     NAMES\n1098ca778176   sequenceiq/baywatch-client:v1.0.0         /etc/bootstrap.sh -d  4 hours ago  Up 4 hours            baywatch-client-14454170059514\nf4097c52fda5   sequenceiq/logrotate:v0.5.1               /start.sh             4 hours ago  Up 4 hours            logrotate-14454169954830\n7b94aedaab30   sequenceiq/docker-consul-watch-plugn:1.0  /start.sh consul://1  4 hours ago  Up 4 hours            consul-watch-14454169884044\nd8128b001427   sequenceiq/ambari:2.1.2-v2                /start-agent          4 hours ago  Up 4 hours            ambari-agent-14454169805924\na8ec90037aaf   swarm:0.4.0                               /swarm join --addr=1  4 hours ago  Up 4 hours  2375/tcp  vmhostgroupmaster12-swarm-agent\nef02b43eacee   sequenceiq/consul:v0.5.0-v5               /bin/start            4 hours ago  Up 4 hours            vmhostgroupmaster12-consul  You should see the  ambari-agent  container running. Copy its id or name and  exec  into the container:  [cloudbreak@vmhostgroupclient11 ~]$ sudo docker exec -it ambari-agent-14454169805924 bash\n[root@docker-ambari tmp]#  Or you can use this one-step command as well:  [cloudbreak@vmhostgroupclient11 ~]$ sudo docker exec -it $(sudo docker ps -f=name=ambari-agent -q) bash\n[root@docker-ambari tmp]#  Data volumes  The disks that are attached to the instances are automatically mounted to  /hadoopfs/fs1 ,  /hadoopfs/fs2 , ...  /hadoopfs/fsN  respectively.\nThese directories are mounted from the host into the ambari-agent container under the same name so these can be accessed from inside.\nIt means that if you'd like to move some data to the instances you can use these volumes and the data will be available from the container instantly to work on it.  An  scp  Example:  $ scp -qr -i ~/.ssh/private-key.pem ~/tmp/data cloudbreak@ client-node :/hadoopfs/fs1\n$ ssh -i ~/.ssh/private-key.pem cloudbreak@ client-node \n[cloudbreak@vmhostgroupclient11 ~]$ sudo docker exec -it $(sudo docker ps -f=name=ambari-agent -q) bash\n[root@docker-ambari tmp]# su hdfs\n[hdfs@docker-ambari tmp]# hadoop fs -put /hadoopfs/fs1/data /tmp\n[hdfs@docker-ambari tmp]# hadoop fs -ls /tmp\nFound 2 items\ndrwxr-xr-x   - hdfs supergroup          0 2015-10-21 13:46 /tmp/data\ndrwx-wx-wx   - hive supergroup          0 2015-10-21 08:51 /tmp/hive  Internal hostnames  After a cluster is created with Cloudbreak, the nodes will have internal hostnames like this:  vmhostgroupclient11.node.dc1.consul  This is because Cloudbreak uses  Consul  to provide DNS services.\nIt means that you won't see entries to the other nodes inside the  /etc/hosts  file, because nodes are registered inside Consul and the hostnames are resolved by Consul as well.  In the current version the  node.dc1.consul  domain is hardcoded and cannot be changed.  Accessing Ambari server from the other nodes  Ambari server is registered as a service in Consul, so it can always be accessed through its domain name  ambari-8080.service.consul  from the other ambari containers.\nIt can be tried by pinging it from one of the  ambari-agent  containers:  ping ambari-8080.service.consul  Cloudbreak gateway node  With every Cloudbreak cluster installation there is a special node called  cbgateway  started that won't run an ambari-agent container so it won't run HDP services either.\nIt can be seen on the Cloudbreak UI among the hostgroups when creating a cluster, but its node count cannot be changed from 1 and it shouldn't be there in the Ambari blueprint.\nIt is by design because this instance has some special tasks:   it runs the Ambari server and its database inside Docker containers  it runs an nginx proxy that is used by the Cloudbreak API to communicate with the cluster securely  it runs the Swarm manager that orchestrates the Docker containers on the whole cluster  it runs the Baywatch server that is responsible for collecting the operational logs from the cluster  it runs a Kerberos KDC container if Kerberos is configured   Logs  Hadoop logs  Hadoop logs are available from the host and from the container as well in the  /hadoopfs/fs1/logs  directory.  Ambari logs  For Ambari logs you can use our helper function,  ambari-log . For further information please check the  Helper functions  section. Alternatively you watch the Ambari logs on the host instance as well under the  `/hadoopfs/fs1/logs  folder as well.  Ambari database  Ambari's database runs on the  cbgateway  node inside a PostgreSQL docker container. To access it SSH to the  gateway  node and run the following command:  [cloudbreak@vmcbgateway0 ~]$ sudo docker exec -it ambari_db psql -U postgres", 
            "title": "Cloudbreak application"
        }, 
        {
            "location": "/database/", 
            "text": "Migrate the databases\n\n\nCreate the database schema or migrate it to the latest version:\n\n\ncbd startdb\ncbd migrate cbdb up\n\n\n\n\nVerify that all scripts have been applied:\n\n\ncbd migrate cbdb status\n\n\n\n\ncbd generate\ncbd migrate cbdb up", 
            "title": "Migration"
        }, 
        {
            "location": "/database/#migrate-the-databases", 
            "text": "Create the database schema or migrate it to the latest version:  cbd startdb\ncbd migrate cbdb up  Verify that all scripts have been applied:  cbd migrate cbdb status  cbd generate\ncbd migrate cbdb up", 
            "title": "Migrate the databases"
        }, 
        {
            "location": "/configuration/", 
            "text": "Configuration\n\n\nConfiguration is based on environment variables. Cloudbreak Deployer always forks a new bash subprocess \nwithout\ninheriting environment variables\n. The only way to set ENV vars relevant for Cloudbreak Deployer is to set them\nin a file called \nProfile\n.\n\n\nTo see all available config variables with their default value:\n\n\ncbd env show\n\n\n\n\nThe \nProfile\n will be simple \nsourced\n in bash terms, so you can use the usual syntaxes to set config values:\n\n\nexport MY_VAR=some_value\nexport OTHER_VAR=dunno\n\n\n\n\nEnv specific Profile\n\n\nLet\u2019s say you want to use a different version of Cloudbreak for \nprod\n and \nqa\n profile.\nYou can specify the Docker image tag via: \nDOCKER_TAG_CLOUDBREAK\n.\n\nProfile\n is always sourced, so you will have two env specific configurations:\n- \nProfile.dev\n\n- \nProfile.qa\n\n\nFor prod you need:\n\n\n\n\ncreate a file called \nProfile.prod\n\n\nwrite the env specific \nexport DOCKER_TAG_CLOUDBREAK=0.3.99\n into \nProfile.prod\n\n\nset the env variable: \nCBD_DEFAULT_PROFILE=prod\n\n\n\n\nTo use the \nprod\n specific profile once:\n\n\nCBD_DEFAULT_PROFILE=prod cbd some_commands\n\n\n\n\nFor permanent setting you can \nexport CBD_DEFAULT_PROFILE=prod\n in your \n.bash_profile\n.\n\n\nAvailable Configurations\n\n\nSMTP\n\n\nIf you want to change SMTP parameters, put the corresponding lines into your \nProfile\n. You can also see the default values of the parameters in the following box.\n\n\nexport CLOUDBREAK_SMTP_SENDER_USERNAME=\nexport CLOUDBREAK_SMTP_SENDER_PASSWORD=\nexport CLOUDBREAK_SMTP_SENDER_HOST=\nexport CLOUDBREAK_SMTP_SENDER_PORT=25\nexport CLOUDBREAK_SMTP_SENDER_FROM=\nexport CLOUDBREAK_SMTP_AUTH=true\nexport CLOUDBREAK_SMTP_STARTTLS_ENABLE=true\nexport CLOUDBREAK_SMTP_TYPE=smtp\n\n\n\n\nAccess from custom domains\n\n\nCloudbreak deployer uses UAA as an identity provider and supports multi tenancy. In UAA terminology this is referred as identity zones. An identity zone is accessed through a unique subdomain. If the standard UAA responds to \nhttps://uaa.10.244.0.34.xip.io\n a zone on this UAA would be accessed through \nhttps://testzone1.uaa.10.244.0.34.xip.io\n.\n\n\nAs an example in our hosted deployment the \nidentity.sequenceiq.com\n domain refers to our identity server and the \nUAA_ZONE_DOMAIN\n variable has to be set to that domain. This variable is necessary for UAA to identify which zone provider should handle the requests that arrives to the given domain.\n\n\nIf you want to use a custom domain for your identity or deployment, put the \nUAA_ZONE_DOMAIN\n line into your\n\nProfile\n. You can see an example in the following box:\n\n\nexport UAA_ZONE_DOMAIN=my-subdomain.example.com\n\n\n\n\nConsul\n\n\nConsul\n is used for DNS resolution. All Cloudbreak related services are registered as\n\nsomeservice.service.consul\n. Consul\u2019s built in DNS server is able to \u201cfall-back\u201d on an other DNS server.\nThis option is called \n-recursor\n. Clodbreak Deployer first tries to discover the DNS settings of the host,\nby looking for \nnameserver\n entry in \n/etc/resolv.conf\n. If it finds one consul will use it as a recursor,\notherwise \n8.8.8.8\n will be used.\n\n\nFor a full list of available consul config options, see the \ndocs\n.\n\n\nYou can pass any additional consul configuration by defining a \nDOCKER_CONSUL_OPTIONS\n in \nProfile\n.\n\n\nAzure Resource manager command\n\n\n\n\ncbd azure configure-arm\n\n\ncbd azure deploy-dash\n\nSee the documentation \nhere\n.\n\n\n\n\nCaveats\n\n\nThe \nCloudbreak Deployer\n tool opens a clean bash subshell, without inheriting environment variables.\n\n\nOnly the following environment variables \nare\n inherited:\n\n\n\n\nHOME\n\n\nDEBUG\n\n\nTRACE\n\n\nCBD_DEFAULT_PROFILE\n\n\nall \nDOCKER_XXX", 
            "title": "Advanced Configuration"
        }, 
        {
            "location": "/configuration/#configuration", 
            "text": "Configuration is based on environment variables. Cloudbreak Deployer always forks a new bash subprocess  without\ninheriting environment variables . The only way to set ENV vars relevant for Cloudbreak Deployer is to set them\nin a file called  Profile .  To see all available config variables with their default value:  cbd env show  The  Profile  will be simple  sourced  in bash terms, so you can use the usual syntaxes to set config values:  export MY_VAR=some_value\nexport OTHER_VAR=dunno", 
            "title": "Configuration"
        }, 
        {
            "location": "/configuration/#env-specific-profile", 
            "text": "Let\u2019s say you want to use a different version of Cloudbreak for  prod  and  qa  profile.\nYou can specify the Docker image tag via:  DOCKER_TAG_CLOUDBREAK . Profile  is always sourced, so you will have two env specific configurations:\n-  Profile.dev \n-  Profile.qa  For prod you need:   create a file called  Profile.prod  write the env specific  export DOCKER_TAG_CLOUDBREAK=0.3.99  into  Profile.prod  set the env variable:  CBD_DEFAULT_PROFILE=prod   To use the  prod  specific profile once:  CBD_DEFAULT_PROFILE=prod cbd some_commands  For permanent setting you can  export CBD_DEFAULT_PROFILE=prod  in your  .bash_profile .", 
            "title": "Env specific Profile"
        }, 
        {
            "location": "/configuration/#available-configurations", 
            "text": "SMTP  If you want to change SMTP parameters, put the corresponding lines into your  Profile . You can also see the default values of the parameters in the following box.  export CLOUDBREAK_SMTP_SENDER_USERNAME=\nexport CLOUDBREAK_SMTP_SENDER_PASSWORD=\nexport CLOUDBREAK_SMTP_SENDER_HOST=\nexport CLOUDBREAK_SMTP_SENDER_PORT=25\nexport CLOUDBREAK_SMTP_SENDER_FROM=\nexport CLOUDBREAK_SMTP_AUTH=true\nexport CLOUDBREAK_SMTP_STARTTLS_ENABLE=true\nexport CLOUDBREAK_SMTP_TYPE=smtp  Access from custom domains  Cloudbreak deployer uses UAA as an identity provider and supports multi tenancy. In UAA terminology this is referred as identity zones. An identity zone is accessed through a unique subdomain. If the standard UAA responds to  https://uaa.10.244.0.34.xip.io  a zone on this UAA would be accessed through  https://testzone1.uaa.10.244.0.34.xip.io .  As an example in our hosted deployment the  identity.sequenceiq.com  domain refers to our identity server and the  UAA_ZONE_DOMAIN  variable has to be set to that domain. This variable is necessary for UAA to identify which zone provider should handle the requests that arrives to the given domain.  If you want to use a custom domain for your identity or deployment, put the  UAA_ZONE_DOMAIN  line into your Profile . You can see an example in the following box:  export UAA_ZONE_DOMAIN=my-subdomain.example.com  Consul  Consul  is used for DNS resolution. All Cloudbreak related services are registered as someservice.service.consul . Consul\u2019s built in DNS server is able to \u201cfall-back\u201d on an other DNS server.\nThis option is called  -recursor . Clodbreak Deployer first tries to discover the DNS settings of the host,\nby looking for  nameserver  entry in  /etc/resolv.conf . If it finds one consul will use it as a recursor,\notherwise  8.8.8.8  will be used.  For a full list of available consul config options, see the  docs .  You can pass any additional consul configuration by defining a  DOCKER_CONSUL_OPTIONS  in  Profile .", 
            "title": "Available Configurations"
        }, 
        {
            "location": "/configuration/#azure-resource-manager-command", 
            "text": "cbd azure configure-arm  cbd azure deploy-dash \nSee the documentation  here .", 
            "title": "Azure Resource manager command"
        }, 
        {
            "location": "/configuration/#caveats", 
            "text": "The  Cloudbreak Deployer  tool opens a clean bash subshell, without inheriting environment variables.  Only the following environment variables  are  inherited:   HOME  DEBUG  TRACE  CBD_DEFAULT_PROFILE  all  DOCKER_XXX", 
            "title": "Caveats"
        }, 
        {
            "location": "/development/", 
            "text": "Development\n\n\nCloudbreak application\n\n\nLocal Development Setup\n\n\nTo use this development environment on OSX, you need to have Docker and Boot2docker installed.\n\n\nSimplest way to prepare the working environment is to start the Cloudbreak on your local machine is to use the \nCloudbreak Deployer\n.\n\n\nFirst you need to create a \nsandbox\n directory which will store the necessary  configuration files and dependencies of \nCloudbreak Deployer\n. This directory must be created outside of the cloned Cloudbreak git repository:\n\n\nmkdir cbd-local\n\n\n\n\nTo start the complete Cloudbreak ecosystem on your machine just execute the following sequence of commands:\n\n\ncd cbd-local\ncurl https://raw.githubusercontent.com/sequenceiq/cloudbreak-deployer/master/install | sh \n cbd --version\ncbd update master\ncbd init\ncbd start\n\n\n\n\nIf everything went well then the Cloudbreak will be available on http://192.168.59.103:3000. For more details and config parameters please check the documentation of \nCloudbreak Deployer\n.\n\n\nThe deployer has generated a \ncerts\n directory under \ncbd-local\n directory which will be needed later on to set up the IDEA properly.\n\n\nThe next step is to edit the \ncbd-local/Profile\n file with any editor and add the CB_SCHEMA_SCRIPTS_LOCATION environment variable which configures the location of SQL scripts that are in the 'core/src/main/resources/schema' directory in the cloned Cloudbreak git repository. Please note that the full path needs to be configured and env variables like $USER cannot be used.\n\n\nexport CB_SCHEMA_SCRIPTS_LOCATION=/Users/myusername/prj/cloudbreak/core/src/main/resources/schema\n\n\n\n\nIn order to kill Cloudbreak container running inside the boot2docker and redirect the Cloudbreak related traffic to the Cloudbreak running in IDEA use the followig command:\n\n\ncbd util local-dev\n\n\n\n\nIDEA\n\n\nCloudbreak can be imported into IDEA as gradle project. Once it is done, you need to import the proper code formatter by using the \nFile -\n Import Settings...\n menu and selecting the \nidea_settings.jar\n located in the \nconfig\n directory in Cloudbreak git repository.\n\n\nTo launch the Cloudbreak application execute the \ncom.sequenceiq.cloudbreak.CloudbreakApplication\n class with VM options:\n\n\n-XX:MaxPermSize=1024m\n-Dcb.cert.dir=FULL_PATH_OF_THE_CERTS_DIR_GENERATED_BY_CBD\n-Dcb.client.id=cloudbreak\n-Dcb.client.secret=cbsecret2015\n-Dcb.db.port.5432.tcp.addr=192.168.59.103\n-Dcb.db.port.5432.tcp.port=5432\n-Dcb.identity.server.url=http://192.168.59.103:8089\n-Dserver.port=9091\n\n\n\n\nThe \n-Dcb.cert.dir=FULL_PATH_OF_THE_CERTS_DIR_GENERATED_BY_CBD\n value above needs to be replaced with the full path of \ncerts\n directory generated by Cloudbreak Deployer e.g. \n-Dcb.cert.dir=/Users/myusername/prj/cbd-local/certs\n\n\nCommand line\n\n\nTo run Cloudbreak from command line you have to create a property file, for example application.properties, with the content below, and execute \n./gradlew bootRun -Dspring.config.location=file:/path/of/property/application.properties\n command at project root.\n\n\n-Dcb.cert.dir=FULL_PATH_OF_THE_CERTS_DIR_GENERATED_BY_CBD\n-Dcb.client.id=cloudbreak\n-Dcb.client.secret=cbsecret2015\n-Dcb.db.port.5432.tcp.addr=192.168.59.103\n-Dcb.db.port.5432.tcp.port=5432\n-Dcb.identity.server.url=http://192.168.59.103:8089\n-Dserver.port=9091\n\n\n\n\nDatabase development\n\n\nIf schema change is required in Cloudbreak database (cbdb), then the developer needs to write SQL scripts to migrate the database accordingly. In Cloudbreak the schema migration is managed by \nMYBATIS Migrations\n and the cbd tool provides an easy-to-use wrapper for it. The syntax for using the migration commands is \ncbd migrate \ndatabase name\n \ncommand\n [parameters]\n e.g. \ncbd migrate migrate status\n.\n\n\nCreate a SQL template for schema changes:\n\n\ncbd migrate cbdb new \nCLOUD-123 schema change for new feature\n\n\n\n\n\nAs as result of the above command an SQL file template is generated under the path specified in \nCB_SCHEMA_SCRIPTS_LOCATION\n environment variable, which is defined in Profile. The structure of the generated SQL template looks like the following:\n\n\n-- // CLOUD-123 schema change for new feature\n-- Migration SQL that makes the change goes here.\n\n\n\n-- //@UNDO\n-- SQL to undo the change goes here.\n\n\n\n\nOnce you have implemented your SQLs then you can execute them with:\n\n\ncbd migrate cbdb up\n\n\n\n\nIf you would like to rollback the last SQL file, then just use the down command:\n\n\ncbd migrate cbdb down\n\n\n\n\nOn order to check the status of database\n\n\ncbd migrate cbdb status\n\n#Every script that has not been executed will be marked as ...pending... in the output of status command:\n\n------------------------------------------------------------------------\n-- MyBatis Migrations - status\n------------------------------------------------------------------------\nID             Applied At          Description\n================================================================================\n20150421140021 2015-07-08 10:04:28 create changelog\n20150421150000 2015-07-08 10:04:28 CLOUD-607 create baseline schema\n20150507121756 2015-07-08 10:04:28 CLOUD-576 change instancegrouptype hostgroup to core\n20151008090632    ...pending...    CLOUD-123 schema change for new feature\n\n------------------------------------------------------------------------\n\n\n\n\nBuilding\n\n\nGradle is used for build and dependency management. Gradle wrapper is added to Cloudbreak git repository, therefore building can be done with:\n\n\n./gradlew clean build\n\n\n\n\nCloudbreak deployer\n\n\nContribution\n\n\nDevelopment process should happen on separate branches. Then a pull-request should be opened as usual.\n\n\nTo build the project\n\n\n# make deps needed only once\nmake deps\n\nmake install\n\n\n\n\nTo run the unit tests:\n\n\nmake tests\n\n\n\n\nIf you want to test the binary CircleCI build from your branch named \nfix-something\n, to validate the PR binary \ncbd\n tool will be tested. It is built by CircleCI for each branch.\n\n\ncbd update fix-something\n\n\n\n\nSnapshots\n\n\nWe recommend to always use the latest release, but you might want to check new features or bugfixes.\nAll successful builds from the \nmaster\n branch are uploaded to the public S3 bucket. You can download it:\n\n\ncurl -L public-repo-1.hortonworks.com/HDP/cloudbreak/cloudbreak-deployer_snapshot_$(uname)_x86_64.tgz | tar -xz\n\n\n\n\nInstead of overwriting the released version, download it to a \nlocal directory\n and useit by refering as \n./cbd\n\n\n./cbd --version\n\n\n\n\nTesting\n\n\nShell scripts shouldn\u2019t raise exceptions when it comes to unit testing. \nbasht\n is\n used for testing. See the reasoning: \nwhy not bats or shunit2\n.\n\n\nPlease cover your bash functions with unit tests and run test with:\n\n\nmake tests\n\n\n\n\nRelease Process of the Clodbreak Deployer tool\n\n\nThe master branch is always built on \nCircleCI\n.\nWhen you wan\u2019t a new release, all you have to do:\n\n\nmake release-next-ver\n\n\n\n\nmake release-next-ver\n performs the following steps:\n\n\n\n\nOn the \nmaster\n branch:\n\n\nUpdates the \nVERSION\n file by increasing the \npatch\n version number (for example from 0.5.2 to 0.5.3)\n\n\nUpdates \nCHANGELOG.md\n with the release date\n\n\nCreates a new \nUnreleased\n section in top of \nCHANGELOG.md\n\n\n\n\n\n\nCreates a PullRequest for the release branch:\n\n\ncreate a new branch with a name like \nrelease-0.5.x\n\n\nthis branch should be the same as \norigin/master\n\n\ncreate a pull request into \nrelease\n branch\n\n\n\n\n\n\n\n\nAcceptance\n\n\nNow you should test this release. You can get it by \ncbd update release-x.y.z\n. Comment with LGTM (Looking Good To Me).\n\n\nOnce the PR is merged, CircleCI will:\n\n\n\n\ncreate a new release on \nGitHub releases tab\n, with the\n help of \ngh-release\n.\n\n\nit will create the git tag with \nv\n prefix like: \nv0.0.3", 
            "title": "Development"
        }, 
        {
            "location": "/development/#development", 
            "text": "", 
            "title": "Development"
        }, 
        {
            "location": "/development/#cloudbreak-application", 
            "text": "Local Development Setup  To use this development environment on OSX, you need to have Docker and Boot2docker installed.  Simplest way to prepare the working environment is to start the Cloudbreak on your local machine is to use the  Cloudbreak Deployer .  First you need to create a  sandbox  directory which will store the necessary  configuration files and dependencies of  Cloudbreak Deployer . This directory must be created outside of the cloned Cloudbreak git repository:  mkdir cbd-local  To start the complete Cloudbreak ecosystem on your machine just execute the following sequence of commands:  cd cbd-local\ncurl https://raw.githubusercontent.com/sequenceiq/cloudbreak-deployer/master/install | sh   cbd --version\ncbd update master\ncbd init\ncbd start  If everything went well then the Cloudbreak will be available on http://192.168.59.103:3000. For more details and config parameters please check the documentation of  Cloudbreak Deployer .  The deployer has generated a  certs  directory under  cbd-local  directory which will be needed later on to set up the IDEA properly.  The next step is to edit the  cbd-local/Profile  file with any editor and add the CB_SCHEMA_SCRIPTS_LOCATION environment variable which configures the location of SQL scripts that are in the 'core/src/main/resources/schema' directory in the cloned Cloudbreak git repository. Please note that the full path needs to be configured and env variables like $USER cannot be used.  export CB_SCHEMA_SCRIPTS_LOCATION=/Users/myusername/prj/cloudbreak/core/src/main/resources/schema  In order to kill Cloudbreak container running inside the boot2docker and redirect the Cloudbreak related traffic to the Cloudbreak running in IDEA use the followig command:  cbd util local-dev  IDEA  Cloudbreak can be imported into IDEA as gradle project. Once it is done, you need to import the proper code formatter by using the  File -  Import Settings...  menu and selecting the  idea_settings.jar  located in the  config  directory in Cloudbreak git repository.  To launch the Cloudbreak application execute the  com.sequenceiq.cloudbreak.CloudbreakApplication  class with VM options:  -XX:MaxPermSize=1024m\n-Dcb.cert.dir=FULL_PATH_OF_THE_CERTS_DIR_GENERATED_BY_CBD\n-Dcb.client.id=cloudbreak\n-Dcb.client.secret=cbsecret2015\n-Dcb.db.port.5432.tcp.addr=192.168.59.103\n-Dcb.db.port.5432.tcp.port=5432\n-Dcb.identity.server.url=http://192.168.59.103:8089\n-Dserver.port=9091  The  -Dcb.cert.dir=FULL_PATH_OF_THE_CERTS_DIR_GENERATED_BY_CBD  value above needs to be replaced with the full path of  certs  directory generated by Cloudbreak Deployer e.g.  -Dcb.cert.dir=/Users/myusername/prj/cbd-local/certs  Command line  To run Cloudbreak from command line you have to create a property file, for example application.properties, with the content below, and execute  ./gradlew bootRun -Dspring.config.location=file:/path/of/property/application.properties  command at project root.  -Dcb.cert.dir=FULL_PATH_OF_THE_CERTS_DIR_GENERATED_BY_CBD\n-Dcb.client.id=cloudbreak\n-Dcb.client.secret=cbsecret2015\n-Dcb.db.port.5432.tcp.addr=192.168.59.103\n-Dcb.db.port.5432.tcp.port=5432\n-Dcb.identity.server.url=http://192.168.59.103:8089\n-Dserver.port=9091  Database development  If schema change is required in Cloudbreak database (cbdb), then the developer needs to write SQL scripts to migrate the database accordingly. In Cloudbreak the schema migration is managed by  MYBATIS Migrations  and the cbd tool provides an easy-to-use wrapper for it. The syntax for using the migration commands is  cbd migrate  database name   command  [parameters]  e.g.  cbd migrate migrate status .  Create a SQL template for schema changes:  cbd migrate cbdb new  CLOUD-123 schema change for new feature   As as result of the above command an SQL file template is generated under the path specified in  CB_SCHEMA_SCRIPTS_LOCATION  environment variable, which is defined in Profile. The structure of the generated SQL template looks like the following:  -- // CLOUD-123 schema change for new feature\n-- Migration SQL that makes the change goes here.\n\n\n\n-- //@UNDO\n-- SQL to undo the change goes here.  Once you have implemented your SQLs then you can execute them with:  cbd migrate cbdb up  If you would like to rollback the last SQL file, then just use the down command:  cbd migrate cbdb down  On order to check the status of database  cbd migrate cbdb status\n\n#Every script that has not been executed will be marked as ...pending... in the output of status command:\n\n------------------------------------------------------------------------\n-- MyBatis Migrations - status\n------------------------------------------------------------------------\nID             Applied At          Description\n================================================================================\n20150421140021 2015-07-08 10:04:28 create changelog\n20150421150000 2015-07-08 10:04:28 CLOUD-607 create baseline schema\n20150507121756 2015-07-08 10:04:28 CLOUD-576 change instancegrouptype hostgroup to core\n20151008090632    ...pending...    CLOUD-123 schema change for new feature\n\n------------------------------------------------------------------------  Building  Gradle is used for build and dependency management. Gradle wrapper is added to Cloudbreak git repository, therefore building can be done with:  ./gradlew clean build", 
            "title": "Cloudbreak application"
        }, 
        {
            "location": "/development/#cloudbreak-deployer", 
            "text": "Contribution  Development process should happen on separate branches. Then a pull-request should be opened as usual.  To build the project  # make deps needed only once\nmake deps\n\nmake install  To run the unit tests:  make tests  If you want to test the binary CircleCI build from your branch named  fix-something , to validate the PR binary  cbd  tool will be tested. It is built by CircleCI for each branch.  cbd update fix-something  Snapshots  We recommend to always use the latest release, but you might want to check new features or bugfixes.\nAll successful builds from the  master  branch are uploaded to the public S3 bucket. You can download it:  curl -L public-repo-1.hortonworks.com/HDP/cloudbreak/cloudbreak-deployer_snapshot_$(uname)_x86_64.tgz | tar -xz  Instead of overwriting the released version, download it to a  local directory  and useit by refering as  ./cbd  ./cbd --version  Testing  Shell scripts shouldn\u2019t raise exceptions when it comes to unit testing.  basht  is\n used for testing. See the reasoning:  why not bats or shunit2 .  Please cover your bash functions with unit tests and run test with:  make tests  Release Process of the Clodbreak Deployer tool  The master branch is always built on  CircleCI .\nWhen you wan\u2019t a new release, all you have to do:  make release-next-ver  make release-next-ver  performs the following steps:   On the  master  branch:  Updates the  VERSION  file by increasing the  patch  version number (for example from 0.5.2 to 0.5.3)  Updates  CHANGELOG.md  with the release date  Creates a new  Unreleased  section in top of  CHANGELOG.md    Creates a PullRequest for the release branch:  create a new branch with a name like  release-0.5.x  this branch should be the same as  origin/master  create a pull request into  release  branch     Acceptance  Now you should test this release. You can get it by  cbd update release-x.y.z . Comment with LGTM (Looking Good To Me).  Once the PR is merged, CircleCI will:   create a new release on  GitHub releases tab , with the\n help of  gh-release .  it will create the git tag with  v  prefix like:  v0.0.3", 
            "title": "Cloudbreak deployer"
        }, 
        {
            "location": "/changelog/", 
            "text": "Unreleased\n\n\nFixed\n\n\n\n\nconsul recursor now exculdes both docker ip and bridge ip to avoid recursive dns recursor chain\n\n\ndocs fixed about getting default credentials (cbd login)\n\n\nupdates cb-shell to 0.5.37 to fix ssl issues\n\n\n\n\nAdded\n\n\n\n\nCommand \ncbd azure configure-arm\n will create your arm application which can used by cloudbreak\n\n\nCommand \ncbd azure deploy-dash\n will deploy a dash application in your Azure account\n\n\nCommand \ncbd start\n will execute the migration by default. If SKIP_DB_MIGRATION_ON_START envvar set to true in Profile, the migration will be skipped\n\n\nUsing Dns SRV record in our services instead of ambassador\n\n\nUsing docker linking system in third party services instead of ambassador\n\n\nIntegration tests are added, where cbd binary is called, not only sourced functions\n\n\nDocker based CentOS integration test make target added\n\n\nUaa db migration\n\n\nSMTP default parameters added: \nCLOUDBREAK_SMTP_AUTH\n and \nCLOUDBREAK_SMTP_STARTTLS_ENABLE\n and \nCLOUDBREAK_SMTP_TYPE\n\n\nLocal development Uluwatu configuration by ULUWATU_VOLUME_HOST environment variable\n\n\nLocal development Sultans configuration by SULTANS_VOLUME_HOST environment variable\n\n\ninstall script for fixed version and install-latest for latest release added\n\n\nEach snapshot artifact is uploaded as http://public-repo-1.hortonworks.com/HDP/cloudbreak/cbd-snapshot-$(uname).tgz\n\n\n\n\nRemoved\n\n\n\n\nFull removal of ambassador\n\n\n\n\nChanged\n\n\n\n\ncbd start\n doesn\u2019t start if compose yaml regeneration is needed\n\n\ncbd generate\n is less verbose, diff doesnt shown\n\n\ncbd doctor\n shows diff if generate would change\n\n\ncbd regenerate\n creates backup files if changes detected\n\n\nsequenceiq/uaadb:1.0.1 is used instead of postgres:9.4.1\n\n\n\n\n[v1.0.3] - 2015-09-03\n\n\nFixed\n\n\n\n\nAuthentication error with \ncloudbreak-shell\n and \ncloudbreak-shell-quiet\n is fixed\n\n\nCommand \ncbd update \nbranch\n checks for artifact\n\n\n\n\nAdded\n\n\n\n\nbinary version of gnu-sed 4.2.2 is now included, to solve lot of osx/busybox issues\n\n\nconsul recursor test are added\n\n\n\n\nRemoved\n\n\nChanged\n\n\n\n\n\n\nsequenceiq/cloudbreak image updated to 1.0.3\n\n\n\n\n\n\ndebug() function made multiline capable. Use \\n in messages\n\n\n\n\nrefactor bridge ip discovery to run helper docker command only once\n\n\nconsul recursor handling refactored to be more robust\n\n\n\n\n[v1.0.2] - 2015-08-25\n\n\nFixed\n\n\nAdded\n\n\n\n\nDOCKER_CONSUL_OPTIONS\n config option to provide arbitrary consul option\n\n\n\n\nRemoved\n\n\nChanged\n\n\n\n\nFixed docker version checker to be 1.8.1 compatible. (docker added --format option)\n\n\nsequenceiq/cloudbreak image updated to 1.0.2\n\n\nconsul image changed from sequenceiq/consul to gliderlabs/consul\n\n\nconsul image updated to 0.5.2 (from 0.5.0)\n\n\nconsul discovers host dns settings, and uses the configured nameserver as recursor\n\n\n\n\n[v1.0.0] - 2015-08-15\n\n\nFixed\n\n\nAdded\n\n\nRemoved\n\n\nChanged\n\n\n[v1.0.0] - 2015-07-23\n\n\nFixed\n\n\n\n\nGA Release\n\n\n\n\nAdded\n\n\nRemoved\n\n\nChanged\n\n\n[v0.5.8] - 2015-07-23\n\n\nFixed\n\n\n\n\nFix CircleCI release. CircleCI doesn\u2019t allow --rm on docker run\n\n\n\n\nAdded\n\n\nRemoved\n\n\nChanged\n\n\n[v0.5.7] - 2015-07-23\n\n\nFixed\n\n\n\n\nFix make release dependency\n\n\nFix CHANGELOG generation at \nmake release-next-ver\n avoid inserting extra -e\n\n\n\n\nAdded\n\n\nRemoved\n\n\nChanged\n\n\n[v0.5.6] - 2015-07-23\n\n\nFixed\n\n\nAdded\n\n\n\n\nRelease artifacts are published at public-repo-1.hortonworks.com/HDP/cloudbreak/\n\n\n\n\nRemoved\n\n\nChanged\n\n\n[v0.5.5] - 2015-07-10\n\n\nFixed\n\n\nAdded\n\n\n\n\nCommand \npull-parallel\n added for quicker/simultaneous image pull\n\n\nRelease process includes upload to public-repo s3 bucket\n\n\n\n\nRemoved\n\n\nChanged\n\n\n\n\nLicense changed from MIT to Apache v2\n\n\nrelease artifact includes additional files: license/readme/notes\n\n\n\n\n[v0.5.4] - 2015-07-03\n\n\nFixed\n\n\nAdded\n\n\n\n\nNew \ncbd-cleanup\n command for removing old images or exited containers\n\n\nBaywatch default parameters added: \nCB_BAYWATCH_ENABLED\n and\nCB_BAYWATCH_EXTERN_LOCATION\n\n\nLogs are saved via lospout\n\n\nTLS client certificate needed by Cloudbreak is generated with \ncbd generate\n\n\nCommand \naws delete-role\n added\n\n\nCommand \naws generate-role\n added\n\n\nCommand \naws show-role\n added\n\n\nCommand \ncloudbreak-shell\n added\n\n\nCommand \ncloudbreak-shell-quiet\n added\n\n\nCommand \nlocal-dev\n added\n\n\nCommand \ntoken\n added\n\n\n\n\nRemoved\n\n\nChanged\n\n\n\n\nAWS authentication env varibale is fixed to use the correct AWS_SECRET_ACCESS_KEY (instead the old AWS_SECRET_KEY)\n\n\nUsing sequenceiq/ambassadord:0.5.0 docker image instead of progrium/ambassadord:latest\n\n\n\n\n[v0.5.3] - 2015-06-03\n\n\nFixed\n\n\n\n\nOne-liner installer fixed, to work if previous cbd exists on path.\n\n\ncbd update\n upstream changes on go-bahser broke the selfupdate functionality\n\n\nIn some environment cloudbreak starts really slow. See: \ndetails\n, see: \ncommit\n\n\n\n\nAdded\n\n\n\n\nNew release proposal can be done by \nmake release-next-ver\n\n\n\n\nRemoved\n\n\nChanged\n\n\n[v0.5.2] - 2015-05-21\n\n\nFixed\n\n\nAdded\n\n\nRemoved\n\n\nChanged\n\n\n\n\nCommand \ndoctor\n hints to run boot2docker shellinit if env is unset\n\n\nCommand \ninit\n in case of OSX, DOCKER_XXX envs are initialized in local profile (Profile)\n\n\nDefault docker images are updated to:\n\n\nsequenceiq/cloudbreak:0.5.93\n\n\nsequenceiq/cbdb:0.5.92\n\n\nsequenceiq/uluwatu:0.5.28\n\n\nsequenceiq/sultans:0.5.3\n\n\nsequenceiq/periscope:0.5.5\n\n\n\n\n\n\n\n\n[v0.5.1] - 2015-05-18\n\n\nFixed\n\n\n\n\nIssue #55: Sed handles more robust the issue with: curl includes an extra CR char in header fields.\n\n\n\n\nAdded\n\n\nRemoved\n\n\n\n\ndeployer doesn\u2019t specify cloud specific image defaults. If left empty, they fall back\n  to defaults specified in \njava code\n\n\nCB_AZURE_IMAGE_URI\n\n\nCB_AWS_AMI_MAP\n\n\nCB_OPENSTACK_IMAGE\n\n\nCB_GCP_SOURCE_IMAGE_PATH\n\n\n\n\n\n\n\n\nChanged\n\n\n\n\nCommand \nlogs\n got usage example for specifying servies as filter\n\n\nDefault docker images are updated to:\n\n\nsequenceiq/cloudbreak:0.5.49\n\n\nsequenceiq/uluwatu:0.5.16\n\n\nsequenceiq/sutans:0.5.2\n\n\n\n\n\n\n\n\n[v0.5.0] - 2015-05-08\n\n\nFixed\n\n\n\n\nCommand \npull\n generates yaml files in case they are missing #31\n\n\n\n\nAdded\n\n\n\n\nCommand \nlogin\n Shows Uluwatu login url and credentials\n\n\nCommand \nregenerate\n deletes and generates docker-compose.yml and uaa.yml\n\n\nCommand \ndelete\n added: deletes yamls and dbs\n\n\nCommand \ncloudbreak-shell\n added, right now it internale use DEBUG=1 fn fn-call\n\n\nCommand \nversion\n does correct \nSemantic Versioning\n check to advise an upgrade\n\n\nCommand \ngenerate\n checks and shows if Profile change would result in yaml change.\n\n\nCommand \nstart\n: prints uluwatu url and credential hint\n\n\nCommand \ndoctor\n: fixes boot2docker date/time if not the same as on the host\n\n\nInternal command: \nbrowse\n added to be able to automatically open a browser to a specified url.\n\n\nMini Getting Started guide added into README\n\n\nmake dev-debug\n installs a special cbd on local OSX, which doesn\u2019t includes *.bash scrips, only refers them\n   by path. No need to \nmake dev\n to test small changes in bash scripts.\n\n\nLoad AWS key and AWS id from Profile\n\n\nCommand \ninit\n helps to guess the PUBLIC_IP in public clouds: google, amazon\n\n\n\n\nRemoved\n\n\nChanged\n\n\n\n\nCommand \ncbd env export\n adds export to the begining of each line\n\n\ncbd logs accepts optional [services] parameter\n\n\ndocker-compose uses \ncbreak_\n prefix for container naming instead of the directory name\n\n\nCommand \ngenerate\n prints out some more usefull info\n\n\nuaa.yml generation wont overwrite, just instruct to move existing file (like docker-compose.yml generation)\n\n\nCommand \ninit\n hint fixed on linux.\n\n\nCommand \ninit\n advise to run \ngenerate\n if it finds a Profile\n\n\nCommand \ninit\n set PRIVATE_IP the same as PUBLIC_IP for boot2docker\n\n\nCommand \nmigrate\n is introduced for db migration see \nMigrate the databases\n section of README\n\n\nCommand \nstartdb\n starts the cbdb and pcdb database containers only\n\n\nDatabases are not deleted after boot2docker restart\n\n\nImport ULU_HOST_ADDRESS and ULU_SULTANS_ADDRESS from Profile\n\n\n\n\n[v0.1.0] - 2015-04-16\n\n\nFixed\n\n\n\n\nSelfupdate updates the actual running binary intead of the fixed /us/local/bin/cbd\n\n\nSMTP default port is 25, to fix number conversion exception\n\n\n\n\nAdded\n\n\n\n\nCommand \ninit\n creates Profile\n\n\nInstall cbd to a directory which is available on $PATH\n\n\nDocker based test for the one-liner install from README.md: \nmake install-test\n\n\n\n\nRemoved\n\n\n\n\nupdate-snap\n command removed, replaced by parametrized \nupdate\n\n\n\n\nChanged\n\n\n\n\nCloudbreak/Persicope/Uluwatu/Sultans Dcoker images upgraded to 0.4.x\n\n\nUse the built in 'checksum' function instead of the external 'shasum' to generate secrets\n\n\nCommand \nupdate\n by default updates from latest Github release, parameter can point to branch on CircleCI\n\n\nDOCKER_XXX env varibles are inherited, so they not needed in Profile\n\n\ngenerate\n and compose specific commands are only available when \nProfile\n exists\n\n\ngenerate\n command genertes docker-compose.yml \nand\n uaa.yml\n\n\nPRIVATE_IP\n env var defaults to bridge IP (only PUBLC_IP is required in Profile)\n\n\nuse \nsulans-bin\n docker image istead of sultans\n\n\n\n\n[v0.0.9] - 2015-04-14\n\n\nFixed\n\n\n\n\nBash 4.3 is included in the binary, extracted into .deps/bin upon start\n\n\n\n\nAdded\n\n\nRemoved\n\n\nChanged\n\n\n[v0.0.8] - 2015-04-13\n\n\nFixed\n\n\n\n\nFixing deps module, golang fn: checksum added\n\n\nCircleCI mdule defines required jq\n\n\nFixing PATH issue for binary deps\n\n\n\n\nAdded\n\n\n\n\nuaadb start added\n\n\nidentity server start added\n\n\nmake dev\n added to mac based development\n\n\npull\n command added\n\n\nlogs\n command added\n\n\n\n\nRemoved\n\n\nChanged\n\n\n\n\nDocker containers are managed by \ndocker-compose\n\n\n\n\n[v0.0.7] - 2015-03-26\n\n\nFixed\n\n\nAdded\n\n\n\n\nmake tests\n runs unit tests\n\n\ndocker unit tests are added\n\n\nstart command added: WIP consul, registrator starts\n\n\nkill command addd: stops and removes cloudbreak specific containers\n\n\nSKIP_XXX skips the container start\n\n\n\n\nRemoved\n\n\nChanged\n\n\n\n\nenv command namespace is always exported, not only in DEBUG mode\n\n\nenv export: machine friendly config list\n\n\nenv show: human readable config list\n\n\ncircle runs unit tests\n\n\nsnapshot binaries include branch name in version string\n\n\n\n\n[v0.0.6] - 2015-03-25\n\n\nFixed\n\n\n\n\nremoved dos2unix dependency for the update command\n\n\n\n\nAdded\n\n\n\n\ndoctor command added\n\n\ndocker-check-version command added\n\n\ncci-latest accepts branch as parameter, needed for PR testing\n\n\nexport fn command in DEBUG mode\n\n\nexport env command in DEBUG mode\n\n\ndoctor: add instruction about setting DOCKER_XXX env vars in Profile\n\n\ninfo() function added to print green text to STDOUT\n\n\n\n\nRemoved\n\n\nChanged\n\n\n\n\nHOME env var is also inherited (boot2docker version failed)\n\n\nrelease process fully automatized\n\n\n\n\n[v0.0.5] - 2015-03-23\n\n\n\n\nupdate\n command works without dos2unix\n\n\n\n\n[v0.0.4] - 2015-03-23\n\n\nFixed\n\n\n\n\ndebug function fixed\n\n\nDEBUG, TRACE and CBD_DEFAULT_PROFILE env vars are inherited\n\n\n\n\nAdded\n\n\n\n\nProfile handling added with docs\n\n\nOne-liner install added\n\n\nDocs: install and update process described\n\n\nDocs: release process described with sample git commands\n\n\nPrint version number in debug mode\n\n\nupdate-snap\n downloads binary from latest os specific CircleCI binary artifact.\n\n\n\n\nRemoved\n\n\nChanged\n\n\n\n\nTool specific library renamed from cloudbreak.bash to deployer.bash\n\n\n\n\n[v0.0.6] - 2015-03-25\n\n\n[v0.0.3] - 2015-03-19\n\n\nFixed\n\n\n\n\nmake release\n creates binary with X.X.X version when on release branch otherwise X.X.X-gitrev\n\n\n\n\nAdded\n\n\n\n\nDocs: release process described\n\n\n\n\nRemoved\n\n\nChanged\n\n\n[v0.0.2] - 2015-03-19\n\n\nAdded\n\n\nAdded\n- selfupdate command\n- gray debug to stderr\n\n\n[v0.0.1] - 2015-03-18\n\n\nAdded\n\n\n\n\nhelp command added\n\n\nversion command added\n\n\nAdded --version\n\n\nCircleCI build\n\n\nLinux/Darwin binary releases on github", 
            "title": "Changelog"
        }, 
        {
            "location": "/changelog/#unreleased", 
            "text": "Fixed   consul recursor now exculdes both docker ip and bridge ip to avoid recursive dns recursor chain  docs fixed about getting default credentials (cbd login)  updates cb-shell to 0.5.37 to fix ssl issues   Added   Command  cbd azure configure-arm  will create your arm application which can used by cloudbreak  Command  cbd azure deploy-dash  will deploy a dash application in your Azure account  Command  cbd start  will execute the migration by default. If SKIP_DB_MIGRATION_ON_START envvar set to true in Profile, the migration will be skipped  Using Dns SRV record in our services instead of ambassador  Using docker linking system in third party services instead of ambassador  Integration tests are added, where cbd binary is called, not only sourced functions  Docker based CentOS integration test make target added  Uaa db migration  SMTP default parameters added:  CLOUDBREAK_SMTP_AUTH  and  CLOUDBREAK_SMTP_STARTTLS_ENABLE  and  CLOUDBREAK_SMTP_TYPE  Local development Uluwatu configuration by ULUWATU_VOLUME_HOST environment variable  Local development Sultans configuration by SULTANS_VOLUME_HOST environment variable  install script for fixed version and install-latest for latest release added  Each snapshot artifact is uploaded as http://public-repo-1.hortonworks.com/HDP/cloudbreak/cbd-snapshot-$(uname).tgz   Removed   Full removal of ambassador   Changed   cbd start  doesn\u2019t start if compose yaml regeneration is needed  cbd generate  is less verbose, diff doesnt shown  cbd doctor  shows diff if generate would change  cbd regenerate  creates backup files if changes detected  sequenceiq/uaadb:1.0.1 is used instead of postgres:9.4.1", 
            "title": "Unreleased"
        }, 
        {
            "location": "/changelog/#v103-2015-09-03", 
            "text": "Fixed   Authentication error with  cloudbreak-shell  and  cloudbreak-shell-quiet  is fixed  Command  cbd update  branch  checks for artifact   Added   binary version of gnu-sed 4.2.2 is now included, to solve lot of osx/busybox issues  consul recursor test are added   Removed  Changed    sequenceiq/cloudbreak image updated to 1.0.3    debug() function made multiline capable. Use \\n in messages   refactor bridge ip discovery to run helper docker command only once  consul recursor handling refactored to be more robust", 
            "title": "[v1.0.3] - 2015-09-03"
        }, 
        {
            "location": "/changelog/#v102-2015-08-25", 
            "text": "Fixed  Added   DOCKER_CONSUL_OPTIONS  config option to provide arbitrary consul option   Removed  Changed   Fixed docker version checker to be 1.8.1 compatible. (docker added --format option)  sequenceiq/cloudbreak image updated to 1.0.2  consul image changed from sequenceiq/consul to gliderlabs/consul  consul image updated to 0.5.2 (from 0.5.0)  consul discovers host dns settings, and uses the configured nameserver as recursor", 
            "title": "[v1.0.2] - 2015-08-25"
        }, 
        {
            "location": "/changelog/#v100-2015-08-15", 
            "text": "Fixed  Added  Removed  Changed", 
            "title": "[v1.0.0] - 2015-08-15"
        }, 
        {
            "location": "/changelog/#v100-2015-07-23", 
            "text": "Fixed   GA Release   Added  Removed  Changed", 
            "title": "[v1.0.0] - 2015-07-23"
        }, 
        {
            "location": "/changelog/#v058-2015-07-23", 
            "text": "Fixed   Fix CircleCI release. CircleCI doesn\u2019t allow --rm on docker run   Added  Removed  Changed", 
            "title": "[v0.5.8] - 2015-07-23"
        }, 
        {
            "location": "/changelog/#v057-2015-07-23", 
            "text": "Fixed   Fix make release dependency  Fix CHANGELOG generation at  make release-next-ver  avoid inserting extra -e   Added  Removed  Changed", 
            "title": "[v0.5.7] - 2015-07-23"
        }, 
        {
            "location": "/changelog/#v056-2015-07-23", 
            "text": "Fixed  Added   Release artifacts are published at public-repo-1.hortonworks.com/HDP/cloudbreak/   Removed  Changed", 
            "title": "[v0.5.6] - 2015-07-23"
        }, 
        {
            "location": "/changelog/#v055-2015-07-10", 
            "text": "Fixed  Added   Command  pull-parallel  added for quicker/simultaneous image pull  Release process includes upload to public-repo s3 bucket   Removed  Changed   License changed from MIT to Apache v2  release artifact includes additional files: license/readme/notes", 
            "title": "[v0.5.5] - 2015-07-10"
        }, 
        {
            "location": "/changelog/#v054-2015-07-03", 
            "text": "Fixed  Added   New  cbd-cleanup  command for removing old images or exited containers  Baywatch default parameters added:  CB_BAYWATCH_ENABLED  and CB_BAYWATCH_EXTERN_LOCATION  Logs are saved via lospout  TLS client certificate needed by Cloudbreak is generated with  cbd generate  Command  aws delete-role  added  Command  aws generate-role  added  Command  aws show-role  added  Command  cloudbreak-shell  added  Command  cloudbreak-shell-quiet  added  Command  local-dev  added  Command  token  added   Removed  Changed   AWS authentication env varibale is fixed to use the correct AWS_SECRET_ACCESS_KEY (instead the old AWS_SECRET_KEY)  Using sequenceiq/ambassadord:0.5.0 docker image instead of progrium/ambassadord:latest", 
            "title": "[v0.5.4] - 2015-07-03"
        }, 
        {
            "location": "/changelog/#v053-2015-06-03", 
            "text": "Fixed   One-liner installer fixed, to work if previous cbd exists on path.  cbd update  upstream changes on go-bahser broke the selfupdate functionality  In some environment cloudbreak starts really slow. See:  details , see:  commit   Added   New release proposal can be done by  make release-next-ver   Removed  Changed", 
            "title": "[v0.5.3] - 2015-06-03"
        }, 
        {
            "location": "/changelog/#v052-2015-05-21", 
            "text": "Fixed  Added  Removed  Changed   Command  doctor  hints to run boot2docker shellinit if env is unset  Command  init  in case of OSX, DOCKER_XXX envs are initialized in local profile (Profile)  Default docker images are updated to:  sequenceiq/cloudbreak:0.5.93  sequenceiq/cbdb:0.5.92  sequenceiq/uluwatu:0.5.28  sequenceiq/sultans:0.5.3  sequenceiq/periscope:0.5.5", 
            "title": "[v0.5.2] - 2015-05-21"
        }, 
        {
            "location": "/changelog/#v051-2015-05-18", 
            "text": "Fixed   Issue #55: Sed handles more robust the issue with: curl includes an extra CR char in header fields.   Added  Removed   deployer doesn\u2019t specify cloud specific image defaults. If left empty, they fall back\n  to defaults specified in  java code  CB_AZURE_IMAGE_URI  CB_AWS_AMI_MAP  CB_OPENSTACK_IMAGE  CB_GCP_SOURCE_IMAGE_PATH     Changed   Command  logs  got usage example for specifying servies as filter  Default docker images are updated to:  sequenceiq/cloudbreak:0.5.49  sequenceiq/uluwatu:0.5.16  sequenceiq/sutans:0.5.2", 
            "title": "[v0.5.1] - 2015-05-18"
        }, 
        {
            "location": "/changelog/#v050-2015-05-08", 
            "text": "Fixed   Command  pull  generates yaml files in case they are missing #31   Added   Command  login  Shows Uluwatu login url and credentials  Command  regenerate  deletes and generates docker-compose.yml and uaa.yml  Command  delete  added: deletes yamls and dbs  Command  cloudbreak-shell  added, right now it internale use DEBUG=1 fn fn-call  Command  version  does correct  Semantic Versioning  check to advise an upgrade  Command  generate  checks and shows if Profile change would result in yaml change.  Command  start : prints uluwatu url and credential hint  Command  doctor : fixes boot2docker date/time if not the same as on the host  Internal command:  browse  added to be able to automatically open a browser to a specified url.  Mini Getting Started guide added into README  make dev-debug  installs a special cbd on local OSX, which doesn\u2019t includes *.bash scrips, only refers them\n   by path. No need to  make dev  to test small changes in bash scripts.  Load AWS key and AWS id from Profile  Command  init  helps to guess the PUBLIC_IP in public clouds: google, amazon   Removed  Changed   Command  cbd env export  adds export to the begining of each line  cbd logs accepts optional [services] parameter  docker-compose uses  cbreak_  prefix for container naming instead of the directory name  Command  generate  prints out some more usefull info  uaa.yml generation wont overwrite, just instruct to move existing file (like docker-compose.yml generation)  Command  init  hint fixed on linux.  Command  init  advise to run  generate  if it finds a Profile  Command  init  set PRIVATE_IP the same as PUBLIC_IP for boot2docker  Command  migrate  is introduced for db migration see  Migrate the databases  section of README  Command  startdb  starts the cbdb and pcdb database containers only  Databases are not deleted after boot2docker restart  Import ULU_HOST_ADDRESS and ULU_SULTANS_ADDRESS from Profile", 
            "title": "[v0.5.0] - 2015-05-08"
        }, 
        {
            "location": "/changelog/#v010-2015-04-16", 
            "text": "Fixed   Selfupdate updates the actual running binary intead of the fixed /us/local/bin/cbd  SMTP default port is 25, to fix number conversion exception   Added   Command  init  creates Profile  Install cbd to a directory which is available on $PATH  Docker based test for the one-liner install from README.md:  make install-test   Removed   update-snap  command removed, replaced by parametrized  update   Changed   Cloudbreak/Persicope/Uluwatu/Sultans Dcoker images upgraded to 0.4.x  Use the built in 'checksum' function instead of the external 'shasum' to generate secrets  Command  update  by default updates from latest Github release, parameter can point to branch on CircleCI  DOCKER_XXX env varibles are inherited, so they not needed in Profile  generate  and compose specific commands are only available when  Profile  exists  generate  command genertes docker-compose.yml  and  uaa.yml  PRIVATE_IP  env var defaults to bridge IP (only PUBLC_IP is required in Profile)  use  sulans-bin  docker image istead of sultans", 
            "title": "[v0.1.0] - 2015-04-16"
        }, 
        {
            "location": "/changelog/#v009-2015-04-14", 
            "text": "Fixed   Bash 4.3 is included in the binary, extracted into .deps/bin upon start   Added  Removed  Changed", 
            "title": "[v0.0.9] - 2015-04-14"
        }, 
        {
            "location": "/changelog/#v008-2015-04-13", 
            "text": "Fixed   Fixing deps module, golang fn: checksum added  CircleCI mdule defines required jq  Fixing PATH issue for binary deps   Added   uaadb start added  identity server start added  make dev  added to mac based development  pull  command added  logs  command added   Removed  Changed   Docker containers are managed by  docker-compose", 
            "title": "[v0.0.8] - 2015-04-13"
        }, 
        {
            "location": "/changelog/#v007-2015-03-26", 
            "text": "Fixed  Added   make tests  runs unit tests  docker unit tests are added  start command added: WIP consul, registrator starts  kill command addd: stops and removes cloudbreak specific containers  SKIP_XXX skips the container start   Removed  Changed   env command namespace is always exported, not only in DEBUG mode  env export: machine friendly config list  env show: human readable config list  circle runs unit tests  snapshot binaries include branch name in version string", 
            "title": "[v0.0.7] - 2015-03-26"
        }, 
        {
            "location": "/changelog/#v006-2015-03-25", 
            "text": "Fixed   removed dos2unix dependency for the update command   Added   doctor command added  docker-check-version command added  cci-latest accepts branch as parameter, needed for PR testing  export fn command in DEBUG mode  export env command in DEBUG mode  doctor: add instruction about setting DOCKER_XXX env vars in Profile  info() function added to print green text to STDOUT   Removed  Changed   HOME env var is also inherited (boot2docker version failed)  release process fully automatized", 
            "title": "[v0.0.6] - 2015-03-25"
        }, 
        {
            "location": "/changelog/#v005-2015-03-23", 
            "text": "update  command works without dos2unix", 
            "title": "[v0.0.5] - 2015-03-23"
        }, 
        {
            "location": "/changelog/#v004-2015-03-23", 
            "text": "Fixed   debug function fixed  DEBUG, TRACE and CBD_DEFAULT_PROFILE env vars are inherited   Added   Profile handling added with docs  One-liner install added  Docs: install and update process described  Docs: release process described with sample git commands  Print version number in debug mode  update-snap  downloads binary from latest os specific CircleCI binary artifact.   Removed  Changed   Tool specific library renamed from cloudbreak.bash to deployer.bash", 
            "title": "[v0.0.4] - 2015-03-23"
        }, 
        {
            "location": "/changelog/#v006-2015-03-25_1", 
            "text": "", 
            "title": "[v0.0.6] - 2015-03-25"
        }, 
        {
            "location": "/changelog/#v003-2015-03-19", 
            "text": "Fixed   make release  creates binary with X.X.X version when on release branch otherwise X.X.X-gitrev   Added   Docs: release process described   Removed  Changed", 
            "title": "[v0.0.3] - 2015-03-19"
        }, 
        {
            "location": "/changelog/#v002-2015-03-19", 
            "text": "Added  Added\n- selfupdate command\n- gray debug to stderr", 
            "title": "[v0.0.2] - 2015-03-19"
        }, 
        {
            "location": "/changelog/#v001-2015-03-18", 
            "text": "", 
            "title": "[v0.0.1] - 2015-03-18"
        }, 
        {
            "location": "/changelog/#added_22", 
            "text": "help command added  version command added  Added --version  CircleCI build  Linux/Darwin binary releases on github", 
            "title": "Added"
        }
    ]
}